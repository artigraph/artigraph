{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"artigraph Declarative Data Production Artigraph is a tool to improve the authorship, management, and quality of data. It emphasizes that the core deliverable of a data pipeline or workflow is the data, not the tasks. Artigraph is hosted by the LF AI and Data Foundation as a Sandbox project. Installation Artigraph can be installed from PyPI on python 3.9+ with pip install arti . Example This sample from the spend example highlights computing the total amount spent from a series of purchase transactions: from pathlib import Path from typing import Annotated from arti import Annotation , Artifact , Graph , producer from arti.formats.json import JSON from arti.storage.local import LocalFile from arti.types import Collection , Date , Float64 , Int64 , Struct from arti.versions import SemVer DIR = Path ( __file__ ) . parent class Vendor ( Annotation ): name : str class Transactions ( Artifact ): \"\"\"Transactions partitioned by day.\"\"\" type = Collection ( element = Struct ( fields = { \"id\" : Int64 (), \"date\" : Date (), \"amount\" : Float64 ()}), partition_by = ( \"date\" ,), ) class TotalSpend ( Artifact ): \"\"\"Aggregate spend over all time.\"\"\" type = Float64 () format = JSON () storage = LocalFile () @producer ( version = SemVer ( major = 1 , minor = 0 , patch = 0 )) def aggregate_transactions ( transactions : Annotated [ list [ dict ], Transactions ] ) -> Annotated [ float , TotalSpend ]: return sum ( txn [ \"amount\" ] for txn in transactions ) with Graph ( name = \"test-graph\" ) as g : g . artifacts . vendor . transactions = Transactions ( annotations = [ Vendor ( name = \"Acme\" )], format = JSON (), storage = LocalFile ( path = str ( DIR / \"transactions\" / \" {date.iso} .json\" )), ) g . artifacts . spend = aggregate_transactions ( transactions = g . artifacts . vendor . transactions ) The full example can be run easily with docker run --rm artigraph/example-spend : INFO : root : Writing mock Transactions data : INFO : root : /usr/src/app/transactions/ 2021 - 10 - 01 . json : [{ 'id' : 1 , 'amount' : 9.95 }, { 'id' : 2 , 'amount' : 7.5 }] INFO : root : /usr/src/app/transactions/ 2021 - 10 - 02 . json : [{ 'id' : 3 , 'amount' : 5.0 }, { 'id' : 4 , 'amount' : 12.0 }, { 'id' : 4 , 'amount' : 7.55 }] INFO : root : Building aggregate_transactions ( transactions = Transactions ( format = JSON (), storage = LocalFile ( path = '/usr/src/app/transactions/{date.iso}.json' ), annotations =( Vendor ( name = 'Acme' ),)))... INFO : root : Build finished . INFO : root : Final Spend data : INFO : root : /tmp/test-graph/spend/7564053533177891797/s pend . json : 42.0 Community Everyone is welcome to join the community - learn more in out support and contributing pages! Presentations 2022-01-27: Requesting Sandbox Incubation with LF AI & Data ( deck , presentation @ 6m35s)","title":"Home"},{"location":"#artigraph","text":"Declarative Data Production Artigraph is a tool to improve the authorship, management, and quality of data. It emphasizes that the core deliverable of a data pipeline or workflow is the data, not the tasks. Artigraph is hosted by the LF AI and Data Foundation as a Sandbox project.","title":"artigraph"},{"location":"#installation","text":"Artigraph can be installed from PyPI on python 3.9+ with pip install arti .","title":"Installation"},{"location":"#example","text":"This sample from the spend example highlights computing the total amount spent from a series of purchase transactions: from pathlib import Path from typing import Annotated from arti import Annotation , Artifact , Graph , producer from arti.formats.json import JSON from arti.storage.local import LocalFile from arti.types import Collection , Date , Float64 , Int64 , Struct from arti.versions import SemVer DIR = Path ( __file__ ) . parent class Vendor ( Annotation ): name : str class Transactions ( Artifact ): \"\"\"Transactions partitioned by day.\"\"\" type = Collection ( element = Struct ( fields = { \"id\" : Int64 (), \"date\" : Date (), \"amount\" : Float64 ()}), partition_by = ( \"date\" ,), ) class TotalSpend ( Artifact ): \"\"\"Aggregate spend over all time.\"\"\" type = Float64 () format = JSON () storage = LocalFile () @producer ( version = SemVer ( major = 1 , minor = 0 , patch = 0 )) def aggregate_transactions ( transactions : Annotated [ list [ dict ], Transactions ] ) -> Annotated [ float , TotalSpend ]: return sum ( txn [ \"amount\" ] for txn in transactions ) with Graph ( name = \"test-graph\" ) as g : g . artifacts . vendor . transactions = Transactions ( annotations = [ Vendor ( name = \"Acme\" )], format = JSON (), storage = LocalFile ( path = str ( DIR / \"transactions\" / \" {date.iso} .json\" )), ) g . artifacts . spend = aggregate_transactions ( transactions = g . artifacts . vendor . transactions ) The full example can be run easily with docker run --rm artigraph/example-spend : INFO : root : Writing mock Transactions data : INFO : root : /usr/src/app/transactions/ 2021 - 10 - 01 . json : [{ 'id' : 1 , 'amount' : 9.95 }, { 'id' : 2 , 'amount' : 7.5 }] INFO : root : /usr/src/app/transactions/ 2021 - 10 - 02 . json : [{ 'id' : 3 , 'amount' : 5.0 }, { 'id' : 4 , 'amount' : 12.0 }, { 'id' : 4 , 'amount' : 7.55 }] INFO : root : Building aggregate_transactions ( transactions = Transactions ( format = JSON (), storage = LocalFile ( path = '/usr/src/app/transactions/{date.iso}.json' ), annotations =( Vendor ( name = 'Acme' ),)))... INFO : root : Build finished . INFO : root : Final Spend data : INFO : root : /tmp/test-graph/spend/7564053533177891797/s pend . json : 42.0","title":"Example"},{"location":"#community","text":"Everyone is welcome to join the community - learn more in out support and contributing pages!","title":"Community"},{"location":"#presentations","text":"2022-01-27: Requesting Sandbox Incubation with LF AI & Data ( deck , presentation @ 6m35s)","title":"Presentations"},{"location":"CHANGELOG/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [Unreleased] Added Changed Removed v0.0.1 Added Add project skeleton by @JacobHayes in https://github.com/artigraph/artigraph/pull/1 CI Tweaks by @JacobHayes in https://github.com/artigraph/artigraph/pull/6 Apply isort/black formatting in pre-commit (pytest only checks) by @JacobHayes in https://github.com/artigraph/artigraph/pull/13 Add dev tool to automate worktree creation by @JacobHayes in https://github.com/artigraph/artigraph/pull/14 Add start to a set of the core Artigraph interfaces! by @JacobHayes in https://github.com/artigraph/artigraph/pull/5 Add internal typing stubs by @JacobHayes in https://github.com/artigraph/artigraph/pull/26 Add base Fingerprint class by @JacobHayes in https://github.com/artigraph/artigraph/pull/35 Add base Version classes by @JacobHayes in https://github.com/artigraph/artigraph/pull/36 Migrate ancillary classes to Pydantic Models by @JacobHayes in https://github.com/artigraph/artigraph/pull/60 Initial example of Python TypeSystem by @mikss in https://github.com/artigraph/artigraph/pull/58 Introduce python view, pickle format, local storage by @mikss in https://github.com/artigraph/artigraph/pull/62 move read/write outside of View by @mikss in https://github.com/artigraph/artigraph/pull/65 Int as a view, not Python by @mikss in https://github.com/artigraph/artigraph/pull/66 Support registration priority and use for Views by @JacobHayes in https://github.com/artigraph/artigraph/pull/69 Rename non-standard dunder attributes to sunder by @JacobHayes in https://github.com/artigraph/artigraph/pull/70 Prep for pydantic Artifacts/Producers by @JacobHayes in https://github.com/artigraph/artigraph/pull/71 Use pydantic for Artifacts by @JacobHayes in https://github.com/artigraph/artigraph/pull/72 Expand Python TypeSystem by @mikss in https://github.com/artigraph/artigraph/pull/74 Use pydantic for Producers by @JacobHayes in https://github.com/artigraph/artigraph/pull/73 Add ObjectBox and frozen Type.metadata by @JacobHayes in https://github.com/artigraph/artigraph/pull/81 Add Type.nullable and Enum(Type) by @JacobHayes in https://github.com/artigraph/artigraph/pull/82 Add pydantic and initial sgqlc typesystems by @joycex99 in https://github.com/artigraph/artigraph/pull/75 Replace Type.metadata with TypeSystem hints by @JacobHayes in https://github.com/artigraph/artigraph/pull/90 Support setting sgqlc schema by @JacobHayes in https://github.com/artigraph/artigraph/pull/98 Refactor PartitionKeys and add StoragePartition by @JacobHayes in https://github.com/artigraph/artigraph/pull/99 CI Multiple Python Versions and py 3.10 support by @JacobHayes in https://github.com/artigraph/artigraph/pull/116 Add Graph.build and lots of other changes by @JacobHayes in https://github.com/artigraph/artigraph/pull/133 Fix typo in Collection.__init__ by @bnaul in https://github.com/artigraph/artigraph/pull/143 Open Source by @JacobHayes in https://github.com/artigraph/artigraph/pull/148 Bump pydantic to 1.9+ by @JacobHayes in https://github.com/artigraph/artigraph/pull/153 Changed Removed","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"CHANGELOG/#unreleased","text":"","title":"[Unreleased]"},{"location":"CHANGELOG/#added","text":"","title":"Added"},{"location":"CHANGELOG/#changed","text":"","title":"Changed"},{"location":"CHANGELOG/#removed","text":"","title":"Removed"},{"location":"CHANGELOG/#v001","text":"","title":"v0.0.1"},{"location":"CHANGELOG/#added_1","text":"Add project skeleton by @JacobHayes in https://github.com/artigraph/artigraph/pull/1 CI Tweaks by @JacobHayes in https://github.com/artigraph/artigraph/pull/6 Apply isort/black formatting in pre-commit (pytest only checks) by @JacobHayes in https://github.com/artigraph/artigraph/pull/13 Add dev tool to automate worktree creation by @JacobHayes in https://github.com/artigraph/artigraph/pull/14 Add start to a set of the core Artigraph interfaces! by @JacobHayes in https://github.com/artigraph/artigraph/pull/5 Add internal typing stubs by @JacobHayes in https://github.com/artigraph/artigraph/pull/26 Add base Fingerprint class by @JacobHayes in https://github.com/artigraph/artigraph/pull/35 Add base Version classes by @JacobHayes in https://github.com/artigraph/artigraph/pull/36 Migrate ancillary classes to Pydantic Models by @JacobHayes in https://github.com/artigraph/artigraph/pull/60 Initial example of Python TypeSystem by @mikss in https://github.com/artigraph/artigraph/pull/58 Introduce python view, pickle format, local storage by @mikss in https://github.com/artigraph/artigraph/pull/62 move read/write outside of View by @mikss in https://github.com/artigraph/artigraph/pull/65 Int as a view, not Python by @mikss in https://github.com/artigraph/artigraph/pull/66 Support registration priority and use for Views by @JacobHayes in https://github.com/artigraph/artigraph/pull/69 Rename non-standard dunder attributes to sunder by @JacobHayes in https://github.com/artigraph/artigraph/pull/70 Prep for pydantic Artifacts/Producers by @JacobHayes in https://github.com/artigraph/artigraph/pull/71 Use pydantic for Artifacts by @JacobHayes in https://github.com/artigraph/artigraph/pull/72 Expand Python TypeSystem by @mikss in https://github.com/artigraph/artigraph/pull/74 Use pydantic for Producers by @JacobHayes in https://github.com/artigraph/artigraph/pull/73 Add ObjectBox and frozen Type.metadata by @JacobHayes in https://github.com/artigraph/artigraph/pull/81 Add Type.nullable and Enum(Type) by @JacobHayes in https://github.com/artigraph/artigraph/pull/82 Add pydantic and initial sgqlc typesystems by @joycex99 in https://github.com/artigraph/artigraph/pull/75 Replace Type.metadata with TypeSystem hints by @JacobHayes in https://github.com/artigraph/artigraph/pull/90 Support setting sgqlc schema by @JacobHayes in https://github.com/artigraph/artigraph/pull/98 Refactor PartitionKeys and add StoragePartition by @JacobHayes in https://github.com/artigraph/artigraph/pull/99 CI Multiple Python Versions and py 3.10 support by @JacobHayes in https://github.com/artigraph/artigraph/pull/116 Add Graph.build and lots of other changes by @JacobHayes in https://github.com/artigraph/artigraph/pull/133 Fix typo in Collection.__init__ by @bnaul in https://github.com/artigraph/artigraph/pull/143 Open Source by @JacobHayes in https://github.com/artigraph/artigraph/pull/148 Bump pydantic to 1.9+ by @JacobHayes in https://github.com/artigraph/artigraph/pull/153","title":"Added"},{"location":"CHANGELOG/#changed_1","text":"","title":"Changed"},{"location":"CHANGELOG/#removed_1","text":"","title":"Removed"},{"location":"CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at artigraph-security@lists.lfaidata.foundation. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Code Of Conduct"},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at artigraph-security@lists.lfaidata.foundation. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"CODE_OF_CONDUCT/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"CODE_OF_CONDUCT/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"CONTRIBUTING/","text":"Contributing to Artigraph Thank you for your interest in contributing to Artigraph! This document explains our contribution process and procedures. If you just need help or have a question, refer to our support page . How to Contribute a Bug Fix or Enhancement Contributions can be submitted via Pull Requests to the golden branch and must: be submitted under the Apache 2.0 license. include a Developer Certificate of Origin signoff ( git commit -s ) include tests and documentation match the Coding Style Project committers will review the contribution in a timely manner and advise of any changes needed to merge the request. Coding Style Code is formatted with black and isort . Docstring style is not yet standardized, but they should generally follow PEP257 . Development Workflow The default branch is golden (poking fun at \"golden data\"). The project is managed using poetry . We use pre-commit to automate rapid feedback via git hooks. Environment Setup If you work on macOS, the .envrc script (used by direnv ) in the repo root can automate project and environment setup for both Intel and M1 computers. Run bash .envrc to: - install brew (if necessary) - install useful system packages ( direnv , git , and pyenv ) via the Brewfile - install the correct python version with pyenv - create a virtual environment and install dependencies - install and configure pre-commit After that completes, configure direnv for your shell and run exec $SHELL . With direnv configured, the project's virtual environment will automatically be activated (and python and package versions synced!) upon cd into the repo. If you use another platform or would rather install manually, use poetry directly to manage your virtual environment(s). Contributions supporting direnv for other platforms would be appreciated!","title":"Contributing"},{"location":"CONTRIBUTING/#contributing-to-artigraph","text":"Thank you for your interest in contributing to Artigraph! This document explains our contribution process and procedures. If you just need help or have a question, refer to our support page .","title":"Contributing to Artigraph"},{"location":"CONTRIBUTING/#how-to-contribute-a-bug-fix-or-enhancement","text":"Contributions can be submitted via Pull Requests to the golden branch and must: be submitted under the Apache 2.0 license. include a Developer Certificate of Origin signoff ( git commit -s ) include tests and documentation match the Coding Style Project committers will review the contribution in a timely manner and advise of any changes needed to merge the request.","title":"How to Contribute a Bug Fix or Enhancement"},{"location":"CONTRIBUTING/#coding-style","text":"Code is formatted with black and isort . Docstring style is not yet standardized, but they should generally follow PEP257 .","title":"Coding Style"},{"location":"CONTRIBUTING/#development-workflow","text":"The default branch is golden (poking fun at \"golden data\"). The project is managed using poetry . We use pre-commit to automate rapid feedback via git hooks.","title":"Development Workflow"},{"location":"CONTRIBUTING/#environment-setup","text":"If you work on macOS, the .envrc script (used by direnv ) in the repo root can automate project and environment setup for both Intel and M1 computers. Run bash .envrc to: - install brew (if necessary) - install useful system packages ( direnv , git , and pyenv ) via the Brewfile - install the correct python version with pyenv - create a virtual environment and install dependencies - install and configure pre-commit After that completes, configure direnv for your shell and run exec $SHELL . With direnv configured, the project's virtual environment will automatically be activated (and python and package versions synced!) upon cd into the repo. If you use another platform or would rather install manually, use poetry directly to manage your virtual environment(s). Contributions supporting direnv for other platforms would be appreciated!","title":"Environment Setup"},{"location":"GOVERNANCE/","text":"Overview This project aims to be governed in a transparent, accessible way for the benefit of the community. All participation in this project is open and not bound to corporate affiliation. Participants are bound to the project's Code of Conduct . Project Roles Contributor The contributor role is the starting role for anyone participating in the project and wishing to contribute code. Process for Becoming a Contributor Review the Contribution Guidelines to ensure your contribution is inline with the project's coding and styling guidelines. Submit your code as a PR with the appropriate DCO signoff Have your submission approved by the committer(s) and merged into the codebase. Committer The committer role enables the contributor to commit code directly to the repository, but also comes with the responsibility of being a responsible leader in the community. The existing committers are: Name GitHub ID Jacob Hayes @JacobHayes Process for Becoming a Committer Show your experience with the codebase through contributions and engagement on the community channels. Request to become a committer. To do this, create a new pull request that adds your name and details to the table above and request existing committers to approve. After the majority of committers approve you, merge in the PR. Be sure to tag whomever is managing the GitHub permissions to update the committers team in GitHub. Committer Responsibilities Triage GitHub issues and perform pull request reviews for other committers and the community. Make sure that ongoing PRs are moving forward at the right pace or closing them. In general continue to be willing to spend at least 25% of ones time working on the project (~1.25 business days per week). When Does a Committer Lose Committer Status If a committer is no longer interested or cannot perform the committer duties listed above, they should volunteer to be moved to emeritus status. In extreme cases this can also occur by a vote of the committers per the voting process below. Lead The project committers will elect a lead (and optionally a co-lead) which will be the primary point of contact for the project and representative to the TAC upon becoming an Active stage project. The lead(s) will be responsible for the overall project health and direction, coordination of activities, and working with other projects and committees as needed for the continuted growth of the project. The current project lead is: @JacobHayes. Release Process Project releases will occur on a scheduled basis as agreed to by the committers. Conflict Resolution and Voting In general, we prefer that technical issues and committer membership are amicably worked out between the persons involved. If a dispute cannot be decided independently, the committers can be called in to decide an issue. If the committers themselves cannot decide an issue, the issue will be resolved by voting. The voting process is a simple majority in which each committer receives one vote. Communication This project, just like all of open source, is a global community. In addition to the Code of Conduct , this project will: Keep all communication on open channels (mailing list, forums, chat). Be respectful of time and language differences between community members (such as scheduling meetings, email/issue responsiveness, etc). Ensure tools are able to be used by community members regardless of their region. If you have concerns about communication challenges for this project, please contact the committers.","title":"Governance"},{"location":"GOVERNANCE/#overview","text":"This project aims to be governed in a transparent, accessible way for the benefit of the community. All participation in this project is open and not bound to corporate affiliation. Participants are bound to the project's Code of Conduct .","title":"Overview"},{"location":"GOVERNANCE/#project-roles","text":"","title":"Project Roles"},{"location":"GOVERNANCE/#contributor","text":"The contributor role is the starting role for anyone participating in the project and wishing to contribute code.","title":"Contributor"},{"location":"GOVERNANCE/#process-for-becoming-a-contributor","text":"Review the Contribution Guidelines to ensure your contribution is inline with the project's coding and styling guidelines. Submit your code as a PR with the appropriate DCO signoff Have your submission approved by the committer(s) and merged into the codebase.","title":"Process for Becoming a Contributor"},{"location":"GOVERNANCE/#committer","text":"The committer role enables the contributor to commit code directly to the repository, but also comes with the responsibility of being a responsible leader in the community. The existing committers are: Name GitHub ID Jacob Hayes @JacobHayes","title":"Committer"},{"location":"GOVERNANCE/#process-for-becoming-a-committer","text":"Show your experience with the codebase through contributions and engagement on the community channels. Request to become a committer. To do this, create a new pull request that adds your name and details to the table above and request existing committers to approve. After the majority of committers approve you, merge in the PR. Be sure to tag whomever is managing the GitHub permissions to update the committers team in GitHub.","title":"Process for Becoming a Committer"},{"location":"GOVERNANCE/#committer-responsibilities","text":"Triage GitHub issues and perform pull request reviews for other committers and the community. Make sure that ongoing PRs are moving forward at the right pace or closing them. In general continue to be willing to spend at least 25% of ones time working on the project (~1.25 business days per week).","title":"Committer Responsibilities"},{"location":"GOVERNANCE/#when-does-a-committer-lose-committer-status","text":"If a committer is no longer interested or cannot perform the committer duties listed above, they should volunteer to be moved to emeritus status. In extreme cases this can also occur by a vote of the committers per the voting process below.","title":"When Does a Committer Lose Committer Status"},{"location":"GOVERNANCE/#lead","text":"The project committers will elect a lead (and optionally a co-lead) which will be the primary point of contact for the project and representative to the TAC upon becoming an Active stage project. The lead(s) will be responsible for the overall project health and direction, coordination of activities, and working with other projects and committees as needed for the continuted growth of the project. The current project lead is: @JacobHayes.","title":"Lead"},{"location":"GOVERNANCE/#release-process","text":"Project releases will occur on a scheduled basis as agreed to by the committers.","title":"Release Process"},{"location":"GOVERNANCE/#conflict-resolution-and-voting","text":"In general, we prefer that technical issues and committer membership are amicably worked out between the persons involved. If a dispute cannot be decided independently, the committers can be called in to decide an issue. If the committers themselves cannot decide an issue, the issue will be resolved by voting. The voting process is a simple majority in which each committer receives one vote.","title":"Conflict Resolution and Voting"},{"location":"GOVERNANCE/#communication","text":"This project, just like all of open source, is a global community. In addition to the Code of Conduct , this project will: Keep all communication on open channels (mailing list, forums, chat). Be respectful of time and language differences between community members (such as scheduling meetings, email/issue responsiveness, etc). Ensure tools are able to be used by community members regardless of their region. If you have concerns about communication challenges for this project, please contact the committers.","title":"Communication"},{"location":"SECURITY/","text":"Security Policy Reporting a Vulnerability Send vulnerability reports to artigraph-security@lists.lfaidata.foundation and a committer will respond soon.","title":"Security"},{"location":"SECURITY/#security-policy","text":"","title":"Security Policy"},{"location":"SECURITY/#reporting-a-vulnerability","text":"Send vulnerability reports to artigraph-security@lists.lfaidata.foundation and a committer will respond soon.","title":"Reporting a Vulnerability"},{"location":"SUPPORT/","text":"Artigraph Support How to Ask for Help If you have trouble installing, building, or using Artigraph, but there's not yet reason to suspect you've encountered a genuine bug, start a Discussions . This is a great place for questions such has \"How do I...\". How to Report a Bug or Request an Enhancement Artigraph manages bug and enhancements via Issues . The issue template will guide you on making an effective report. How to Report a Security Vulnerability If you think you've found a potential vulnerability in Artigraph, follow the steps in the security policy to responsibly disclose it. How to Contribute We'd love to have your contribution - please refer to our contributing page for direction!","title":"Support"},{"location":"SUPPORT/#artigraph-support","text":"","title":"Artigraph Support"},{"location":"SUPPORT/#how-to-ask-for-help","text":"If you have trouble installing, building, or using Artigraph, but there's not yet reason to suspect you've encountered a genuine bug, start a Discussions . This is a great place for questions such has \"How do I...\".","title":"How to Ask for Help"},{"location":"SUPPORT/#how-to-report-a-bug-or-request-an-enhancement","text":"Artigraph manages bug and enhancements via Issues . The issue template will guide you on making an effective report.","title":"How to Report a Bug or Request an Enhancement"},{"location":"SUPPORT/#how-to-report-a-security-vulnerability","text":"If you think you've found a potential vulnerability in Artigraph, follow the steps in the security policy to responsibly disclose it.","title":"How to Report a Security Vulnerability"},{"location":"SUPPORT/#how-to-contribute","text":"We'd love to have your contribution - please refer to our contributing page for direction!","title":"How to Contribute"},{"location":"reference/arti/","text":"Module arti None None View Source from __future__ import annotations import importlib.metadata __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) __version__ = importlib . metadata . version ( \"arti\" ) import threading from typing import Optional from arti.annotations import Annotation from arti.artifacts import Artifact from arti.backends import Backend , Connection from arti.executors import Executor from arti.fingerprints import Fingerprint from arti.formats import Format from arti.graphs import Graph , GraphSnapshot from arti.io import read , register_reader , register_writer , write from arti.partitions import CompositeKey , CompositeKeyTypes , InputFingerprints , PartitionKey from arti.producers import PartitionDependencies , Producer , producer from arti.statistics import Statistic from arti.storage import Storage , StoragePartition , StoragePartitions from arti.thresholds import Threshold from arti.types import Type , TypeAdapter , TypeSystem from arti.versions import Version from arti.views import View # Export all interfaces. __all__ = [ \"Annotation\" , \"Artifact\" , \"Backend\" , \"CompositeKey\" , \"CompositeKeyTypes\" , \"Connection\" , \"Executor\" , \"Fingerprint\" , \"Format\" , \"Graph\" , \"GraphSnapshot\" , \"InputFingerprints\" , \"PartitionDependencies\" , \"PartitionKey\" , \"Producer\" , \"Statistic\" , \"Storage\" , \"StoragePartition\" , \"StoragePartitions\" , \"Threshold\" , \"Type\" , \"TypeAdapter\" , \"TypeSystem\" , \"Version\" , \"View\" , \"producer\" , \"read\" , \"register_reader\" , \"register_writer\" , \"write\" , ] class _Context ( threading . local ): def __init__ ( self ) -> None : super () . __init__ () self . graph : Optional [ Graph ] = None context = _Context () Sub-modules arti.annotations arti.artifacts arti.backends arti.executors arti.fingerprints arti.formats arti.graphs arti.internal arti.io arti.partitions arti.producers arti.statistics arti.storage arti.thresholds arti.types arti.versions arti.views Variables CompositeKey CompositeKeyTypes InputFingerprints PartitionDependencies StoragePartitions Functions producer def producer ( * , annotations : 'Optional[tuple[Annotation, ...]]' = None , map : 'Optional[MapSig]' = None , name : 'Optional[str]' = None , validate_outputs : 'Optional[ValidateSig]' = None , version : 'Optional[Version]' = None ) -> 'Callable[[BuildSig], type[Producer]]' View Source def producer ( * , annotations : Optional [ tuple[Annotation, ... ] ] = None , map : Optional [ MapSig ] = None , name : Optional [ str ] = None , validate_outputs : Optional [ ValidateSig ] = None , version : Optional [ Version ] = None , ) -> Callable [ [BuildSig ] , type [ Producer ] ]: def decorate ( build : BuildSig ) -> type [ Producer ] : nonlocal name name = build . __name__ if name is None else name __annotations__ : dict [ str, Any ] = {} for param in signature ( build ). parameters . values () : with wrap_exc ( ValueError , prefix = f \"{name} {param.name} param\" ) : view = View . from_annotation ( param . annotation , mode = \"READ\" ) __annotations__ [ param.name ] = view . artifact_class # If overriding , set an explicit \"annotations\" hint until [ 1 ] is released . # # 1 : https : // github . com / samuelcolvin / pydantic / pull / 3018 if annotations : __annotations__ [ \"annotations\" ] = tuple [ Annotation, ... ] if version : __annotations__ [ \"version\" ] = Version return type ( name , ( Producer ,), { k : v for k , v in { \"__annotations__\" : __annotations__ , \"__module__\" : get_module_name ( depth = 2 ), # Not our module , but our caller ' s . \"annotations\" : annotations , \"build\" : staticmethod ( build ), \"map\" : None if map is None else staticmethod ( map ), \"validate_outputs\" : ( None if validate_outputs is None else staticmethod ( validate_outputs ) ), \"version\" : version , } . items () if v is not None } , ) return decorate read def read ( type_ : 'Type' , format : 'Format' , storage_partitions : 'Sequence[StoragePartition]' , view : 'View' ) -> 'Any' View Source def read ( type_ : Type , format : Format , storage_partitions : Sequence [ StoragePartition ] , view : View ) -> Any : if not storage_partitions : # NOTE : Aside from simplifying this check up front , multiple dispatch with unknown list # element type can be ambiguous / error . raise FileNotFoundError ( \"No data\" ) if len ( storage_partitions ) > 1 and not is_partitioned ( type_ ) : raise ValueError ( f \"Multiple partitions can only be read into a partitioned Collection, not {type_}\" ) # TODO Checks that the returned data matches the Type / View # # Likely add a View method that can handle this type + schema checking , filtering to column / row subsets if necessary , etc return _read ( type_ , format , storage_partitions , view ) register_reader def register_reader ( * args : 'Any' ) -> 'Callable[[REGISTERED], REGISTERED]' Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return cast ( Callable [ ..., Any ] , super (). register ( * args )) register_writer def register_writer ( * args : 'Any' ) -> 'Callable[[REGISTERED], REGISTERED]' Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return cast ( Callable [ ..., Any ] , super (). register ( * args )) write def write ( data : 'Any' , type_ : 'Type' , format : 'Format' , storage_partition : 'StoragePartitionVar' , view : 'View' ) -> 'StoragePartitionVar' View Source def write ( data : Any , type_ : Type , for mat : Format , storage_partition : StoragePartitionVar , view : View ) -> StoragePartitionVar : if ( updated := _write ( data , type_ , for mat , storage_partition , view )) is not None : return updated return storage_partition Classes Annotation class Annotation ( __pydantic_self__ , ** data : Any ) View Source class Annotation ( Model ): \"\"\"An Annotation is a piece of human knowledge associated with an Artifact.\"\"\" _abstract_ = True Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Artifact class Artifact ( __pydantic_self__ , ** data : Any ) View Source class Artifact ( Model ) : \" \"\" An Artifact is the base structure describing an existing or generated dataset. An Artifact is comprised of three key elements: - `type`: spec of the data's structure, such as data types, nullable, etc. - `format`: the data's serialized format, such as CSV, Parquet, database native, etc. - `storage`: the data's persistent storage system, such as blob storage, database native, etc. In addition to the core elements, an Artifact can be tagged with additional `annotations` (to associate it with human knowledge) and `statistics` (to track derived characteristics over time). \"\" \" type : Type format : Format = Field ( default_factory = Format . get_default ) storage : Storage [ StoragePartition ] = Field ( default_factory = Storage . get_default ) annotations : tuple [ Annotation , ... ] = () statistics : tuple [ Statistic , ... ] = () # Hide `producer_output` in repr to prevent showing the entire upstream graph. # # ProducerOutput is a ForwardRef/cyclic import. Quote the entire hint to force full resolution # during `.update_forward_refs`, rather than `Optional[ForwardRef(\"ProducerOutput\")]`. producer_output : Optional [ ProducerOutput ] = Field ( None , repr = False ) # NOTE: Narrow the fields that affect the fingerprint to minimize changes (which trigger # recompute). Importantly, avoid fingerprinting the `.producer_output` (ie: the *upstream* # producer) to prevent cascading fingerprint changes (Producer.fingerprint accesses the *input* # Artifact.fingerprints). Even so, this may still be quite sensitive. _fingerprint_includes_ = frozenset ( [ \"type\" , \"format\" , \"storage\" ] ) @validator ( \"format\" , always = True ) @classmethod def _validate_format ( cls , format : Format , values : dict [ str , Any ] ) -> Format : if ( type_ := values . get ( \"type\" )) is not None : return format . _visit_type ( type_ ) return format @validator ( \"storage\" , always = True ) @classmethod def _validate_storage ( cls , storage : Storage [ StoragePartition ] , values : dict [ str , Any ] ) -> Storage [ StoragePartition ] : if ( type_ := values . get ( \"type\" )) is not None : storage = storage . _visit_type ( type_ ) if ( format_ := values . get ( \"format\" )) is not None : storage = storage . _visit_format ( format_ ) return storage @validator ( \"annotations\" , \"statistics\" , always = True , pre = True ) @classmethod def _merge_class_defaults ( cls , value : tuple [ Any , ... ] , field : ModelField ) -> tuple [ Any , ... ] : return tuple ( chain ( get_field_default ( cls , field . name ) or (), value )) @classmethod def cast ( cls , value : Any ) -> Artifact : \" \"\" Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `ArtifactBox` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, we attempt to map to a `Type` and return an Artifact instance with defaulted Format and Storage \"\" \" from arti . formats . json import JSON from arti . producers import Producer from arti . storage . literal import StringLiteral from arti . types . python import python_type_system if isinstance ( value , Artifact ) : return value if isinstance ( value , Producer ) : output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ) : return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma: no cover # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( f \"{type(value).__name__} doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \"{type(value).__name__} produces {len(output_artifacts)} Artifacts. Try assigning each to a new name in the Graph!\" ) annotation = get_annotation_from_value ( value ) return cls ( type = python_type_system . to_artigraph ( annotation , hints = {} ), format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods cast def cast ( value : 'Any' ) -> 'Artifact' Attempt to convert an arbitrary value to an appropriate Artifact instance. Artifact.cast is used to convert values assigned to an ArtifactBox (such as Graph.artifacts ) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, we attempt to map to a Type and return an Artifact instance with defaulted Format and Storage View Source @classmethod def cast ( cls , value : Any ) -> Artifact : \"\"\"Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `ArtifactBox` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, we attempt to map to a `Type` and return an Artifact instance with defaulted Format and Storage \"\"\" from arti.formats.json import JSON from arti.producers import Producer from arti.storage.literal import StringLiteral from arti.types.python import python_type_system if isinstance ( value , Artifact ): return value if isinstance ( value , Producer ): output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ): return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma: no cover # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( f \" { type ( value ) . __name__ } doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \" { type ( value ) . __name__ } produces { len ( output_artifacts ) } Artifacts. Try assigning each to a new name in the Graph!\" ) annotation = get_annotation_from_value ( value ) return cls ( type = python_type_system . to_artigraph ( annotation , hints = {}), format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), ) construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Backend class Backend ( __pydantic_self__ , ** data : Any ) View Source class Backend ( Model , Generic [ ConnectionVar ] ) : \"\"\"Backend represents a storage for internal Artigraph metadata. Backend storage is an addressable location (local path, database connection, etc) that tracks metadata for a collection of Graphs over time, including: - the Artifact(s)->Producer->Artifact(s) dependency graph - Artifact Annotations, Statistics, Partitions, and other metadata - Artifact and Producer Fingerprints - etc \"\"\" @contextmanager @abstractmethod def connect ( self ) -> Iterator [ ConnectionVar ] : raise NotImplementedError () Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic Descendants arti.backends.memory.MemoryBackend Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods connect def connect ( self ) -> 'Iterator[ConnectionVar]' View Source @contextmanager @abstractmethod def connect ( self ) -> Iterator [ ConnectionVar ] : raise NotImplementedError () copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Connection class Connection ( / , * args , ** kwargs ) View Source class Connection : \" \"\" Connection is a wrapper around an active connection to a Backend resource. For example, a Backend connecting to a database might wrap up a SQLAlchemy connection in a Connection subclass implementing the required methods. \"\" \" # Artifact partitions - independent of a specific GraphSnapshot @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \" \"\" Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a GraphSnapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \" \"\" Add more partitions for a Storage spec. \"\" \" raise NotImplementedError () # Graph @abstractmethod def read_graph ( self , name : str , fingerprint : Fingerprint ) -> Graph : \" \"\" Fetch an instance of the named Graph. \"\" \" raise NotImplementedError () @abstractmethod def write_graph ( self , graph : Graph ) -> None : \" \"\" Write the Graph and all linked Artifacts and Producers to the database. \"\" \" raise NotImplementedError () # GraphSnapshot @abstractmethod def read_snapshot ( self , name : str , fingerprint : Fingerprint ) -> GraphSnapshot : \" \"\" Fetch an instance of the named GraphSnapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_snapshot ( self , snapshot : GraphSnapshot ) -> None : \" \"\" Write the GraphSnapshot to the database. \"\" \" raise NotImplementedError () @abstractmethod def read_snapshot_tag ( self , name : str , tag : str ) -> GraphSnapshot : \" \"\" Fetch the GraphSnapshot for the named tag. \"\" \" raise NotImplementedError () @abstractmethod def write_snapshot_tag ( self , snapshot : GraphSnapshot , tag : str , overwrite : bool = False ) -> None : \" \"\" Stamp a GraphSnapshot with an arbitrary tag. \"\" \" raise NotImplementedError () @abstractmethod def read_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \" \"\" Read the known Partitions for the named Artifact in a specific GraphSnapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \" \"\" Link the Partitions to the named Artifact in a specific GraphSnapshot. \"\" \" raise NotImplementedError () # Helpers def write_artifact_and_graph_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_snapshot_partitions ( snapshot , artifact_key , artifact , partitions ) @contextmanager def connect ( self ) -> Iterator [ Self ] : \" \"\" Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... \"\" \" yield self @classmethod def __get_validators__ ( cls ) -> list [ Callable [[ Any , ModelField ] , Any ]] : \" \"\" Return an empty list of \" validators \". Allows using a Connection (which is not a model) as a field in other models without setting `arbitrary_types_allowed` (which applies broadly). [1]. 1: https://docs.pydantic.dev/usage/types/#generic-classes-as-types \"\" \" return [] Descendants arti.backends.memory.MemoryConnection Methods connect def connect ( self ) -> 'Iterator[Self]' Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... View Source @contextmanager def connect ( self ) -> Iterator [ Self ] : \"\"\"Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... \"\"\" yield self read_artifact_partitions def read_artifact_partitions ( self , artifact : 'Artifact' , input_fingerprints : 'InputFingerprints' = {} ) -> 'StoragePartitions' Read all known Partitions for this Storage spec. If input_fingerprints is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless input_fingerprints is provided matching those for a GraphSnapshot. View Source @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \" \"\" Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a GraphSnapshot. \"\" \" raise NotImplementedError () read_graph def read_graph ( self , name : 'str' , fingerprint : 'Fingerprint' ) -> 'Graph' Fetch an instance of the named Graph. View Source @abstractmethod def read_graph ( self , name : str , fingerprint : Fingerprint ) -> Graph : \"\"\"Fetch an instance of the named Graph.\"\"\" raise NotImplementedError () read_snapshot def read_snapshot ( self , name : 'str' , fingerprint : 'Fingerprint' ) -> 'GraphSnapshot' Fetch an instance of the named GraphSnapshot. View Source @abstractmethod def read_snapshot ( self , name : str , fingerprint : Fingerprint ) -> GraphSnapshot : \"\"\"Fetch an instance of the named GraphSnapshot.\"\"\" raise NotImplementedError () read_snapshot_partitions def read_snapshot_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' ) -> 'StoragePartitions' Read the known Partitions for the named Artifact in a specific GraphSnapshot. View Source @abstractmethod def read_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \"\"\"Read the known Partitions for the named Artifact in a specific GraphSnapshot.\"\"\" raise NotImplementedError () read_snapshot_tag def read_snapshot_tag ( self , name : 'str' , tag : 'str' ) -> 'GraphSnapshot' Fetch the GraphSnapshot for the named tag. View Source @abstractmethod def read_snapshot_tag ( self , name : str , tag : str ) -> GraphSnapshot : \"\"\"Fetch the GraphSnapshot for the named tag.\"\"\" raise NotImplementedError () write_artifact_and_graph_partitions def write_artifact_and_graph_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' View Source def write_artifact_and_graph_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_snapshot_partitions ( snapshot , artifact_key , artifact , partitions ) write_artifact_partitions def write_artifact_partitions ( self , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' Add more partitions for a Storage spec. View Source @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \"\"\"Add more partitions for a Storage spec.\"\"\" raise NotImplementedError () write_graph def write_graph ( self , graph : 'Graph' ) -> 'None' Write the Graph and all linked Artifacts and Producers to the database. View Source @abstractmethod def write_graph ( self , graph : Graph ) -> None : \"\"\"Write the Graph and all linked Artifacts and Producers to the database.\"\"\" raise NotImplementedError () write_snapshot def write_snapshot ( self , snapshot : 'GraphSnapshot' ) -> 'None' Write the GraphSnapshot to the database. View Source @abstractmethod def write_snapshot ( self , snapshot : GraphSnapshot ) -> None : \"\"\"Write the GraphSnapshot to the database.\"\"\" raise NotImplementedError () write_snapshot_partitions def write_snapshot_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' Link the Partitions to the named Artifact in a specific GraphSnapshot. View Source @abstractmethod def write_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \"\"\"Link the Partitions to the named Artifact in a specific GraphSnapshot.\"\"\" raise NotImplementedError () write_snapshot_tag def write_snapshot_tag ( self , snapshot : 'GraphSnapshot' , tag : 'str' , overwrite : 'bool' = False ) -> 'None' Stamp a GraphSnapshot with an arbitrary tag. View Source @abstractmethod def write_snapshot_tag ( self , snapshot : GraphSnapshot , tag : str , overwrite : bool = False ) -> None : \"\"\"Stamp a GraphSnapshot with an arbitrary tag.\"\"\" raise NotImplementedError () Executor class Executor ( __pydantic_self__ , ** data : Any ) View Source class Executor ( Model ) : @abc . abstractmethod def build ( self , snapshot : GraphSnapshot ) -> None : raise NotImplementedError () def get_producer_inputs ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer ) -> InputPartitions : return InputPartitions ( { name : connection . read_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact ) for name , artifact in producer . inputs . items () } ) def discover_producer_partitions ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , partition_input_fingerprints : InputFingerprints , ) -> set [ CompositeKey ] : # NOTE : The output partitions may be built , but not yet associated with this GraphSnapshot # ( eg : raw input data changed , but no changes trickled into this specific Producer ). Hence # we 'll fetch all StoragePartitions for each Storage, filtered to the PKs and # input_fingerprints we' ve computed * are * for this snapshot - and then link them to the # snapshot . existing_output_partitions = { output : connection . read_artifact_partitions ( output , partition_input_fingerprints ) for output in snapshot . graph . producer_outputs [ producer ] } for artifact , partitions in existing_output_partitions . items () : connection . write_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact , partitions ) # TODO : Guarantee all outputs have the same set of identified partitions . Currently , this # pretends a partition is built for all outputs if _any_ are built for that partition . return { partition . keys for partition in chain . from_iterable ( existing_output_partitions . values ()) } def build_producer_partition ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , existing_partition_keys : set [ CompositeKey ] , input_fingerprint : Fingerprint , partition_dependencies : frozendict [ str, StoragePartitions ] , partition_key : CompositeKey , ) -> None : # TODO : Should this \"skip if exists\" live here or higher up ? if partition_key in existing_partition_keys : pk_str = f \" for: {dict(partition_key)}\" if partition_key else \".\" logging . info ( f \"Skipping existing {type(producer).__name__} output{pk_str}\" ) return logging . info ( f \"Building {producer} output for {partition_key}...\" ) # TODO : Catch DispatchError and give a nicer error ... maybe add this to our # @dispatch wrapper ( eg : msg arg , or even fn that returns the message to # raise ). arguments = { name : snapshot . read ( artifact = producer . inputs [ name ] , storage_partitions = partition_dependencies [ name ] , view = view , ) for name , view in producer . _build_inputs_ . items () } outputs = producer . build ( ** arguments ) if len ( producer . _outputs_ ) == 1 : outputs = ( outputs ,) validation_passed , validation_message = producer . validate_outputs ( * outputs ) if not validation_passed : raise ValueError ( validation_message ) for i , output in enumerate ( outputs ) : snapshot . write ( output , artifact = snapshot . graph . producer_outputs [ producer ][ i ] , input_fingerprint = input_fingerprint , keys = partition_key , view = producer . _outputs_ [ i ] , ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.executors.local.LocalExecutor Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods build def build ( self , snapshot : 'GraphSnapshot' ) -> 'None' View Source @abc . abstractmethod def build ( self , snapshot : GraphSnapshot ) -> None : raise NotImplementedError () build_producer_partition def build_producer_partition ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' , * , existing_partition_keys : 'set[CompositeKey]' , input_fingerprint : 'Fingerprint' , partition_dependencies : 'frozendict[str, StoragePartitions]' , partition_key : 'CompositeKey' ) -> 'None' View Source def build_producer_partition ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , existing_partition_keys : set [ CompositeKey ] , input_fingerprint : Fingerprint , partition_dependencies : frozendict [ str, StoragePartitions ] , partition_key : CompositeKey , ) -> None : # TODO : Should this \"skip if exists\" live here or higher up ? if partition_key in existing_partition_keys : pk_str = f \" for: {dict(partition_key)}\" if partition_key else \".\" logging . info ( f \"Skipping existing {type(producer).__name__} output{pk_str}\" ) return logging . info ( f \"Building {producer} output for {partition_key}...\" ) # TODO : Catch DispatchError and give a nicer error ... maybe add this to our # @dispatch wrapper ( eg : msg arg , or even fn that returns the message to # raise ). arguments = { name : snapshot . read ( artifact = producer . inputs [ name ] , storage_partitions = partition_dependencies [ name ] , view = view , ) for name , view in producer . _build_inputs_ . items () } outputs = producer . build ( ** arguments ) if len ( producer . _outputs_ ) == 1 : outputs = ( outputs ,) validation_passed , validation_message = producer . validate_outputs ( * outputs ) if not validation_passed : raise ValueError ( validation_message ) for i , output in enumerate ( outputs ) : snapshot . write ( output , artifact = snapshot . graph . producer_outputs [ producer ][ i ] , input_fingerprint = input_fingerprint , keys = partition_key , view = producer . _outputs_ [ i ] , ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. discover_producer_partitions def discover_producer_partitions ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' , * , partition_input_fingerprints : 'InputFingerprints' ) -> 'set[CompositeKey]' View Source def discover_producer_partitions ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , partition_input_fingerprints : InputFingerprints , ) -> set [ CompositeKey ] : # NOTE : The output partitions may be built , but not yet associated with this GraphSnapshot # ( eg : raw input data changed , but no changes trickled into this specific Producer ). Hence # we 'll fetch all StoragePartitions for each Storage, filtered to the PKs and # input_fingerprints we' ve computed * are * for this snapshot - and then link them to the # snapshot . existing_output_partitions = { output : connection . read_artifact_partitions ( output , partition_input_fingerprints ) for output in snapshot . graph . producer_outputs [ producer ] } for artifact , partitions in existing_output_partitions . items () : connection . write_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact , partitions ) # TODO : Guarantee all outputs have the same set of identified partitions . Currently , this # pretends a partition is built for all outputs if _any_ are built for that partition . return { partition . keys for partition in chain . from_iterable ( existing_output_partitions . values ()) } get_producer_inputs def get_producer_inputs ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' ) -> 'InputPartitions' View Source def get_producer_inputs ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer ) -> InputPartitions : return InputPartitions ( { name : connection . read_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact ) for name , artifact in producer . inputs . items () } ) json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Fingerprint class Fingerprint ( __pydantic_self__ , ** data : Any ) View Source class Fingerprint ( Model ) : \"\"\"Fingerprint represents a unique identity as an int64 value. Using an int(64) has a number of convenient properties: - can be combined independent of order with XOR - can be stored relatively cheaply - empty 0 values drop out when combined (5 ^ 0 = 5) - is relatively cross-platform (across databases, languages, etc) There are two \" special \" Fingerprints w/ factory functions that, when combined with other Fingerprints: - `empty()`: returns `empty()` - `identity()`: return the other Fingerprint \"\"\" key : Optional [ int64 ] def combine ( self , * others : Fingerprint ) -> Fingerprint : return reduce ( operator . xor , others , self ) @classmethod def empty ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None ) @classmethod def from_int ( cls , x : int , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x )) @classmethod def from_int64 ( cls , x : int64 , / ) -> Fingerprint : return cls ( key = x ) @classmethod def from_string ( cls , x : str , / ) -> Fingerprint : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x ))) @classmethod def from_uint64 ( cls , x : uint64 , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x )) @classmethod def identity ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 )) @property def is_empty ( self ) -> bool : return self . key is None @property def is_identity ( self ) -> bool : return self . key == 0 __and__ = _gen_fingerprint_binop ( operator . __and__ ) __lshift__ = _gen_fingerprint_binop ( operator . __lshift__ ) __or__ = _gen_fingerprint_binop ( operator . __or__ ) __rshift__ = _gen_fingerprint_binop ( operator . __rshift__ ) __xor__ = _gen_fingerprint_binop ( operator . __xor__ ) def __eq__ ( self , other : object ) -> bool : if isinstance ( other , int ) : other = Fingerprint . from_int ( other ) if isinstance ( other , Fingerprint ) : return self . key == other . key return NotImplemented Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values empty def empty ( ) -> 'Fingerprint' Return a Fingerprint that, when combined, will return Fingerprint.empty() View Source @classmethod def empty ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None ) from_int def from_int ( x : 'int' , / ) -> 'Fingerprint' View Source @classmethod def from_int ( cls , x : int , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x )) from_int64 def from_int64 ( x : 'int64' , / ) -> 'Fingerprint' View Source @classmethod def from_int64 ( cls , x : int64 , / ) -> Fingerprint : return cls ( key = x ) from_orm def from_orm ( obj : Any ) -> 'Model' from_string def from_string ( x : 'str' , / ) -> 'Fingerprint' Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. View Source @classmethod def from_string ( cls , x : str , / ) -> Fingerprint : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x ))) from_uint64 def from_uint64 ( x : 'uint64' , / ) -> 'Fingerprint' View Source @classmethod def from_uint64 ( cls , x : uint64 , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x )) identity def identity ( ) -> 'Fingerprint' Return a Fingerprint that, when combined, will return the other Fingerprint. View Source @classmethod def identity ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 )) parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint is_empty is_identity Methods combine def combine ( self , * others : 'Fingerprint' ) -> 'Fingerprint' View Source def combine ( self , * others : Fingerprint ) -> Fingerprint : return reduce ( operator . xor , others , self ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Format class Format ( __pydantic_self__ , ** data : Any ) View Source class Format ( Model ): \"\"\"Format represents file formats such as CSV, Parquet, native (eg: databases), etc. Formats are associated with a type system that provides a bridge between the internal Artigraph types and any external type information. \"\"\" _abstract_ = True type_system : ClassVar [ TypeSystem ] extension : str = \"\" def _visit_type ( self , type_ : Type ) -> Self : # Ensure our type system can handle the provided type. self . type_system . to_system ( type_ , hints = {}) return self @classmethod def get_default ( cls ) -> Format : from arti.formats.json import JSON return JSON () # TODO: Support some sort of configurable defaults. Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.formats.json.JSON arti.formats.pickle.Pickle Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_default def get_default ( ) -> 'Format' View Source @classmethod def get_default ( cls ) -> Format : from arti.formats.json import JSON return JSON () # TODO: Support some sort of configurable defaults. parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Graph class Graph ( __pydantic_self__ , ** data : Any ) View Source class Graph ( Model ) : \"\"\"Graph stores a web of Artifacts connected by Producers.\"\"\" name : str artifacts : ArtifactBox = Field ( default_factory = lambda : ArtifactBox ( ** BOX_KWARGS [ SEALED ] )) # The Backend * itself * should not affect the results of a Graph build , though the contents # certainly may ( eg : stored annotations ), so we avoid serializing it . This also prevent # embedding any credentials . backend : Backend [ Connection ] = Field ( default_factory = _get_memory_backend , exclude = True ) path_tags : frozendict [ str, str ] = frozendict () # Graph starts off sealed , but is opened within a ` with Graph (...) ` context _status : Optional [ bool ] = PrivateAttr ( None ) _artifact_to_key : frozendict [ Artifact, str ] = PrivateAttr ( frozendict ()) @validator ( \"artifacts\" ) @classmethod def _convert_artifacts ( cls , artifacts : ArtifactBox ) -> ArtifactBox : return ArtifactBox ( artifacts , ** BOX_KWARGS [ SEALED ] ) def __enter__ ( self ) -> Graph : if arti . context . graph is not None : raise ValueError ( f \"Another graph is being defined: {arti.context.graph}\" ) arti . context . graph = self self . _toggle ( OPEN ) return self def __exit__ ( self , exc_type : Optional [ type[BaseException ] ] , exc_value : Optional [ BaseException ] , exc_traceback : Optional [ TracebackType ] , ) -> None : arti . context . graph = None self . _toggle ( SEALED ) # Confirm the dependencies are acyclic TopologicalSorter ( self . dependencies ). prepare () def _toggle ( self , status : bool ) -> None : # The Graph object is \"frozen\" , so we must bypass the assignment checks . object . __setattr__ ( self , \"artifacts\" , ArtifactBox ( self . artifacts , ** BOX_KWARGS [ status ] )) self . _status = status self . _artifact_to_key = frozendict ( { artifact : key for key , artifact in self . artifacts . walk () } ) @property def artifact_to_key ( self ) -> frozendict [ Artifact, str ] : return self . _artifact_to_key @requires_sealed def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : return self . snapshot (). build ( executor ) @requires_sealed def snapshot ( self , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Identify a \" unique \" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" return GraphSnapshot . from_graph ( self , connection = connection ) @cached_property @requires_sealed def dependencies ( self ) -> NodeDependencies : artifact_deps = { artifact : ( frozenset ( { artifact . producer_output . producer } ) if artifact . producer_output is not None else frozenset () ) for _ , artifact in self . artifacts . walk () } producer_deps = { # NOTE : multi - output Producers will appear multiple times ( but be deduped ) producer_output . producer : frozenset ( producer_output . producer . inputs . values ()) for artifact in artifact_deps if ( producer_output : = artifact . producer_output ) is not None } return NodeDependencies ( artifact_deps | producer_deps ) @cached_property @requires_sealed def producers ( self ) -> frozenset [ Producer ] : return frozenset ( self . producer_outputs ) @cached_property @requires_sealed def producer_outputs ( self ) -> frozendict [ Producer, tuple[Artifact, ... ] ]: d = defaultdict [ Producer, dict[int, Artifact ] ] ( dict ) for _ , artifact in self . artifacts . walk () : if artifact . producer_output is None : continue output = artifact . producer_output d [ output.producer ][ output.position ] = artifact return frozendict ( ( producer , tuple ( artifacts_by_position [ i ] for i in sorted ( artifacts_by_position ))) for producer , artifacts_by_position in d . items () ) # TODO : io . read / write probably need a bit of sanity checking ( probably somewhere else ), eg : type # ~= view . Doing validation on the data , etc . Should some of this live on the View ? @requires_sealed def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , snapshot : Optional [ GraphSnapshot ] = None , connection : Optional [ Connection ] = None , ) -> Any : key = self . artifact_to_key [ artifact ] if annotation is None and view is None : raise ValueError ( \"Either `annotation` or `view` must be passed\" ) if annotation is not None and view is not None : raise ValueError ( \"Only one of `annotation` or `view` may be passed\" ) if annotation is not None : view = View . get_class_for ( annotation )( artifact_class = type ( artifact ), type = artifact . type , mode = \"READ\" ) view . check_annotation_compatibility ( annotation ) view . check_artifact_compatibility ( artifact ) assert view is not None # mypy gets mixed up with ^ if storage_partitions is None : # We want to allow reading raw Artifacts even if other raw Artifacts are missing ( which # prevents snapshotting ). if snapshot is None and artifact . producer_output is None : # NOTE : We 're not using read_artifact_partitions as the underlying data may have # changed. The backend may have pointers to old versions (which is expected), but we # only want to return the current values. storage_partitions = artifact.storage.discover_partitions() else: snapshot = snapshot or self.snapshot() with (connection or self.backend).connect() as conn: storage_partitions = conn.read_snapshot_partitions(snapshot, key, artifact) return io.read( type_=artifact.type, format=artifact.format, storage_partitions=storage_partitions, view=view, ) @requires_sealed def write( self, data: Any, *, artifact: Artifact, input_fingerprint: Fingerprint = Fingerprint.empty(), keys: CompositeKey = CompositeKey(), view: Optional[View] = None, snapshot: Optional[GraphSnapshot] = None, connection: Optional[Connection] = None, ) -> StoragePartition: key = self.artifact_to_key[artifact] if snapshot is not None and artifact.producer_output is None: raise ValueError( f\"Writing to a raw Artifact (`{key}`) with a GraphSnapshot is not supported.\" ) if view is None: view = View.get_class_for(type(data))( artifact_class=type(artifact), type=artifact.type, mode=\"WRITE\" ) view.check_annotation_compatibility(type(data)) view.check_artifact_compatibility(artifact) storage_partition = artifact.storage.generate_partition( input_fingerprint=input_fingerprint, keys=keys, with_content_fingerprint=False ) storage_partition = io.write( data, type_=artifact.type, format=artifact.format, storage_partition=storage_partition, view=view, ).with_content_fingerprint() # TODO: Should we only do this in bulk? We might want the backends to transparently batch # requests, but that' s not so friendly with the transient \".connect\" . with ( connection or self . backend ). connect () as conn : conn . write_artifact_partitions ( artifact , ( storage_partition ,)) # Skip linking this partition to the snapshot if it affects raw Artifacts ( which would # trigger an id change ). if snapshot is not None and artifact . producer_output is not None : conn . write_snapshot_partitions ( snapshot , key , artifact , ( storage_partition ,)) return storage_partition Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables artifact_to_key fingerprint Methods build def build ( self , executor : 'Optional[Executor]' = None ) -> 'GraphSnapshot' View Source @requires_sealed def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : return self . snapshot (). build ( executor ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dependencies def dependencies ( ... ) dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . producer_outputs def producer_outputs ( ... ) producers def producers ( ... ) read def read ( self , artifact : 'Artifact' , * , annotation : 'Optional[Any]' = None , storage_partitions : 'Optional[Sequence[StoragePartition]]' = None , view : 'Optional[View]' = None , snapshot : 'Optional[GraphSnapshot]' = None , connection : 'Optional[Connection]' = None ) -> 'Any' View Source @requires_sealed def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , snapshot : Optional [ GraphSnapshot ] = None , connection : Optional [ Connection ] = None , ) -> Any : key = self . artifact_to_key [ artifact ] if annotation is None and view is None : raise ValueError ( \"Either `annotation` or `view` must be passed\" ) if annotation is not None and view is not None : raise ValueError ( \"Only one of `annotation` or `view` may be passed\" ) if annotation is not None : view = View . get_class_for ( annotation )( artifact_class = type ( artifact ), type = artifact . type , mode = \"READ\" ) view . check_annotation_compatibility ( annotation ) view . check_artifact_compatibility ( artifact ) assert view is not None # mypy gets mixed up with ^ if storage_partitions is None : # We want to allow reading raw Artifacts even if other raw Artifacts are missing ( which # prevents snapshotting ). if snapshot is None and artifact . producer_output is None : # NOTE : We ' re not using read_artifact_partitions as the underlying data may have # changed . The backend may have pointers to old versions ( which is expected ), but we # only want to return the current values . storage_partitions = artifact . storage . discover_partitions () else : snapshot = snapshot or self . snapshot () with ( connection or self . backend ). connect () as conn : storage_partitions = conn . read_snapshot_partitions ( snapshot , key , artifact ) return io . read ( type_ = artifact . type , format = artifact . format , storage_partitions = storage_partitions , view = view , ) snapshot def snapshot ( self , * , connection : 'Optional[Connection]' = None ) -> 'GraphSnapshot' Identify a \"unique\" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a snapshot of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. View Source @requires_sealed def snapshot ( self , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Identify a \" unique \" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" return GraphSnapshot . from_graph ( self , connection = connection ) write def write ( self , data : 'Any' , * , artifact : 'Artifact' , input_fingerprint : 'Fingerprint' = Fingerprint ( key = None ), keys : 'CompositeKey' = {}, view : 'Optional[View]' = None , snapshot : 'Optional[GraphSnapshot]' = None , connection : 'Optional[Connection]' = None ) -> 'StoragePartition' View Source @requires_sealed def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , snapshot : Optional [ GraphSnapshot ] = None , connection : Optional [ Connection ] = None , ) -> StoragePartition : key = self . artifact_to_key [ artifact ] if snapshot is not None and artifact . producer_output is None : raise ValueError ( f \"Writing to a raw Artifact (`{key}`) with a GraphSnapshot is not supported.\" ) if view is None : view = View . get_class_for ( type ( data ))( artifact_class = type ( artifact ), type = artifact . type , mode = \"WRITE\" ) view . check_annotation_compatibility ( type ( data )) view . check_artifact_compatibility ( artifact ) storage_partition = artifact . storage . generate_partition ( input_fingerprint = input_fingerprint , keys = keys , with_content_fingerprint = False ) storage_partition = io . write ( data , type_ = artifact . type , format = artifact . format , storage_partition = storage_partition , view = view , ). with_content_fingerprint () # TODO : Should we only do this in bulk ? We might want the backends to transparently batch # requests , but that ' s not so friendly with the transient \".connect\" . with ( connection or self . backend ). connect () as conn : conn . write_artifact_partitions ( artifact , ( storage_partition ,)) # Skip linking this partition to the snapshot if it affects raw Artifacts ( which would # trigger an id change ). if snapshot is not None and artifact . producer_output is not None : conn . write_snapshot_partitions ( snapshot , key , artifact , ( storage_partition ,)) return storage_partition GraphSnapshot class GraphSnapshot ( __pydantic_self__ , ** data : Any ) View Source class GraphSnapshot ( Model ) : \"\"\"GraphSnapshot represents the state of a Graph and the referenced raw data at a point in time. GraphSnapshot encodes the structure of the Graph plus a snapshot of the raw Artifact data (partition kinds and contents) at a point in time. Any change that would affect data should prompt an ID change. \"\"\" id : Fingerprint graph : Graph @property def artifacts ( self ) -> ArtifactBox : return self . graph . artifacts @property def backend ( self ) -> Backend [ Connection ] : return self . graph . backend @property def name ( self ) -> str : return self . graph . name @classmethod # TODO : Should this use a ( TTL ) cache ? Raw data changes ( especially in tests ) still need to be detected . def from_graph ( cls , graph : Graph , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Snapshot the Graph and all existing raw data. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifact data during Producer builds. \"\"\" # TODO : Resolve and statically set all available fingerprints . Specifically , we should pin # the Producer . fingerprint , which may by dynamic ( eg : version is a Timestamp ). Unbuilt # Artifact ( partitions ) won 't be fully resolved yet. snapshot_id, known_artifact_partitions = graph.fingerprint, dict[str, StoragePartitions]() for node, _ in graph.dependencies.items(): snapshot_id = snapshot_id.combine(node.fingerprint) if isinstance(node, Artifact): key = graph.artifact_to_key[node] snapshot_id = snapshot_id.combine(Fingerprint.from_string(key)) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we' ll have to handle things a bit # differently depending on if the external Artifacts are Produced ( in an upstream # Graph ) or not . if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . storage . discover_partitions () ) if not known_artifact_partitions [ key ] : content_str = \"partitions\" if is_partitioned ( node . type ) else \"data\" raise ValueError ( f \"No {content_str} found for `{key}`: {node}\" ) snapshot_id = snapshot_id . combine ( *[ partition.fingerprint for partition in known_artifact_partitions[key ] ] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma : no cover # NOTE : This shouldn ' t happen unless the logic above is faulty . raise ValueError ( \"Fingerprint is empty!\" ) snapshot = cls ( graph = graph , id = snapshot_id ) # Write the discovered partitions ( if not already known ) and link to this new snapshot . with ( connection or snapshot . backend ). connect () as conn : conn . write_graph ( graph ) conn . write_snapshot ( snapshot ) for key , partitions in known_artifact_partitions . items () : conn . write_artifact_and_graph_partitions ( snapshot , key , snapshot . artifacts [ key ] , partitions ) return snapshot def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : if executor is None : from arti . executors . local import LocalExecutor executor = LocalExecutor () executor . build ( self ) return self def tag ( self , tag : str , * , overwrite : bool = False , connection : Optional [ Connection ] = None ) -> None : with ( connection or self . backend ). connect () as conn : conn . write_snapshot_tag ( self , tag , overwrite ) @classmethod def from_tag ( cls , name : str , tag : str , * , connectable : Union [ Backend[Connection ] , Connection ] ) -> GraphSnapshot : with connectable . connect () as conn : return conn . read_snapshot_tag ( name , tag ) def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> Any : return self . graph . read ( artifact , annotation = annotation , storage_partitions = storage_partitions , view = view , snapshot = self , connection = connection , ) def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> StoragePartition : return self . graph . write ( data , artifact = artifact , input_fingerprint = input_fingerprint , keys = keys , view = view , snapshot = self , connection = connection , ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_graph def from_graph ( graph : 'Graph' , * , connection : 'Optional[Connection]' = None ) -> 'GraphSnapshot' Snapshot the Graph and all existing raw data. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifact data during Producer builds. View Source @classmethod # TODO : Should this use a ( TTL ) cache ? Raw data changes ( especially in tests ) still need to be detected . def from_graph ( cls , graph : Graph , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Snapshot the Graph and all existing raw data. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifact data during Producer builds. \"\"\" # TODO : Resolve and statically set all available fingerprints . Specifically , we should pin # the Producer . fingerprint , which may by dynamic ( eg : version is a Timestamp ). Unbuilt # Artifact ( partitions ) won 't be fully resolved yet. snapshot_id, known_artifact_partitions = graph.fingerprint, dict[str, StoragePartitions]() for node, _ in graph.dependencies.items(): snapshot_id = snapshot_id.combine(node.fingerprint) if isinstance(node, Artifact): key = graph.artifact_to_key[node] snapshot_id = snapshot_id.combine(Fingerprint.from_string(key)) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we' ll have to handle things a bit # differently depending on if the external Artifacts are Produced ( in an upstream # Graph ) or not . if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . storage . discover_partitions () ) if not known_artifact_partitions [ key ] : content_str = \"partitions\" if is_partitioned ( node . type ) else \"data\" raise ValueError ( f \"No {content_str} found for `{key}`: {node}\" ) snapshot_id = snapshot_id . combine ( *[ partition.fingerprint for partition in known_artifact_partitions[key ] ] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma : no cover # NOTE : This shouldn ' t happen unless the logic above is faulty . raise ValueError ( \"Fingerprint is empty!\" ) snapshot = cls ( graph = graph , id = snapshot_id ) # Write the discovered partitions ( if not already known ) and link to this new snapshot . with ( connection or snapshot . backend ). connect () as conn : conn . write_graph ( graph ) conn . write_snapshot ( snapshot ) for key , partitions in known_artifact_partitions . items () : conn . write_artifact_and_graph_partitions ( snapshot , key , snapshot . artifacts [ key ] , partitions ) return snapshot from_orm def from_orm ( obj : Any ) -> 'Model' from_tag def from_tag ( name : 'str' , tag : 'str' , * , connectable : 'Union[Backend[Connection], Connection]' ) -> 'GraphSnapshot' View Source @classmethod def from_tag ( cls , name : str , tag : str , * , connectable : Union [ Backend[Connection ] , Connection ] ) -> GraphSnapshot : with connectable . connect () as conn : return conn . read_snapshot_tag ( name , tag ) parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables artifacts backend fingerprint name Methods build def build ( self , executor : 'Optional[Executor]' = None ) -> 'GraphSnapshot' View Source def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : if executor is None : from arti.executors.local import LocalExecutor executor = LocalExecutor () executor . build ( self ) return self copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . read def read ( self , artifact : 'Artifact' , * , annotation : 'Optional[Any]' = None , storage_partitions : 'Optional[Sequence[StoragePartition]]' = None , view : 'Optional[View]' = None , connection : 'Optional[Connection]' = None ) -> 'Any' View Source def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> Any : return self . graph . read ( artifact , annotation = annotation , storage_partitions = storage_partitions , view = view , snapshot = self , connection = connection , ) tag def tag ( self , tag : 'str' , * , overwrite : 'bool' = False , connection : 'Optional[Connection]' = None ) -> 'None' View Source def tag ( self , tag : str , * , overwrite : bool = False , connection : Optional [ Connection ] = None ) -> None : with ( connection or self . backend ). connect () as conn : conn . write_snapshot_tag ( self , tag , overwrite ) write def write ( self , data : 'Any' , * , artifact : 'Artifact' , input_fingerprint : 'Fingerprint' = Fingerprint ( key = None ), keys : 'CompositeKey' = {}, view : 'Optional[View]' = None , connection : 'Optional[Connection]' = None ) -> 'StoragePartition' View Source def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> StoragePartition : return self . graph . write ( data , artifact = artifact , input_fingerprint = input_fingerprint , keys = keys , view = view , snapshot = self , connection = connection , ) PartitionKey class PartitionKey ( __pydantic_self__ , ** data : Any ) View Source class PartitionKey ( Model ) : _abstract_ = True _by_type_ : ClassVar [ dict[type[Type ] , type [ PartitionKey ] ]] = {} default_key_components : ClassVar [ frozendict[str, str ] ] matching_type : ClassVar [ type[Type ] ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return for attr in ( \"default_key_components\" , \"matching_type\" ) : if not hasattr ( cls , attr ) : raise TypeError ( f \"{cls.__name__} must set `{attr}`\" ) if unknown : = set ( cls . default_key_components ) - cls . key_components : raise TypeError ( f \"Unknown key_components in {cls.__name__}.default_key_components: {unknown}\" ) register ( cls . _by_type_ , cls . matching_type , cls ) @classproperty def key_components ( cls ) -> frozenset [ str ] : return frozenset ( cls . __fields__ ) | frozenset ( name for name in dir ( cls ) if isinstance ( getattr_static ( cls , name ), key_component ) ) @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : raise NotImplementedError ( f \"Unable to parse '{cls.__name__}' from: {key_components}\" ) @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ] @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.partitions.DateKey arti.partitions._IntKey arti.partitions.NullKey Class variables Config key_components Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_key_components def from_key_components ( ** key_components : 'str' ) -> 'PartitionKey' View Source @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : raise NotImplementedError ( f \"Unable to parse '{cls.__name__}' from: {key_components}\" ) from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( type_ : 'Type' ) -> 'type[PartitionKey]' View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' types_from def types_from ( type_ : 'Type' ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Producer class Producer ( __pydantic_self__ , ** data : Any ) View Source class Producer ( Model ) : \"\"\"A Producer is a task that builds one or more Artifacts.\"\"\" # User fields / methods annotations : tuple [ Annotation, ... ] = () version : Version = SemVer ( major = 0 , minor = 0 , patch = 1 ) # The map / build / validate_outputs parameters are intended to be dynamic and set by subclasses , # however mypy doesn 't like the \"incompatible\" signature on subclasses if actually defined here # (nor support ParamSpec yet). `map` is generated during subclassing if not set, `build` is # required, and `validate_outputs` defaults to no-op checks (hence is the only one with a # provided method). # # These must be @classmethods or @staticmethods. map: ClassVar[MapSig] build: ClassVar[BuildSig] if TYPE_CHECKING: validate_outputs: ClassVar[ValidateSig] else: @staticmethod def validate_outputs(*outputs: Any) -> Union[bool, tuple[bool, str]]: \"\"\"Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\"\" return True, \"No validation performed.\" # Internal fields/methods _abstract_: ClassVar[bool] = True _fingerprint_excludes_ = frozenset([\"annotations\"]) # NOTE: The following are set in __init_subclass__ _input_artifact_classes_: ClassVar[frozendict[str, type[Artifact]]] _build_inputs_: ClassVar[BuildInputs] _build_sig_: ClassVar[Signature] _map_inputs_: ClassVar[MapInputs] _map_sig_: ClassVar[Signature] _outputs_: ClassVar[Outputs] @classmethod def __init_subclass__(cls, **kwargs: Any) -> None: super().__init_subclass__(**kwargs) if not cls._abstract_: with wrap_exc(ValueError, prefix=cls.__name__): cls._input_artifact_classes_ = cls._validate_fields() with wrap_exc(ValueError, prefix=\".build\"): ( cls._build_sig_, cls._build_inputs_, cls._outputs_, ) = cls._validate_build_sig() with wrap_exc(ValueError, prefix=\".validate_output\"): cls._validate_validate_output_sig() with wrap_exc(ValueError, prefix=\".map\"): cls._map_sig_, cls._map_inputs_ = cls._validate_map_sig() cls._validate_no_unused_fields() @classmethod def _validate_fields(cls) -> frozendict[str, type[Artifact]]: # NOTE: Aside from the base producer fields, all others should (currently) be Artifacts. # # Users can set additional class attributes, but they must be properly hinted as ClassVars. # These won' t interact with the \"framework\" and can 't be parameters to build/map. artifact_fields = {k: v for k, v in cls.__fields__.items() if k not in Producer.__fields__} for name, field in artifact_fields.items(): with wrap_exc(ValueError, prefix=f\".{name}\"): if not (field.default is None and field.default_factory is None and field.required): raise ValueError(\"field must not have a default nor be Optional.\") if not lenient_issubclass(field.outer_type_, Artifact): raise ValueError( f\"type hint must be an Artifact subclass, got: {field.outer_type_}\" ) return frozendict({name: field.outer_type_ for name, field in artifact_fields.items()}) @classmethod def _validate_parameters( cls, sig: Signature, *, validator: Callable[[str, Parameter], _T] ) -> Iterator[_T]: if undefined_params := set(sig.parameters) - set(cls._input_artifact_classes_): raise ValueError( f\"the following parameter(s) must be defined as a field: {undefined_params}\" ) for name, param in sig.parameters.items(): with wrap_exc(ValueError, prefix=f\" {name} param\"): if param.annotation is param.empty: raise ValueError(\"must have a type hint.\") if param.default is not param.empty: raise ValueError(\"must not have a default.\") if param.kind not in (param.POSITIONAL_OR_KEYWORD, param.KEYWORD_ONLY): raise ValueError(\"must be usable as a keyword argument.\") yield validator(name, param) @classmethod def _validate_build_param(cls, name: str, param: Parameter) -> tuple[str, View]: annotation = param.annotation field_artifact_class = cls._input_artifact_classes_[param.name] # If there is no Artifact hint, add in the field value as the default. if get_item_from_annotated(annotation, Artifact, is_subclass=True) is None: annotation = Annotated[annotation, field_artifact_class] view = View.from_annotation(annotation, mode=\"READ\") if view.artifact_class != field_artifact_class: raise ValueError( f\"annotation Artifact class ({view.artifact_class}) does not match that set on the field ({field_artifact_class}).\" ) return name, view @classmethod def _validate_build_sig_return(cls, annotation: Any, *, i: int) -> View: with wrap_exc(ValueError, prefix=f\" {ordinal(i+1)} return\"): return View.from_annotation(annotation, mode=\"WRITE\") @classmethod def _validate_build_sig(cls) -> tuple[Signature, BuildInputs, Outputs]: \"\"\"Validate the .build method\"\"\" if not hasattr(cls, \"build\"): raise ValueError(\"must be implemented\") if not isinstance(getattr_static(cls, \"build\"), (classmethod, staticmethod)): raise ValueError(\"must be a @classmethod or @staticmethod\") build_sig = signature(cls.build, force_tuple_return=True, remove_owner=True) # Validate the parameters build_inputs = BuildInputs( cls._validate_parameters(build_sig, validator=cls._validate_build_param) ) # Validate the return definition return_annotation = build_sig.return_annotation if return_annotation is build_sig.empty: # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError(\"a return value must be set with the output Artifact(s).\") if return_annotation == (NoneType,): raise ValueError(\"missing return signature\") outputs = Outputs( cls._validate_build_sig_return(annotation, i=i) for i, annotation in enumerate(return_annotation) ) # Validate all outputs have equivalent partitioning schemes. # # We currently require the partition key type *and* name to match, but in the future we # might be able to extend the dependency metadata to support heterogeneous names if # necessary. seen_key_types = {PartitionKey.types_from(view.type) for view in outputs} if len(seen_key_types) != 1: raise ValueError(\"all outputs must have the same partitioning scheme\") return build_sig, build_inputs, outputs @classmethod def _validate_validate_output_sig(cls) -> None: build_output_hints = [ get_args(hint)[0] if get_origin(hint) is Annotated else hint for hint in cls._build_sig_.return_annotation ] match_build_str = f\"match the `.build` return (`{build_output_hints}`)\" validate_parameters = signature(cls.validate_outputs).parameters def param_matches(param: Parameter, build_return: type) -> bool: # Skip checking non-hinted parameters to allow lambdas. # # NOTE: Parameter type hints are *contravariant* (you can' t pass a \"Manager\" into a # function expecting an \"Employee\" ), hence the lenient_issubclass has build_return as # the subtype and param . annotation as the supertype . return param . annotation is param . empty or lenient_issubclass ( build_return , param . annotation ) if ( # Allow ` * args : Any ` or ` * args : T ` for ` build (...) -> tuple [ T, ... ] ` len ( validate_parameters ) == 1 and ( param : = tuple ( validate_parameters . values ()) [ 0 ] ). kind == param . VAR_POSITIONAL ) : if not all ( param_matches ( param , output_hint ) for output_hint in build_output_hints ) : with wrap_exc ( ValueError , prefix = f \" {param.name} param\" ) : raise ValueError ( f \"type hint must be `Any` or {match_build_str}\" ) else : # Otherwise , check pairwise if len ( validate_parameters ) != len ( build_output_hints ) : raise ValueError ( f \"must {match_build_str}\" ) for i , ( name , param ) in enumerate ( validate_parameters . items ()) : with wrap_exc ( ValueError , prefix = f \" {name} param\" ) : if param . default is not param . empty : raise ValueError ( \"must not have a default.\" ) if param . kind not in ( param . POSITIONAL_ONLY , param . POSITIONAL_OR_KEYWORD ) : raise ValueError ( \"must be usable as a positional argument.\" ) if not param_matches ( param , ( expected : = build_output_hints [ i ] )) : raise ValueError ( f \"type hint must match the {ordinal(i+1)} `.build` return (`{expected}`)\" ) # TODO : Validate return signature ? @classmethod def _validate_map_param ( cls , name : str , param : Parameter ) -> str : # TODO : Should we add some ArtifactPartition [ MyArtifact ] type ? if param . annotation != StoragePartitions : raise ValueError ( \"type hint must be `StoragePartitions`\" ) return name @classmethod def _validate_map_sig ( cls ) -> tuple [ Signature, MapInputs ] : \"\"\"Validate partitioned Artifacts and the .map method\"\"\" if not hasattr ( cls , \"map\" ) : # TODO : Add runtime checking of ` map ` output ( ie : output aligns w / output # artifacts and such ). if any ( is_partitioned ( view . type ) for view in cls . _outputs_ ) : raise ValueError ( \"must be implemented when the `build` outputs are partitioned\" ) def map ( ** kwargs : StoragePartitions ) -> PartitionDependencies : return PartitionDependencies ( { NotPartitioned : frozendict ( kwargs ) } ) # Narrow the map signature , which is validated below and used at graph build time ( via # cls . _map_inputs_ ) to determine what arguments to pass to map . map . __signature__ = Signature ( # type : ignore [ attr-defined ] [ Parameter(name=name, annotation=StoragePartitions, kind=Parameter.KEYWORD_ONLY) for name, artifact in cls._input_artifact_classes_.items() if name in cls._build_inputs_ ] , return_annotation = PartitionDependencies , ) cls . map = cast ( MapSig , staticmethod ( map )) if not isinstance ( getattr_static ( cls , \"map\" ), ( classmethod , staticmethod )) : raise ValueError ( \"must be a @classmethod or @staticmethod\" ) map_sig = signature ( cls . map ) map_inputs = MapInputs ( cls . _validate_parameters ( map_sig , validator = cls . _validate_map_param )) return map_sig , map_inputs # TODO : Verify map output hint matches TBD spec @classmethod def _validate_no_unused_fields ( cls ) -> None : if unused_fields : = set ( cls . _input_artifact_classes_ ) - ( set ( cls . _build_sig_ . parameters ) | set ( cls . _map_sig_ . parameters ) ) : raise ValueError ( f \"the following fields aren't used in `.build` or `.map`: {unused_fields}\" ) @validator ( \"*\" ) @classmethod def _validate_instance_artifact_args ( cls , value : Artifact , field : ModelField ) -> Artifact : if ( view : = cls . _build_inputs_ . get ( field . name )) is not None : view . check_artifact_compatibility ( value ) return value # NOTE : pydantic defines . __iter__ to return ` self . __dict__ . items () ` to support ` dict ( model ) ` , # but we want to override to support easy expansion / assignment to a Graph without ` . out () ` ( eg : # ` g . artifacts . a , g . artifacts . b = MyProducer (...) ` ). def __iter__ ( self ) -> Iterator [ Artifact ] : # type : ignore [ override ] ret = self . out () if not isinstance ( ret , tuple ) : ret = ( ret ,) return iter ( ret ) def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str, StoragePartitions ] ) -> Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_inputs_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected {expected_names}, got {input_names}\" ) # We only care if the * code * or * input partition contents * changed , not if the input file # paths changed ( but have the same content as a prior run ). return Fingerprint . from_string ( self . _class_key_ ). combine ( self . version . fingerprint , * ( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), ) def compute_dependencies ( self , input_partitions : InputPartitions ) -> tuple [ PartitionDependencies, InputFingerprints ] : # TODO : Validate the partition_dependencies against the Producer ' s partitioning scheme and # such ( basically , check user error ). eg : if output is not partitioned , we expect only 1 # entry in partition_dependencies ( NotPartitioned ). partition_dependencies = self . map ( ** { name : partitions for name , partitions in input_partitions . items () if name in self . _map_inputs_ } ) partition_input_fingerprints = InputFingerprints ( { composite_key : self . compute_input_fingerprint ( dependency_partitions ) for composite_key , dependency_partitions in partition_dependencies . items () } ) return partition_dependencies , partition_input_fingerprints @property def inputs ( self ) -> dict [ str, Artifact ] : return { k : getattr ( self , k ) for k in self . _input_artifact_classes_ } def out ( self , * outputs : Artifact ) -> Union [ Artifact, tuple[Artifact, ... ] ]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : outputs = tuple ( view . artifact_class ( type = view . type ) for view in self . _outputs_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = \", \" . join ( [ str(v) for v in self._build_sig_.return_annotation ] ) raise ValueError ( f \"{self._class_key_}.out() - expected {expected_n} arguments of ({ret_str}), but got: {outputs}\" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : view = self . _outputs_ [ ord ] with wrap_exc ( ValueError , prefix = f \"{self._class_key_}.out() {ordinal(ord+1)} argument\" ) : view . check_artifact_compatibility ( artifact ) if artifact . producer_output is not None : raise ValueError ( f \"{artifact} is produced by {artifact.producer_output.producer}!\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord ) } ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_outputs def validate_outputs ( * outputs : 'Any' ) -> 'Union[bool, tuple[bool, str]]' Validate the Producer.build outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of build will be passed in as it was returned, for example: def build(...): return 1, 2 will result in validate_outputs(1, 2) . NOTE: validate_outputs is a stopgap until Statistics and Thresholds are fully implemented. View Source @staticmethod def validate_outputs ( * outputs : Any ) -> Union [ bool , tuple [ bool , str ]] : \" \"\" Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\" \" return True , \"No validation performed.\" Instance variables fingerprint inputs Methods compute_dependencies def compute_dependencies ( self , input_partitions : 'InputPartitions' ) -> 'tuple[PartitionDependencies, InputFingerprints]' View Source def compute_dependencies ( self , input_partitions : InputPartitions ) -> tuple [ PartitionDependencies , InputFingerprints ] : # TODO : Validate the partition_dependencies against the Producer 's partitioning scheme and # such (basically, check user error). eg: if output is not partitioned, we expect only 1 # entry in partition_dependencies (NotPartitioned). partition_dependencies = self.map( **{ name: partitions for name, partitions in input_partitions.items() if name in self._map_inputs_ } ) partition_input_fingerprints = InputFingerprints( { composite_key: self.compute_input_fingerprint(dependency_partitions) for composite_key, dependency_partitions in partition_dependencies.items() } ) return partition_dependencies, partition_input_fingerprints compute_input_fingerprint def compute_input_fingerprint ( self , dependency_partitions : 'frozendict[str, StoragePartitions]' ) -> 'Fingerprint' View Source def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str , StoragePartitions ] ) - > Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_inputs_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected {expected_names}, got {input_names}\" ) # We only care if the * code * or * input partition contents * changed , not if the input file # paths changed ( but have the same content as a prior run ). return Fingerprint . from_string ( self . _class_key_ ) . combine ( self . version . fingerprint , *( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . out def out ( self , * outputs : 'Artifact' ) -> 'Union[Artifact, tuple[Artifact, ...]]' Configure the output Artifacts this Producer will build. The arguments are matched to the Producer.build return signature in order. View Source def out ( self , * outputs : Artifact ) -> Union [ Artifact, tuple[Artifact, ... ] ]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : outputs = tuple ( view . artifact_class ( type = view . type ) for view in self . _outputs_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = \", \" . join ( [ str(v) for v in self._build_sig_.return_annotation ] ) raise ValueError ( f \"{self._class_key_}.out() - expected {expected_n} arguments of ({ret_str}), but got: {outputs}\" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : view = self . _outputs_ [ ord ] with wrap_exc ( ValueError , prefix = f \"{self._class_key_}.out() {ordinal(ord+1)} argument\" ) : view . check_artifact_compatibility ( artifact ) if artifact . producer_output is not None : raise ValueError ( f \"{artifact} is produced by {artifact.producer_output.producer}!\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord ) } ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs Statistic class Statistic ( __pydantic_self__ , ** data : Any ) View Source class Statistic ( Model ): pass # TODO: Determine the interface for Statistics Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Storage class Storage ( __pydantic_self__ , ** data : Any ) View Source class Storage ( Model , Generic [ StoragePartitionVar_co ] ) : \"\"\"Storage is a data reference identifying 1 or more partitions of data. Storage fields should have defaults set with placeholders for tags and partition keys. This allows automatic injection of the tags and partition keys for simple cases. \"\"\" _abstract_ = True storage_partition_type : ClassVar [ type[StoragePartitionVar_co ] ] # type : ignore [ misc ] # These separators are used in the default resolve_ * helpers to format metadata into # the storage fields . # # The defaults are tailored for \"path\" - like fields . key_value_sep : ClassVar [ str ] = \"=\" partition_name_component_sep : ClassVar [ str ] = \"_\" segment_sep : ClassVar [ str ] = os . sep _key_types : Optional [ CompositeKeyTypes ] = PrivateAttr ( None ) @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return cls . storage_partition_type = get_class_type_vars ( cls ) [ 0 ] expected_field_types = { name : info . outer_type_ for name , info in cls . storage_partition_type . __fields__ . items () if name not in StoragePartition . __fields__ } fields = { name : info . outer_type_ for name , info in cls . __fields__ . items () if name not in Storage . __fields__ } if fields != expected_field_types : raise TypeError ( f \"{cls.__name__} fields must match {cls.storage_partition_type.__name__} ({expected_field_types}), got: {fields}\" ) @classmethod def get_default ( cls ) -> Storage [ StoragePartition ] : from arti . storage . literal import StringLiteral return StringLiteral () # TODO : Support some sort of configurable defaults . def _visit_type ( self , type_ : Type ) -> Self : # TODO : Check support for the types and partitioning on the specified field ( s ). copy = self . copy () copy . _key_types = PartitionKey . types_from ( type_ ) assert copy . key_types is not None key_component_specs = { f \"{name}{self.partition_name_component_sep}{component_name}\" : f \"{{{name}.{component_spec}}}\" for name , pk in copy . key_types . items () for component_name , component_spec in pk . default_key_components . items () } return copy . resolve ( partition_key_spec = self . segment_sep . join ( f \"{name}{self.key_value_sep}{spec}\" for name , spec in key_component_specs . items () ) ) def _visit_format ( self , format_ : Format ) -> Self : return self . resolve ( extension = format_ . extension ) def _visit_graph ( self , graph : Graph ) -> Self : return self . resolve ( graph_name = graph . name , path_tags = self . segment_sep . join ( f \"{tag}{self.key_value_sep}{value}\" for tag , value in graph . path_tags . items () ), ) def _visit_input_fingerprint ( self , input_fingerprint : Fingerprint ) -> Self : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" return self . resolve ( input_fingerprint = input_fingerprint_key ) def _visit_names ( self , names : tuple [ str, ... ] ) -> Self : return self . resolve ( name = names [ -1 ] if names else \"\" , names = self . segment_sep . join ( names )) @property def includes_input_fingerprint_template ( self ) -> bool : return any ( \"{input_fingerprint}\" in val for val in self . _format_fields . values ()) @property def key_types ( self ) -> CompositeKeyTypes : if self . _key_types is None : raise ValueError ( \"`key_types` have not been set yet.\" ) return self . _key_types @property def _format_fields ( self ) -> frozendict [ str, str ] : return frozendict ( { name : value for name in self . __fields__ if lenient_issubclass ( type ( value : = getattr ( self , name )), str ) } ) @classmethod def _check_keys ( cls , key_types : CompositeKeyTypes , keys : CompositeKey ) -> None : # TODO : Confirm the key names and types align if key_types and not keys : raise ValueError ( f \"Expected partition keys {tuple(key_types)} but none were passed\" ) if keys and not key_types : raise ValueError ( f \"Expected no partition keys but got: {keys}\" ) @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartitionVar_co, ... ] : raise NotImplementedError () def generate_partition ( self , keys : CompositeKey = CompositeKey (), input_fingerprint : Fingerprint = Fingerprint . empty (), with_content_fingerprint : bool = True , ) -> StoragePartitionVar_co : self . _check_keys ( self . key_types , keys ) format_kwargs = dict [ Any, Any ] ( keys ) if input_fingerprint . is_empty : if self . includes_input_fingerprint_template : raise ValueError ( f \"{self} requires an input_fingerprint, but none was provided\" ) else : if not self . includes_input_fingerprint_template : raise ValueError ( f \"{self} does not specify a {{input_fingerprint}} template\" ) format_kwargs [ \"input_fingerprint\" ] = str ( input_fingerprint . key ) field_values = { name : ( strip_partition_indexes ( original ). format ( ** format_kwargs ) if lenient_issubclass ( type ( original : = getattr ( self , name )), str ) else original ) for name in self . __fields__ if name in self . storage_partition_type . __fields__ } partition = self . storage_partition_type ( input_fingerprint = input_fingerprint , keys = keys , ** field_values ) if with_content_fingerprint : partition = partition . with_content_fingerprint () return partition def _resolve_field ( self , name : str , spec : str , placeholder_values : dict [ str, str ] ) -> str : for placeholder , value in placeholder_values . items () : if not value : # Strip placeholder * and * any trailing self . segment_sep . trim = \"{\" + placeholder + \"}\" if f \"{trim}{self.segment_sep}\" in spec : trim = f \"{trim}{self.segment_sep}\" # Also strip any trailing separators , eg : if the placeholder was at the end . spec = spec . replace ( trim , \"\" ). rstrip ( self . segment_sep ) if not spec : raise ValueError ( f \"{self}.{name} was empty after removing unused templates\" ) return partial_format ( spec , ** placeholder_values ) def resolve ( self , ** values : str ) -> Self : return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ). if ( new : = self . _resolve_field ( name , original , values )) != original } ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic Descendants arti.storage.google.cloud.storage.GCSFile arti.storage.local.LocalFile arti.storage.literal.StringLiteral Class variables Config key_value_sep partition_name_component_sep segment_sep Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_default def get_default ( ) -> 'Storage[StoragePartition]' View Source @classmethod def get_default ( cls ) -> Storage [ StoragePartition ]: from arti.storage.literal import StringLiteral return StringLiteral () # TODO: Support some sort of configurable defaults. parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint includes_input_fingerprint_template key_types Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. discover_partitions def discover_partitions ( self , input_fingerprints : 'InputFingerprints' = {} ) -> 'tuple[StoragePartitionVar_co, ...]' View Source @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartitionVar_co, ... ] : raise NotImplementedError () generate_partition def generate_partition ( self , keys : 'CompositeKey' = {}, input_fingerprint : 'Fingerprint' = Fingerprint ( key = None ), with_content_fingerprint : 'bool' = True ) -> 'StoragePartitionVar_co' View Source def generate_partition( self, keys: CompositeKey = CompositeKey(), input_fingerprint: Fingerprint = Fingerprint.empty(), with_content_fingerprint: bool = True, ) -> StoragePartitionVar_co: self._check_keys(self.key_types, keys) format_kwargs = dict[Any, Any](keys) if input_fingerprint.is_empty: if self.includes_input_fingerprint_template: raise ValueError(f\"{self} requires an input_fingerprint, but none was provided\") else: if not self.includes_input_fingerprint_template: raise ValueError(f\"{self} does not specify a {{ input_fingerprint }} template\") format_kwargs[\"input_fingerprint\"] = str(input_fingerprint.key) field_values = { name: ( strip_partition_indexes(original).format(**format_kwargs) if lenient_issubclass(type(original := getattr(self, name)), str) else original ) for name in self.__fields__ if name in self.storage_partition_type.__fields__ } partition = self.storage_partition_type( input_fingerprint=input_fingerprint, keys=keys, **field_values ) if with_content_fingerprint: partition = partition.with_content_fingerprint() return partition json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . resolve def resolve ( self , ** values : 'str' ) -> 'Self' View Source def resolve ( self , ** values : str ) -> Self : return self . copy ( update = { name : new for name , or iginal in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ) . if ( new := self . _resolve_field ( name , or iginal , values )) != or iginal } ) StoragePartition class StoragePartition ( __pydantic_self__ , ** data : Any ) View Source class StoragePartition ( Model ) : keys : CompositeKey = CompositeKey () input_fingerprint : Fingerprint = Fingerprint . empty () content_fingerprint : Fingerprint = Fingerprint . empty () def with_content_fingerprint ( self , keep_existing : bool = True ) -> Self : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint () } ) @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.storage.google.cloud.storage.GCSFilePartition arti.storage.local.LocalFilePartition arti.storage.literal.StringLiteralPartition Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods compute_content_fingerprint def compute_content_fingerprint ( self ) -> 'Fingerprint' View Source @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . with_content_fingerprint def with_content_fingerprint ( self , keep_existing : 'bool' = True ) -> 'Self' View Source def with_content_fingerprint ( self , keep_existing : bool = True ) -> Self : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()}) Threshold class Threshold ( __pydantic_self__ , ** data : Any ) View Source class Threshold ( Model ) : type : ClassVar [ type[Type ] ] def check ( self , value : Any ) -> bool : raise NotImplementedError () Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods check def check ( self , value : 'Any' ) -> 'bool' View Source def check ( self , value : Any ) -> bool : raise NotImplementedError () copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Type class Type ( __pydantic_self__ , ** data : Any ) View Source class Type ( Model ) : \"\"\"Type represents a data type.\"\"\" _abstract_ = True # NOTE : Exclude the description to minimize fingerprint changes ( and thus rebuilds ). _fingerprint_excludes_ = frozenset ( [ \"description\" ] ) description : Optional [ str ] nullable : bool = False @property def friendly_key ( self ) -> str : \"\"\"A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. \"\"\" return self . _class_key_ Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.types._Numeric arti.types.Binary arti.types.Boolean arti.types.Date arti.types.DateTime arti.types.Enum arti.types.Geography arti.types.List arti.types.Map arti.types.Null arti.types.Set arti.types.String arti.types.Struct arti.types.Time arti.types.Timestamp Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . TypeAdapter class TypeAdapter ( / , * args , ** kwargs ) View Source class TypeAdapter : \"\"\"TypeAdapter maps between Artigraph types and a foreign type system.\"\"\" key : ClassVar [ str ] = class_name () artigraph : ClassVar [ type[Type ] ] # The internal Artigraph Type system : ClassVar [ Any ] # The external system ' s type priority : ClassVar [ int ] = 0 # Set the priority of this mapping . Higher is better . @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : raise NotImplementedError () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : raise NotImplementedError () @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : raise NotImplementedError () Descendants arti.types._ScalarClassTypeAdapter arti.types.python.PyValueContainer arti.types.python.PyLiteral arti.types.python.PyMap arti.types.python.PyOptional arti.types.python.PyStruct arti.types.bigquery._BigQueryTypeAdapter arti.types.bigquery.ListFieldTypeAdapter arti.types.bigquery.TableTypeAdapter arti.types.numpy.ArrayAdapter arti.types.pandas.SeriesAdapter arti.types.pandas.DataFrameAdapter arti.types.pyarrow._PyarrowTypeAdapter arti.types.pydantic.BaseModelAdapter Class variables key priority Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : raise NotImplementedError () to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : raise NotImplementedError () to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : raise NotImplementedError () TypeSystem class TypeSystem ( __pydantic_self__ , ** data : Any ) View Source class TypeSystem ( Model ) : key : str extends : tuple [ TypeSystem, ... ] = () # NOTE : Use a NoCopyDict to avoid copies of the registry . Otherwise , TypeSystems that extend # this TypeSystem will only see the adapters registered * as of initialization * ( as pydantic # would deepcopy the TypeSystems in the ` extends ` argument ). _adapter_by_key : NoCopyDict [ str, type[TypeAdapter ] ] = PrivateAttr ( default_factory = NoCopyDict ) def register_adapter ( self , adapter : type [ TypeAdapter ] ) -> type [ TypeAdapter ] : return register ( self . _adapter_by_key , adapter . key , adapter ) @property def _priority_sorted_adapters ( self ) -> list [ type[TypeAdapter ] ]: return sorted ( self . _adapter_by_key . values (), key = attrgetter ( \"priority\" ), reverse = True ) def to_artigraph ( self , type_ : Any , * , hints : dict [ str, Any ] , root_type_system : Optional [ TypeSystem ] = None ) -> Type : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ) : return adapter . to_artigraph ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_artigraph ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No {root_type_system} adapter for system type: {type_}.\" ) def to_system ( self , type_ : Type , * , hints : dict [ str, Any ] , root_type_system : Optional [ TypeSystem ] = None ) -> Any : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ) : return adapter . to_system ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_system ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No {root_type_system} adapter for Artigraph type: {type_}.\" ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . register_adapter def register_adapter ( self , adapter : 'type[TypeAdapter]' ) -> 'type[TypeAdapter]' View Source def register_adapter ( self , adapter : type [ TypeAdapter ] ) -> type [ TypeAdapter ] : return register ( self . _adapter_by_key , adapter . key , adapter ) to_artigraph def to_artigraph ( self , type_ : 'Any' , * , hints : 'dict[str, Any]' , root_type_system : 'Optional[TypeSystem]' = None ) -> 'Type' View Source def to_artigraph ( self , type_ : Any , * , hints : dict [ str, Any ] , root_type_system : Optional [ TypeSystem ] = None ) -> Type : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ) : return adapter . to_artigraph ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_artigraph ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No {root_type_system} adapter for system type: {type_}.\" ) to_system def to_system ( self , type_ : 'Type' , * , hints : 'dict[str, Any]' , root_type_system : 'Optional[TypeSystem]' = None ) -> 'Any' View Source def to_system ( self , type_ : Type , * , hints : dict [ str, Any ] , root_type_system : Optional [ TypeSystem ] = None ) -> Any : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ) : return adapter . to_system ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_system ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No {root_type_system} adapter for Artigraph type: {type_}.\" ) Version class Version ( __pydantic_self__ , ** data : Any ) View Source class Version ( Model ): _abstract_ = True Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.versions.GitCommit arti.versions.SemVer arti.versions.String arti.versions.Timestamp Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . View class View ( __pydantic_self__ , ** data : Any ) View Source class View ( Model ) : \"\"\"View represents the in-memory representation of the artifact. Examples include pandas.DataFrame, dask.DataFrame, a BigQuery table. \"\"\" _abstract_ = True _by_python_type_ : ClassVar [ dict[Optional[type ] , type [ View ] ]] = {} priority : ClassVar [ int ] = 0 # Set priority of this view for its python_type . Higher is better . python_type : ClassVar [ Optional[type ] ] type_system : ClassVar [ TypeSystem ] mode : MODE artifact_class : type [ Artifact ] = Artifact type : Type @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if not cls . _abstract_ : register ( cls . _by_python_type_ , cls . python_type , cls , lambda x : x . priority ) @classmethod def _check_type_compatibility ( cls , view_type : Type , artifact_type : Type ) -> None : # TODO : Consider supporting some form of \"reader schema\" where the View 's Type is a subset # of the Artifact' s Type ( and we filter the columns on read ). We could also consider # allowing the Producer 's Type to be a superset of the Artifact' s Type and we 'd filter the # columns on write. # # If implementing, we can leverage the `mode` to determine which should be the \"superset\". if view_type != artifact_type: raise ValueError( f\"the specified Type (`{view_type}`) is not compatible with the Artifact' s Type ( `{ artifact_type }` ). \" ) @validator(\" type \") @classmethod def _validate_type(cls, type_: Type, values: dict[str, Any]) -> Type: artifact_class: Optional[type[Artifact]] = values.get(\" artifact_class \") if artifact_class is None: return type_ # pragma: no cover artifact_type: Optional[Type] = get_field_default(artifact_class, \" type \") if artifact_type is not None: cls._check_type_compatibility(view_type=type_, artifact_type=artifact_type) return type_ @classmethod def _get_kwargs_from_annotation(cls, annotation: Any) -> dict[str, Any]: artifact_class = get_item_from_annotated( annotation, Artifact, is_subclass=True ) or get_field_default(cls, \" artifact_class \") assert artifact_class is not None assert issubclass(artifact_class, Artifact) # Try to extract or infer the Type. We prefer: an explicit Type in the annotation, followed # by an Artifact's default type, falling back to inferring a Type from the type hint. type_ = get_item_from_annotated(annotation, Type, is_subclass=False) if type_ is None: artifact_type: Optional[Type] = get_field_default(artifact_class, \" type \") if artifact_type is None: from arti.types.python import python_type_system type_ = python_type_system.to_artigraph(discard_Annotated(annotation), hints={}) else: type_ = artifact_type # NOTE: We validate that type_ and artifact_type (if set) are compatible in _validate_type, # which will run for *any* instance, not just those created with `.from_annotation`. return {\" artifact_class \": artifact_class, \" type \": type_} @classmethod # TODO: Use typing.Self for return, pending mypy support def get_class_for(cls, annotation: Any) -> builtins.type[View]: view_class = get_item_from_annotated(annotation, cls, is_subclass=True) if view_class is None: # We've already searched for a View instance in the original Annotated args, so just # extract the root annotation. annotation = discard_Annotated(annotation) # Import the View submodules to trigger registration. import_submodules(__path__, __name__) view_class = cls._by_python_type_.get(annotation) # If no match and the type is a subscripted Generic (eg: `list[int]`), try to unwrap any # extra type variables. if view_class is None and (origin := get_origin(annotation)) is not None: view_class = cls._by_python_type_.get(origin) if view_class is None: raise ValueError( f\" { annotation } cannot be matched to a View , try setting one explicitly ( eg : ` Annotated [ int, arti.views.python.Int ] ` ) \" ) return view_class @classmethod # TODO: Use typing.Self for return, pending mypy support def from_annotation(cls, annotation: Any, *, mode: MODE) -> View: view_class = cls.get_class_for(annotation) view = view_class(mode=mode, **cls._get_kwargs_from_annotation(annotation)) view.check_annotation_compatibility(annotation) return view def check_annotation_compatibility(self, annotation: Any) -> None: # We're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \") def check_artifact_compatibility(self, artifact: Artifact) -> None: if not isinstance(artifact, self.artifact_class): raise ValueError(f\" expected an instance of { self . artifact_class } , got { type ( artifact ) } \") self._check_type_compatibility(view_type=self.type, artifact_type=artifact.type) if self.mode in {\" READ \", \" READWRITE \"}: io._read.lookup( type(artifact.type), type(artifact.format), list[artifact.storage.storage_partition_type], # type: ignore[name-defined] type(self), ) if self.mode in {\" WRITE \", \" READWRITE \"}: io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.views.python.PythonBuiltin Class variables Config priority Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_annotation def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods check_annotation_compatibility def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \") check_artifact_compatibility def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Index"},{"location":"reference/arti/#module-arti","text":"None None View Source from __future__ import annotations import importlib.metadata __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) __version__ = importlib . metadata . version ( \"arti\" ) import threading from typing import Optional from arti.annotations import Annotation from arti.artifacts import Artifact from arti.backends import Backend , Connection from arti.executors import Executor from arti.fingerprints import Fingerprint from arti.formats import Format from arti.graphs import Graph , GraphSnapshot from arti.io import read , register_reader , register_writer , write from arti.partitions import CompositeKey , CompositeKeyTypes , InputFingerprints , PartitionKey from arti.producers import PartitionDependencies , Producer , producer from arti.statistics import Statistic from arti.storage import Storage , StoragePartition , StoragePartitions from arti.thresholds import Threshold from arti.types import Type , TypeAdapter , TypeSystem from arti.versions import Version from arti.views import View # Export all interfaces. __all__ = [ \"Annotation\" , \"Artifact\" , \"Backend\" , \"CompositeKey\" , \"CompositeKeyTypes\" , \"Connection\" , \"Executor\" , \"Fingerprint\" , \"Format\" , \"Graph\" , \"GraphSnapshot\" , \"InputFingerprints\" , \"PartitionDependencies\" , \"PartitionKey\" , \"Producer\" , \"Statistic\" , \"Storage\" , \"StoragePartition\" , \"StoragePartitions\" , \"Threshold\" , \"Type\" , \"TypeAdapter\" , \"TypeSystem\" , \"Version\" , \"View\" , \"producer\" , \"read\" , \"register_reader\" , \"register_writer\" , \"write\" , ] class _Context ( threading . local ): def __init__ ( self ) -> None : super () . __init__ () self . graph : Optional [ Graph ] = None context = _Context ()","title":"Module arti"},{"location":"reference/arti/#sub-modules","text":"arti.annotations arti.artifacts arti.backends arti.executors arti.fingerprints arti.formats arti.graphs arti.internal arti.io arti.partitions arti.producers arti.statistics arti.storage arti.thresholds arti.types arti.versions arti.views","title":"Sub-modules"},{"location":"reference/arti/#variables","text":"CompositeKey CompositeKeyTypes InputFingerprints PartitionDependencies StoragePartitions","title":"Variables"},{"location":"reference/arti/#functions","text":"","title":"Functions"},{"location":"reference/arti/#producer","text":"def producer ( * , annotations : 'Optional[tuple[Annotation, ...]]' = None , map : 'Optional[MapSig]' = None , name : 'Optional[str]' = None , validate_outputs : 'Optional[ValidateSig]' = None , version : 'Optional[Version]' = None ) -> 'Callable[[BuildSig], type[Producer]]' View Source def producer ( * , annotations : Optional [ tuple[Annotation, ... ] ] = None , map : Optional [ MapSig ] = None , name : Optional [ str ] = None , validate_outputs : Optional [ ValidateSig ] = None , version : Optional [ Version ] = None , ) -> Callable [ [BuildSig ] , type [ Producer ] ]: def decorate ( build : BuildSig ) -> type [ Producer ] : nonlocal name name = build . __name__ if name is None else name __annotations__ : dict [ str, Any ] = {} for param in signature ( build ). parameters . values () : with wrap_exc ( ValueError , prefix = f \"{name} {param.name} param\" ) : view = View . from_annotation ( param . annotation , mode = \"READ\" ) __annotations__ [ param.name ] = view . artifact_class # If overriding , set an explicit \"annotations\" hint until [ 1 ] is released . # # 1 : https : // github . com / samuelcolvin / pydantic / pull / 3018 if annotations : __annotations__ [ \"annotations\" ] = tuple [ Annotation, ... ] if version : __annotations__ [ \"version\" ] = Version return type ( name , ( Producer ,), { k : v for k , v in { \"__annotations__\" : __annotations__ , \"__module__\" : get_module_name ( depth = 2 ), # Not our module , but our caller ' s . \"annotations\" : annotations , \"build\" : staticmethod ( build ), \"map\" : None if map is None else staticmethod ( map ), \"validate_outputs\" : ( None if validate_outputs is None else staticmethod ( validate_outputs ) ), \"version\" : version , } . items () if v is not None } , ) return decorate","title":"producer"},{"location":"reference/arti/#read","text":"def read ( type_ : 'Type' , format : 'Format' , storage_partitions : 'Sequence[StoragePartition]' , view : 'View' ) -> 'Any' View Source def read ( type_ : Type , format : Format , storage_partitions : Sequence [ StoragePartition ] , view : View ) -> Any : if not storage_partitions : # NOTE : Aside from simplifying this check up front , multiple dispatch with unknown list # element type can be ambiguous / error . raise FileNotFoundError ( \"No data\" ) if len ( storage_partitions ) > 1 and not is_partitioned ( type_ ) : raise ValueError ( f \"Multiple partitions can only be read into a partitioned Collection, not {type_}\" ) # TODO Checks that the returned data matches the Type / View # # Likely add a View method that can handle this type + schema checking , filtering to column / row subsets if necessary , etc return _read ( type_ , format , storage_partitions , view )","title":"read"},{"location":"reference/arti/#register_reader","text":"def register_reader ( * args : 'Any' ) -> 'Callable[[REGISTERED], REGISTERED]' Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return cast ( Callable [ ..., Any ] , super (). register ( * args ))","title":"register_reader"},{"location":"reference/arti/#register_writer","text":"def register_writer ( * args : 'Any' ) -> 'Callable[[REGISTERED], REGISTERED]' Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return cast ( Callable [ ..., Any ] , super (). register ( * args ))","title":"register_writer"},{"location":"reference/arti/#write","text":"def write ( data : 'Any' , type_ : 'Type' , format : 'Format' , storage_partition : 'StoragePartitionVar' , view : 'View' ) -> 'StoragePartitionVar' View Source def write ( data : Any , type_ : Type , for mat : Format , storage_partition : StoragePartitionVar , view : View ) -> StoragePartitionVar : if ( updated := _write ( data , type_ , for mat , storage_partition , view )) is not None : return updated return storage_partition","title":"write"},{"location":"reference/arti/#classes","text":"","title":"Classes"},{"location":"reference/arti/#annotation","text":"class Annotation ( __pydantic_self__ , ** data : Any ) View Source class Annotation ( Model ): \"\"\"An Annotation is a piece of human knowledge associated with an Artifact.\"\"\" _abstract_ = True","title":"Annotation"},{"location":"reference/arti/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods","text":"","title":"Methods"},{"location":"reference/arti/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#artifact","text":"class Artifact ( __pydantic_self__ , ** data : Any ) View Source class Artifact ( Model ) : \" \"\" An Artifact is the base structure describing an existing or generated dataset. An Artifact is comprised of three key elements: - `type`: spec of the data's structure, such as data types, nullable, etc. - `format`: the data's serialized format, such as CSV, Parquet, database native, etc. - `storage`: the data's persistent storage system, such as blob storage, database native, etc. In addition to the core elements, an Artifact can be tagged with additional `annotations` (to associate it with human knowledge) and `statistics` (to track derived characteristics over time). \"\" \" type : Type format : Format = Field ( default_factory = Format . get_default ) storage : Storage [ StoragePartition ] = Field ( default_factory = Storage . get_default ) annotations : tuple [ Annotation , ... ] = () statistics : tuple [ Statistic , ... ] = () # Hide `producer_output` in repr to prevent showing the entire upstream graph. # # ProducerOutput is a ForwardRef/cyclic import. Quote the entire hint to force full resolution # during `.update_forward_refs`, rather than `Optional[ForwardRef(\"ProducerOutput\")]`. producer_output : Optional [ ProducerOutput ] = Field ( None , repr = False ) # NOTE: Narrow the fields that affect the fingerprint to minimize changes (which trigger # recompute). Importantly, avoid fingerprinting the `.producer_output` (ie: the *upstream* # producer) to prevent cascading fingerprint changes (Producer.fingerprint accesses the *input* # Artifact.fingerprints). Even so, this may still be quite sensitive. _fingerprint_includes_ = frozenset ( [ \"type\" , \"format\" , \"storage\" ] ) @validator ( \"format\" , always = True ) @classmethod def _validate_format ( cls , format : Format , values : dict [ str , Any ] ) -> Format : if ( type_ := values . get ( \"type\" )) is not None : return format . _visit_type ( type_ ) return format @validator ( \"storage\" , always = True ) @classmethod def _validate_storage ( cls , storage : Storage [ StoragePartition ] , values : dict [ str , Any ] ) -> Storage [ StoragePartition ] : if ( type_ := values . get ( \"type\" )) is not None : storage = storage . _visit_type ( type_ ) if ( format_ := values . get ( \"format\" )) is not None : storage = storage . _visit_format ( format_ ) return storage @validator ( \"annotations\" , \"statistics\" , always = True , pre = True ) @classmethod def _merge_class_defaults ( cls , value : tuple [ Any , ... ] , field : ModelField ) -> tuple [ Any , ... ] : return tuple ( chain ( get_field_default ( cls , field . name ) or (), value )) @classmethod def cast ( cls , value : Any ) -> Artifact : \" \"\" Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `ArtifactBox` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, we attempt to map to a `Type` and return an Artifact instance with defaulted Format and Storage \"\" \" from arti . formats . json import JSON from arti . producers import Producer from arti . storage . literal import StringLiteral from arti . types . python import python_type_system if isinstance ( value , Artifact ) : return value if isinstance ( value , Producer ) : output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ) : return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma: no cover # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( f \"{type(value).__name__} doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \"{type(value).__name__} produces {len(output_artifacts)} Artifacts. Try assigning each to a new name in the Graph!\" ) annotation = get_annotation_from_value ( value ) return cls ( type = python_type_system . to_artigraph ( annotation , hints = {} ), format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), )","title":"Artifact"},{"location":"reference/arti/#ancestors-in-mro_1","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/#cast","text":"def cast ( value : 'Any' ) -> 'Artifact' Attempt to convert an arbitrary value to an appropriate Artifact instance. Artifact.cast is used to convert values assigned to an ArtifactBox (such as Graph.artifacts ) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, we attempt to map to a Type and return an Artifact instance with defaulted Format and Storage View Source @classmethod def cast ( cls , value : Any ) -> Artifact : \"\"\"Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `ArtifactBox` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, we attempt to map to a `Type` and return an Artifact instance with defaulted Format and Storage \"\"\" from arti.formats.json import JSON from arti.producers import Producer from arti.storage.literal import StringLiteral from arti.types.python import python_type_system if isinstance ( value , Artifact ): return value if isinstance ( value , Producer ): output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ): return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma: no cover # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( f \" { type ( value ) . __name__ } doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \" { type ( value ) . __name__ } produces { len ( output_artifacts ) } Artifacts. Try assigning each to a new name in the Graph!\" ) annotation = get_annotation_from_value ( value ) return cls ( type = python_type_system . to_artigraph ( annotation , hints = {}), format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), )","title":"cast"},{"location":"reference/arti/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_1","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/#copy_1","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#backend","text":"class Backend ( __pydantic_self__ , ** data : Any ) View Source class Backend ( Model , Generic [ ConnectionVar ] ) : \"\"\"Backend represents a storage for internal Artigraph metadata. Backend storage is an addressable location (local path, database connection, etc) that tracks metadata for a collection of Graphs over time, including: - the Artifact(s)->Producer->Artifact(s) dependency graph - Artifact Annotations, Statistics, Partitions, and other metadata - Artifact and Producer Fingerprints - etc \"\"\" @contextmanager @abstractmethod def connect ( self ) -> Iterator [ ConnectionVar ] : raise NotImplementedError ()","title":"Backend"},{"location":"reference/arti/#ancestors-in-mro_2","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants","text":"arti.backends.memory.MemoryBackend","title":"Descendants"},{"location":"reference/arti/#class-variables_2","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/arti/#construct_2","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_2","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_2","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_2","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_2","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_2","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_2","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_2","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_2","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_2","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/#connect","text":"def connect ( self ) -> 'Iterator[ConnectionVar]' View Source @contextmanager @abstractmethod def connect ( self ) -> Iterator [ ConnectionVar ] : raise NotImplementedError ()","title":"connect"},{"location":"reference/arti/#copy_2","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_2","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_2","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#connection","text":"class Connection ( / , * args , ** kwargs ) View Source class Connection : \" \"\" Connection is a wrapper around an active connection to a Backend resource. For example, a Backend connecting to a database might wrap up a SQLAlchemy connection in a Connection subclass implementing the required methods. \"\" \" # Artifact partitions - independent of a specific GraphSnapshot @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \" \"\" Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a GraphSnapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \" \"\" Add more partitions for a Storage spec. \"\" \" raise NotImplementedError () # Graph @abstractmethod def read_graph ( self , name : str , fingerprint : Fingerprint ) -> Graph : \" \"\" Fetch an instance of the named Graph. \"\" \" raise NotImplementedError () @abstractmethod def write_graph ( self , graph : Graph ) -> None : \" \"\" Write the Graph and all linked Artifacts and Producers to the database. \"\" \" raise NotImplementedError () # GraphSnapshot @abstractmethod def read_snapshot ( self , name : str , fingerprint : Fingerprint ) -> GraphSnapshot : \" \"\" Fetch an instance of the named GraphSnapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_snapshot ( self , snapshot : GraphSnapshot ) -> None : \" \"\" Write the GraphSnapshot to the database. \"\" \" raise NotImplementedError () @abstractmethod def read_snapshot_tag ( self , name : str , tag : str ) -> GraphSnapshot : \" \"\" Fetch the GraphSnapshot for the named tag. \"\" \" raise NotImplementedError () @abstractmethod def write_snapshot_tag ( self , snapshot : GraphSnapshot , tag : str , overwrite : bool = False ) -> None : \" \"\" Stamp a GraphSnapshot with an arbitrary tag. \"\" \" raise NotImplementedError () @abstractmethod def read_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \" \"\" Read the known Partitions for the named Artifact in a specific GraphSnapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \" \"\" Link the Partitions to the named Artifact in a specific GraphSnapshot. \"\" \" raise NotImplementedError () # Helpers def write_artifact_and_graph_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_snapshot_partitions ( snapshot , artifact_key , artifact , partitions ) @contextmanager def connect ( self ) -> Iterator [ Self ] : \" \"\" Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... \"\" \" yield self @classmethod def __get_validators__ ( cls ) -> list [ Callable [[ Any , ModelField ] , Any ]] : \" \"\" Return an empty list of \" validators \". Allows using a Connection (which is not a model) as a field in other models without setting `arbitrary_types_allowed` (which applies broadly). [1]. 1: https://docs.pydantic.dev/usage/types/#generic-classes-as-types \"\" \" return []","title":"Connection"},{"location":"reference/arti/#descendants_1","text":"arti.backends.memory.MemoryConnection","title":"Descendants"},{"location":"reference/arti/#methods_3","text":"","title":"Methods"},{"location":"reference/arti/#connect_1","text":"def connect ( self ) -> 'Iterator[Self]' Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... View Source @contextmanager def connect ( self ) -> Iterator [ Self ] : \"\"\"Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... \"\"\" yield self","title":"connect"},{"location":"reference/arti/#read_artifact_partitions","text":"def read_artifact_partitions ( self , artifact : 'Artifact' , input_fingerprints : 'InputFingerprints' = {} ) -> 'StoragePartitions' Read all known Partitions for this Storage spec. If input_fingerprints is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless input_fingerprints is provided matching those for a GraphSnapshot. View Source @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \" \"\" Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a GraphSnapshot. \"\" \" raise NotImplementedError ()","title":"read_artifact_partitions"},{"location":"reference/arti/#read_graph","text":"def read_graph ( self , name : 'str' , fingerprint : 'Fingerprint' ) -> 'Graph' Fetch an instance of the named Graph. View Source @abstractmethod def read_graph ( self , name : str , fingerprint : Fingerprint ) -> Graph : \"\"\"Fetch an instance of the named Graph.\"\"\" raise NotImplementedError ()","title":"read_graph"},{"location":"reference/arti/#read_snapshot","text":"def read_snapshot ( self , name : 'str' , fingerprint : 'Fingerprint' ) -> 'GraphSnapshot' Fetch an instance of the named GraphSnapshot. View Source @abstractmethod def read_snapshot ( self , name : str , fingerprint : Fingerprint ) -> GraphSnapshot : \"\"\"Fetch an instance of the named GraphSnapshot.\"\"\" raise NotImplementedError ()","title":"read_snapshot"},{"location":"reference/arti/#read_snapshot_partitions","text":"def read_snapshot_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' ) -> 'StoragePartitions' Read the known Partitions for the named Artifact in a specific GraphSnapshot. View Source @abstractmethod def read_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \"\"\"Read the known Partitions for the named Artifact in a specific GraphSnapshot.\"\"\" raise NotImplementedError ()","title":"read_snapshot_partitions"},{"location":"reference/arti/#read_snapshot_tag","text":"def read_snapshot_tag ( self , name : 'str' , tag : 'str' ) -> 'GraphSnapshot' Fetch the GraphSnapshot for the named tag. View Source @abstractmethod def read_snapshot_tag ( self , name : str , tag : str ) -> GraphSnapshot : \"\"\"Fetch the GraphSnapshot for the named tag.\"\"\" raise NotImplementedError ()","title":"read_snapshot_tag"},{"location":"reference/arti/#write_artifact_and_graph_partitions","text":"def write_artifact_and_graph_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' View Source def write_artifact_and_graph_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_snapshot_partitions ( snapshot , artifact_key , artifact , partitions )","title":"write_artifact_and_graph_partitions"},{"location":"reference/arti/#write_artifact_partitions","text":"def write_artifact_partitions ( self , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' Add more partitions for a Storage spec. View Source @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \"\"\"Add more partitions for a Storage spec.\"\"\" raise NotImplementedError ()","title":"write_artifact_partitions"},{"location":"reference/arti/#write_graph","text":"def write_graph ( self , graph : 'Graph' ) -> 'None' Write the Graph and all linked Artifacts and Producers to the database. View Source @abstractmethod def write_graph ( self , graph : Graph ) -> None : \"\"\"Write the Graph and all linked Artifacts and Producers to the database.\"\"\" raise NotImplementedError ()","title":"write_graph"},{"location":"reference/arti/#write_snapshot","text":"def write_snapshot ( self , snapshot : 'GraphSnapshot' ) -> 'None' Write the GraphSnapshot to the database. View Source @abstractmethod def write_snapshot ( self , snapshot : GraphSnapshot ) -> None : \"\"\"Write the GraphSnapshot to the database.\"\"\" raise NotImplementedError ()","title":"write_snapshot"},{"location":"reference/arti/#write_snapshot_partitions","text":"def write_snapshot_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' Link the Partitions to the named Artifact in a specific GraphSnapshot. View Source @abstractmethod def write_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \"\"\"Link the Partitions to the named Artifact in a specific GraphSnapshot.\"\"\" raise NotImplementedError ()","title":"write_snapshot_partitions"},{"location":"reference/arti/#write_snapshot_tag","text":"def write_snapshot_tag ( self , snapshot : 'GraphSnapshot' , tag : 'str' , overwrite : 'bool' = False ) -> 'None' Stamp a GraphSnapshot with an arbitrary tag. View Source @abstractmethod def write_snapshot_tag ( self , snapshot : GraphSnapshot , tag : str , overwrite : bool = False ) -> None : \"\"\"Stamp a GraphSnapshot with an arbitrary tag.\"\"\" raise NotImplementedError ()","title":"write_snapshot_tag"},{"location":"reference/arti/#executor","text":"class Executor ( __pydantic_self__ , ** data : Any ) View Source class Executor ( Model ) : @abc . abstractmethod def build ( self , snapshot : GraphSnapshot ) -> None : raise NotImplementedError () def get_producer_inputs ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer ) -> InputPartitions : return InputPartitions ( { name : connection . read_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact ) for name , artifact in producer . inputs . items () } ) def discover_producer_partitions ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , partition_input_fingerprints : InputFingerprints , ) -> set [ CompositeKey ] : # NOTE : The output partitions may be built , but not yet associated with this GraphSnapshot # ( eg : raw input data changed , but no changes trickled into this specific Producer ). Hence # we 'll fetch all StoragePartitions for each Storage, filtered to the PKs and # input_fingerprints we' ve computed * are * for this snapshot - and then link them to the # snapshot . existing_output_partitions = { output : connection . read_artifact_partitions ( output , partition_input_fingerprints ) for output in snapshot . graph . producer_outputs [ producer ] } for artifact , partitions in existing_output_partitions . items () : connection . write_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact , partitions ) # TODO : Guarantee all outputs have the same set of identified partitions . Currently , this # pretends a partition is built for all outputs if _any_ are built for that partition . return { partition . keys for partition in chain . from_iterable ( existing_output_partitions . values ()) } def build_producer_partition ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , existing_partition_keys : set [ CompositeKey ] , input_fingerprint : Fingerprint , partition_dependencies : frozendict [ str, StoragePartitions ] , partition_key : CompositeKey , ) -> None : # TODO : Should this \"skip if exists\" live here or higher up ? if partition_key in existing_partition_keys : pk_str = f \" for: {dict(partition_key)}\" if partition_key else \".\" logging . info ( f \"Skipping existing {type(producer).__name__} output{pk_str}\" ) return logging . info ( f \"Building {producer} output for {partition_key}...\" ) # TODO : Catch DispatchError and give a nicer error ... maybe add this to our # @dispatch wrapper ( eg : msg arg , or even fn that returns the message to # raise ). arguments = { name : snapshot . read ( artifact = producer . inputs [ name ] , storage_partitions = partition_dependencies [ name ] , view = view , ) for name , view in producer . _build_inputs_ . items () } outputs = producer . build ( ** arguments ) if len ( producer . _outputs_ ) == 1 : outputs = ( outputs ,) validation_passed , validation_message = producer . validate_outputs ( * outputs ) if not validation_passed : raise ValueError ( validation_message ) for i , output in enumerate ( outputs ) : snapshot . write ( output , artifact = snapshot . graph . producer_outputs [ producer ][ i ] , input_fingerprint = input_fingerprint , keys = partition_key , view = producer . _outputs_ [ i ] , )","title":"Executor"},{"location":"reference/arti/#ancestors-in-mro_3","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants_2","text":"arti.executors.local.LocalExecutor","title":"Descendants"},{"location":"reference/arti/#class-variables_3","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/arti/#construct_3","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_3","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_3","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_3","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_3","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_3","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_3","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_3","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_3","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_3","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_4","text":"","title":"Methods"},{"location":"reference/arti/#build","text":"def build ( self , snapshot : 'GraphSnapshot' ) -> 'None' View Source @abc . abstractmethod def build ( self , snapshot : GraphSnapshot ) -> None : raise NotImplementedError ()","title":"build"},{"location":"reference/arti/#build_producer_partition","text":"def build_producer_partition ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' , * , existing_partition_keys : 'set[CompositeKey]' , input_fingerprint : 'Fingerprint' , partition_dependencies : 'frozendict[str, StoragePartitions]' , partition_key : 'CompositeKey' ) -> 'None' View Source def build_producer_partition ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , existing_partition_keys : set [ CompositeKey ] , input_fingerprint : Fingerprint , partition_dependencies : frozendict [ str, StoragePartitions ] , partition_key : CompositeKey , ) -> None : # TODO : Should this \"skip if exists\" live here or higher up ? if partition_key in existing_partition_keys : pk_str = f \" for: {dict(partition_key)}\" if partition_key else \".\" logging . info ( f \"Skipping existing {type(producer).__name__} output{pk_str}\" ) return logging . info ( f \"Building {producer} output for {partition_key}...\" ) # TODO : Catch DispatchError and give a nicer error ... maybe add this to our # @dispatch wrapper ( eg : msg arg , or even fn that returns the message to # raise ). arguments = { name : snapshot . read ( artifact = producer . inputs [ name ] , storage_partitions = partition_dependencies [ name ] , view = view , ) for name , view in producer . _build_inputs_ . items () } outputs = producer . build ( ** arguments ) if len ( producer . _outputs_ ) == 1 : outputs = ( outputs ,) validation_passed , validation_message = producer . validate_outputs ( * outputs ) if not validation_passed : raise ValueError ( validation_message ) for i , output in enumerate ( outputs ) : snapshot . write ( output , artifact = snapshot . graph . producer_outputs [ producer ][ i ] , input_fingerprint = input_fingerprint , keys = partition_key , view = producer . _outputs_ [ i ] , )","title":"build_producer_partition"},{"location":"reference/arti/#copy_3","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_3","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#discover_producer_partitions","text":"def discover_producer_partitions ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' , * , partition_input_fingerprints : 'InputFingerprints' ) -> 'set[CompositeKey]' View Source def discover_producer_partitions ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , partition_input_fingerprints : InputFingerprints , ) -> set [ CompositeKey ] : # NOTE : The output partitions may be built , but not yet associated with this GraphSnapshot # ( eg : raw input data changed , but no changes trickled into this specific Producer ). Hence # we 'll fetch all StoragePartitions for each Storage, filtered to the PKs and # input_fingerprints we' ve computed * are * for this snapshot - and then link them to the # snapshot . existing_output_partitions = { output : connection . read_artifact_partitions ( output , partition_input_fingerprints ) for output in snapshot . graph . producer_outputs [ producer ] } for artifact , partitions in existing_output_partitions . items () : connection . write_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact , partitions ) # TODO : Guarantee all outputs have the same set of identified partitions . Currently , this # pretends a partition is built for all outputs if _any_ are built for that partition . return { partition . keys for partition in chain . from_iterable ( existing_output_partitions . values ()) }","title":"discover_producer_partitions"},{"location":"reference/arti/#get_producer_inputs","text":"def get_producer_inputs ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' ) -> 'InputPartitions' View Source def get_producer_inputs ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer ) -> InputPartitions : return InputPartitions ( { name : connection . read_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact ) for name , artifact in producer . inputs . items () } )","title":"get_producer_inputs"},{"location":"reference/arti/#json_3","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#fingerprint","text":"class Fingerprint ( __pydantic_self__ , ** data : Any ) View Source class Fingerprint ( Model ) : \"\"\"Fingerprint represents a unique identity as an int64 value. Using an int(64) has a number of convenient properties: - can be combined independent of order with XOR - can be stored relatively cheaply - empty 0 values drop out when combined (5 ^ 0 = 5) - is relatively cross-platform (across databases, languages, etc) There are two \" special \" Fingerprints w/ factory functions that, when combined with other Fingerprints: - `empty()`: returns `empty()` - `identity()`: return the other Fingerprint \"\"\" key : Optional [ int64 ] def combine ( self , * others : Fingerprint ) -> Fingerprint : return reduce ( operator . xor , others , self ) @classmethod def empty ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None ) @classmethod def from_int ( cls , x : int , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x )) @classmethod def from_int64 ( cls , x : int64 , / ) -> Fingerprint : return cls ( key = x ) @classmethod def from_string ( cls , x : str , / ) -> Fingerprint : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x ))) @classmethod def from_uint64 ( cls , x : uint64 , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x )) @classmethod def identity ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 )) @property def is_empty ( self ) -> bool : return self . key is None @property def is_identity ( self ) -> bool : return self . key == 0 __and__ = _gen_fingerprint_binop ( operator . __and__ ) __lshift__ = _gen_fingerprint_binop ( operator . __lshift__ ) __or__ = _gen_fingerprint_binop ( operator . __or__ ) __rshift__ = _gen_fingerprint_binop ( operator . __rshift__ ) __xor__ = _gen_fingerprint_binop ( operator . __xor__ ) def __eq__ ( self , other : object ) -> bool : if isinstance ( other , int ) : other = Fingerprint . from_int ( other ) if isinstance ( other , Fingerprint ) : return self . key == other . key return NotImplemented","title":"Fingerprint"},{"location":"reference/arti/#ancestors-in-mro_4","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#class-variables_4","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/arti/#construct_4","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#empty","text":"def empty ( ) -> 'Fingerprint' Return a Fingerprint that, when combined, will return Fingerprint.empty() View Source @classmethod def empty ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None )","title":"empty"},{"location":"reference/arti/#from_int","text":"def from_int ( x : 'int' , / ) -> 'Fingerprint' View Source @classmethod def from_int ( cls , x : int , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x ))","title":"from_int"},{"location":"reference/arti/#from_int64","text":"def from_int64 ( x : 'int64' , / ) -> 'Fingerprint' View Source @classmethod def from_int64 ( cls , x : int64 , / ) -> Fingerprint : return cls ( key = x )","title":"from_int64"},{"location":"reference/arti/#from_orm_4","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#from_string","text":"def from_string ( x : 'str' , / ) -> 'Fingerprint' Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. View Source @classmethod def from_string ( cls , x : str , / ) -> Fingerprint : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x )))","title":"from_string"},{"location":"reference/arti/#from_uint64","text":"def from_uint64 ( x : 'uint64' , / ) -> 'Fingerprint' View Source @classmethod def from_uint64 ( cls , x : uint64 , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x ))","title":"from_uint64"},{"location":"reference/arti/#identity","text":"def identity ( ) -> 'Fingerprint' Return a Fingerprint that, when combined, will return the other Fingerprint. View Source @classmethod def identity ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 ))","title":"identity"},{"location":"reference/arti/#parse_file_4","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_4","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_4","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_4","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_4","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_4","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_4","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_4","text":"fingerprint is_empty is_identity","title":"Instance variables"},{"location":"reference/arti/#methods_5","text":"","title":"Methods"},{"location":"reference/arti/#combine","text":"def combine ( self , * others : 'Fingerprint' ) -> 'Fingerprint' View Source def combine ( self , * others : Fingerprint ) -> Fingerprint : return reduce ( operator . xor , others , self )","title":"combine"},{"location":"reference/arti/#copy_4","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_4","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_4","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#format","text":"class Format ( __pydantic_self__ , ** data : Any ) View Source class Format ( Model ): \"\"\"Format represents file formats such as CSV, Parquet, native (eg: databases), etc. Formats are associated with a type system that provides a bridge between the internal Artigraph types and any external type information. \"\"\" _abstract_ = True type_system : ClassVar [ TypeSystem ] extension : str = \"\" def _visit_type ( self , type_ : Type ) -> Self : # Ensure our type system can handle the provided type. self . type_system . to_system ( type_ , hints = {}) return self @classmethod def get_default ( cls ) -> Format : from arti.formats.json import JSON return JSON () # TODO: Support some sort of configurable defaults.","title":"Format"},{"location":"reference/arti/#ancestors-in-mro_5","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants_3","text":"arti.formats.json.JSON arti.formats.pickle.Pickle","title":"Descendants"},{"location":"reference/arti/#class-variables_5","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_5","text":"","title":"Static methods"},{"location":"reference/arti/#construct_5","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_5","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#get_default","text":"def get_default ( ) -> 'Format' View Source @classmethod def get_default ( cls ) -> Format : from arti.formats.json import JSON return JSON () # TODO: Support some sort of configurable defaults.","title":"get_default"},{"location":"reference/arti/#parse_file_5","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_5","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_5","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_5","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_5","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_5","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_5","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_5","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_6","text":"","title":"Methods"},{"location":"reference/arti/#copy_5","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_5","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_5","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#graph","text":"class Graph ( __pydantic_self__ , ** data : Any ) View Source class Graph ( Model ) : \"\"\"Graph stores a web of Artifacts connected by Producers.\"\"\" name : str artifacts : ArtifactBox = Field ( default_factory = lambda : ArtifactBox ( ** BOX_KWARGS [ SEALED ] )) # The Backend * itself * should not affect the results of a Graph build , though the contents # certainly may ( eg : stored annotations ), so we avoid serializing it . This also prevent # embedding any credentials . backend : Backend [ Connection ] = Field ( default_factory = _get_memory_backend , exclude = True ) path_tags : frozendict [ str, str ] = frozendict () # Graph starts off sealed , but is opened within a ` with Graph (...) ` context _status : Optional [ bool ] = PrivateAttr ( None ) _artifact_to_key : frozendict [ Artifact, str ] = PrivateAttr ( frozendict ()) @validator ( \"artifacts\" ) @classmethod def _convert_artifacts ( cls , artifacts : ArtifactBox ) -> ArtifactBox : return ArtifactBox ( artifacts , ** BOX_KWARGS [ SEALED ] ) def __enter__ ( self ) -> Graph : if arti . context . graph is not None : raise ValueError ( f \"Another graph is being defined: {arti.context.graph}\" ) arti . context . graph = self self . _toggle ( OPEN ) return self def __exit__ ( self , exc_type : Optional [ type[BaseException ] ] , exc_value : Optional [ BaseException ] , exc_traceback : Optional [ TracebackType ] , ) -> None : arti . context . graph = None self . _toggle ( SEALED ) # Confirm the dependencies are acyclic TopologicalSorter ( self . dependencies ). prepare () def _toggle ( self , status : bool ) -> None : # The Graph object is \"frozen\" , so we must bypass the assignment checks . object . __setattr__ ( self , \"artifacts\" , ArtifactBox ( self . artifacts , ** BOX_KWARGS [ status ] )) self . _status = status self . _artifact_to_key = frozendict ( { artifact : key for key , artifact in self . artifacts . walk () } ) @property def artifact_to_key ( self ) -> frozendict [ Artifact, str ] : return self . _artifact_to_key @requires_sealed def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : return self . snapshot (). build ( executor ) @requires_sealed def snapshot ( self , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Identify a \" unique \" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" return GraphSnapshot . from_graph ( self , connection = connection ) @cached_property @requires_sealed def dependencies ( self ) -> NodeDependencies : artifact_deps = { artifact : ( frozenset ( { artifact . producer_output . producer } ) if artifact . producer_output is not None else frozenset () ) for _ , artifact in self . artifacts . walk () } producer_deps = { # NOTE : multi - output Producers will appear multiple times ( but be deduped ) producer_output . producer : frozenset ( producer_output . producer . inputs . values ()) for artifact in artifact_deps if ( producer_output : = artifact . producer_output ) is not None } return NodeDependencies ( artifact_deps | producer_deps ) @cached_property @requires_sealed def producers ( self ) -> frozenset [ Producer ] : return frozenset ( self . producer_outputs ) @cached_property @requires_sealed def producer_outputs ( self ) -> frozendict [ Producer, tuple[Artifact, ... ] ]: d = defaultdict [ Producer, dict[int, Artifact ] ] ( dict ) for _ , artifact in self . artifacts . walk () : if artifact . producer_output is None : continue output = artifact . producer_output d [ output.producer ][ output.position ] = artifact return frozendict ( ( producer , tuple ( artifacts_by_position [ i ] for i in sorted ( artifacts_by_position ))) for producer , artifacts_by_position in d . items () ) # TODO : io . read / write probably need a bit of sanity checking ( probably somewhere else ), eg : type # ~= view . Doing validation on the data , etc . Should some of this live on the View ? @requires_sealed def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , snapshot : Optional [ GraphSnapshot ] = None , connection : Optional [ Connection ] = None , ) -> Any : key = self . artifact_to_key [ artifact ] if annotation is None and view is None : raise ValueError ( \"Either `annotation` or `view` must be passed\" ) if annotation is not None and view is not None : raise ValueError ( \"Only one of `annotation` or `view` may be passed\" ) if annotation is not None : view = View . get_class_for ( annotation )( artifact_class = type ( artifact ), type = artifact . type , mode = \"READ\" ) view . check_annotation_compatibility ( annotation ) view . check_artifact_compatibility ( artifact ) assert view is not None # mypy gets mixed up with ^ if storage_partitions is None : # We want to allow reading raw Artifacts even if other raw Artifacts are missing ( which # prevents snapshotting ). if snapshot is None and artifact . producer_output is None : # NOTE : We 're not using read_artifact_partitions as the underlying data may have # changed. The backend may have pointers to old versions (which is expected), but we # only want to return the current values. storage_partitions = artifact.storage.discover_partitions() else: snapshot = snapshot or self.snapshot() with (connection or self.backend).connect() as conn: storage_partitions = conn.read_snapshot_partitions(snapshot, key, artifact) return io.read( type_=artifact.type, format=artifact.format, storage_partitions=storage_partitions, view=view, ) @requires_sealed def write( self, data: Any, *, artifact: Artifact, input_fingerprint: Fingerprint = Fingerprint.empty(), keys: CompositeKey = CompositeKey(), view: Optional[View] = None, snapshot: Optional[GraphSnapshot] = None, connection: Optional[Connection] = None, ) -> StoragePartition: key = self.artifact_to_key[artifact] if snapshot is not None and artifact.producer_output is None: raise ValueError( f\"Writing to a raw Artifact (`{key}`) with a GraphSnapshot is not supported.\" ) if view is None: view = View.get_class_for(type(data))( artifact_class=type(artifact), type=artifact.type, mode=\"WRITE\" ) view.check_annotation_compatibility(type(data)) view.check_artifact_compatibility(artifact) storage_partition = artifact.storage.generate_partition( input_fingerprint=input_fingerprint, keys=keys, with_content_fingerprint=False ) storage_partition = io.write( data, type_=artifact.type, format=artifact.format, storage_partition=storage_partition, view=view, ).with_content_fingerprint() # TODO: Should we only do this in bulk? We might want the backends to transparently batch # requests, but that' s not so friendly with the transient \".connect\" . with ( connection or self . backend ). connect () as conn : conn . write_artifact_partitions ( artifact , ( storage_partition ,)) # Skip linking this partition to the snapshot if it affects raw Artifacts ( which would # trigger an id change ). if snapshot is not None and artifact . producer_output is not None : conn . write_snapshot_partitions ( snapshot , key , artifact , ( storage_partition ,)) return storage_partition","title":"Graph"},{"location":"reference/arti/#ancestors-in-mro_6","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#class-variables_6","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_6","text":"","title":"Static methods"},{"location":"reference/arti/#construct_6","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_6","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_6","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_6","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_6","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_6","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_6","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_6","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_6","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_6","text":"artifact_to_key fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_7","text":"","title":"Methods"},{"location":"reference/arti/#build_1","text":"def build ( self , executor : 'Optional[Executor]' = None ) -> 'GraphSnapshot' View Source @requires_sealed def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : return self . snapshot (). build ( executor )","title":"build"},{"location":"reference/arti/#copy_6","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dependencies","text":"def dependencies ( ... )","title":"dependencies"},{"location":"reference/arti/#dict_6","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_6","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#producer_outputs","text":"def producer_outputs ( ... )","title":"producer_outputs"},{"location":"reference/arti/#producers","text":"def producers ( ... )","title":"producers"},{"location":"reference/arti/#read_1","text":"def read ( self , artifact : 'Artifact' , * , annotation : 'Optional[Any]' = None , storage_partitions : 'Optional[Sequence[StoragePartition]]' = None , view : 'Optional[View]' = None , snapshot : 'Optional[GraphSnapshot]' = None , connection : 'Optional[Connection]' = None ) -> 'Any' View Source @requires_sealed def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , snapshot : Optional [ GraphSnapshot ] = None , connection : Optional [ Connection ] = None , ) -> Any : key = self . artifact_to_key [ artifact ] if annotation is None and view is None : raise ValueError ( \"Either `annotation` or `view` must be passed\" ) if annotation is not None and view is not None : raise ValueError ( \"Only one of `annotation` or `view` may be passed\" ) if annotation is not None : view = View . get_class_for ( annotation )( artifact_class = type ( artifact ), type = artifact . type , mode = \"READ\" ) view . check_annotation_compatibility ( annotation ) view . check_artifact_compatibility ( artifact ) assert view is not None # mypy gets mixed up with ^ if storage_partitions is None : # We want to allow reading raw Artifacts even if other raw Artifacts are missing ( which # prevents snapshotting ). if snapshot is None and artifact . producer_output is None : # NOTE : We ' re not using read_artifact_partitions as the underlying data may have # changed . The backend may have pointers to old versions ( which is expected ), but we # only want to return the current values . storage_partitions = artifact . storage . discover_partitions () else : snapshot = snapshot or self . snapshot () with ( connection or self . backend ). connect () as conn : storage_partitions = conn . read_snapshot_partitions ( snapshot , key , artifact ) return io . read ( type_ = artifact . type , format = artifact . format , storage_partitions = storage_partitions , view = view , )","title":"read"},{"location":"reference/arti/#snapshot","text":"def snapshot ( self , * , connection : 'Optional[Connection]' = None ) -> 'GraphSnapshot' Identify a \"unique\" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a snapshot of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. View Source @requires_sealed def snapshot ( self , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Identify a \" unique \" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" return GraphSnapshot . from_graph ( self , connection = connection )","title":"snapshot"},{"location":"reference/arti/#write_1","text":"def write ( self , data : 'Any' , * , artifact : 'Artifact' , input_fingerprint : 'Fingerprint' = Fingerprint ( key = None ), keys : 'CompositeKey' = {}, view : 'Optional[View]' = None , snapshot : 'Optional[GraphSnapshot]' = None , connection : 'Optional[Connection]' = None ) -> 'StoragePartition' View Source @requires_sealed def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , snapshot : Optional [ GraphSnapshot ] = None , connection : Optional [ Connection ] = None , ) -> StoragePartition : key = self . artifact_to_key [ artifact ] if snapshot is not None and artifact . producer_output is None : raise ValueError ( f \"Writing to a raw Artifact (`{key}`) with a GraphSnapshot is not supported.\" ) if view is None : view = View . get_class_for ( type ( data ))( artifact_class = type ( artifact ), type = artifact . type , mode = \"WRITE\" ) view . check_annotation_compatibility ( type ( data )) view . check_artifact_compatibility ( artifact ) storage_partition = artifact . storage . generate_partition ( input_fingerprint = input_fingerprint , keys = keys , with_content_fingerprint = False ) storage_partition = io . write ( data , type_ = artifact . type , format = artifact . format , storage_partition = storage_partition , view = view , ). with_content_fingerprint () # TODO : Should we only do this in bulk ? We might want the backends to transparently batch # requests , but that ' s not so friendly with the transient \".connect\" . with ( connection or self . backend ). connect () as conn : conn . write_artifact_partitions ( artifact , ( storage_partition ,)) # Skip linking this partition to the snapshot if it affects raw Artifacts ( which would # trigger an id change ). if snapshot is not None and artifact . producer_output is not None : conn . write_snapshot_partitions ( snapshot , key , artifact , ( storage_partition ,)) return storage_partition","title":"write"},{"location":"reference/arti/#graphsnapshot","text":"class GraphSnapshot ( __pydantic_self__ , ** data : Any ) View Source class GraphSnapshot ( Model ) : \"\"\"GraphSnapshot represents the state of a Graph and the referenced raw data at a point in time. GraphSnapshot encodes the structure of the Graph plus a snapshot of the raw Artifact data (partition kinds and contents) at a point in time. Any change that would affect data should prompt an ID change. \"\"\" id : Fingerprint graph : Graph @property def artifacts ( self ) -> ArtifactBox : return self . graph . artifacts @property def backend ( self ) -> Backend [ Connection ] : return self . graph . backend @property def name ( self ) -> str : return self . graph . name @classmethod # TODO : Should this use a ( TTL ) cache ? Raw data changes ( especially in tests ) still need to be detected . def from_graph ( cls , graph : Graph , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Snapshot the Graph and all existing raw data. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifact data during Producer builds. \"\"\" # TODO : Resolve and statically set all available fingerprints . Specifically , we should pin # the Producer . fingerprint , which may by dynamic ( eg : version is a Timestamp ). Unbuilt # Artifact ( partitions ) won 't be fully resolved yet. snapshot_id, known_artifact_partitions = graph.fingerprint, dict[str, StoragePartitions]() for node, _ in graph.dependencies.items(): snapshot_id = snapshot_id.combine(node.fingerprint) if isinstance(node, Artifact): key = graph.artifact_to_key[node] snapshot_id = snapshot_id.combine(Fingerprint.from_string(key)) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we' ll have to handle things a bit # differently depending on if the external Artifacts are Produced ( in an upstream # Graph ) or not . if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . storage . discover_partitions () ) if not known_artifact_partitions [ key ] : content_str = \"partitions\" if is_partitioned ( node . type ) else \"data\" raise ValueError ( f \"No {content_str} found for `{key}`: {node}\" ) snapshot_id = snapshot_id . combine ( *[ partition.fingerprint for partition in known_artifact_partitions[key ] ] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma : no cover # NOTE : This shouldn ' t happen unless the logic above is faulty . raise ValueError ( \"Fingerprint is empty!\" ) snapshot = cls ( graph = graph , id = snapshot_id ) # Write the discovered partitions ( if not already known ) and link to this new snapshot . with ( connection or snapshot . backend ). connect () as conn : conn . write_graph ( graph ) conn . write_snapshot ( snapshot ) for key , partitions in known_artifact_partitions . items () : conn . write_artifact_and_graph_partitions ( snapshot , key , snapshot . artifacts [ key ] , partitions ) return snapshot def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : if executor is None : from arti . executors . local import LocalExecutor executor = LocalExecutor () executor . build ( self ) return self def tag ( self , tag : str , * , overwrite : bool = False , connection : Optional [ Connection ] = None ) -> None : with ( connection or self . backend ). connect () as conn : conn . write_snapshot_tag ( self , tag , overwrite ) @classmethod def from_tag ( cls , name : str , tag : str , * , connectable : Union [ Backend[Connection ] , Connection ] ) -> GraphSnapshot : with connectable . connect () as conn : return conn . read_snapshot_tag ( name , tag ) def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> Any : return self . graph . read ( artifact , annotation = annotation , storage_partitions = storage_partitions , view = view , snapshot = self , connection = connection , ) def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> StoragePartition : return self . graph . write ( data , artifact = artifact , input_fingerprint = input_fingerprint , keys = keys , view = view , snapshot = self , connection = connection , )","title":"GraphSnapshot"},{"location":"reference/arti/#ancestors-in-mro_7","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#class-variables_7","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_7","text":"","title":"Static methods"},{"location":"reference/arti/#construct_7","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_graph","text":"def from_graph ( graph : 'Graph' , * , connection : 'Optional[Connection]' = None ) -> 'GraphSnapshot' Snapshot the Graph and all existing raw data. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifact data during Producer builds. View Source @classmethod # TODO : Should this use a ( TTL ) cache ? Raw data changes ( especially in tests ) still need to be detected . def from_graph ( cls , graph : Graph , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Snapshot the Graph and all existing raw data. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifact data during Producer builds. \"\"\" # TODO : Resolve and statically set all available fingerprints . Specifically , we should pin # the Producer . fingerprint , which may by dynamic ( eg : version is a Timestamp ). Unbuilt # Artifact ( partitions ) won 't be fully resolved yet. snapshot_id, known_artifact_partitions = graph.fingerprint, dict[str, StoragePartitions]() for node, _ in graph.dependencies.items(): snapshot_id = snapshot_id.combine(node.fingerprint) if isinstance(node, Artifact): key = graph.artifact_to_key[node] snapshot_id = snapshot_id.combine(Fingerprint.from_string(key)) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we' ll have to handle things a bit # differently depending on if the external Artifacts are Produced ( in an upstream # Graph ) or not . if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . storage . discover_partitions () ) if not known_artifact_partitions [ key ] : content_str = \"partitions\" if is_partitioned ( node . type ) else \"data\" raise ValueError ( f \"No {content_str} found for `{key}`: {node}\" ) snapshot_id = snapshot_id . combine ( *[ partition.fingerprint for partition in known_artifact_partitions[key ] ] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma : no cover # NOTE : This shouldn ' t happen unless the logic above is faulty . raise ValueError ( \"Fingerprint is empty!\" ) snapshot = cls ( graph = graph , id = snapshot_id ) # Write the discovered partitions ( if not already known ) and link to this new snapshot . with ( connection or snapshot . backend ). connect () as conn : conn . write_graph ( graph ) conn . write_snapshot ( snapshot ) for key , partitions in known_artifact_partitions . items () : conn . write_artifact_and_graph_partitions ( snapshot , key , snapshot . artifacts [ key ] , partitions ) return snapshot","title":"from_graph"},{"location":"reference/arti/#from_orm_7","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#from_tag","text":"def from_tag ( name : 'str' , tag : 'str' , * , connectable : 'Union[Backend[Connection], Connection]' ) -> 'GraphSnapshot' View Source @classmethod def from_tag ( cls , name : str , tag : str , * , connectable : Union [ Backend[Connection ] , Connection ] ) -> GraphSnapshot : with connectable . connect () as conn : return conn . read_snapshot_tag ( name , tag )","title":"from_tag"},{"location":"reference/arti/#parse_file_7","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_7","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_7","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_7","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_7","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_7","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_7","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_7","text":"artifacts backend fingerprint name","title":"Instance variables"},{"location":"reference/arti/#methods_8","text":"","title":"Methods"},{"location":"reference/arti/#build_2","text":"def build ( self , executor : 'Optional[Executor]' = None ) -> 'GraphSnapshot' View Source def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : if executor is None : from arti.executors.local import LocalExecutor executor = LocalExecutor () executor . build ( self ) return self","title":"build"},{"location":"reference/arti/#copy_7","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_7","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_7","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#read_2","text":"def read ( self , artifact : 'Artifact' , * , annotation : 'Optional[Any]' = None , storage_partitions : 'Optional[Sequence[StoragePartition]]' = None , view : 'Optional[View]' = None , connection : 'Optional[Connection]' = None ) -> 'Any' View Source def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> Any : return self . graph . read ( artifact , annotation = annotation , storage_partitions = storage_partitions , view = view , snapshot = self , connection = connection , )","title":"read"},{"location":"reference/arti/#tag","text":"def tag ( self , tag : 'str' , * , overwrite : 'bool' = False , connection : 'Optional[Connection]' = None ) -> 'None' View Source def tag ( self , tag : str , * , overwrite : bool = False , connection : Optional [ Connection ] = None ) -> None : with ( connection or self . backend ). connect () as conn : conn . write_snapshot_tag ( self , tag , overwrite )","title":"tag"},{"location":"reference/arti/#write_2","text":"def write ( self , data : 'Any' , * , artifact : 'Artifact' , input_fingerprint : 'Fingerprint' = Fingerprint ( key = None ), keys : 'CompositeKey' = {}, view : 'Optional[View]' = None , connection : 'Optional[Connection]' = None ) -> 'StoragePartition' View Source def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> StoragePartition : return self . graph . write ( data , artifact = artifact , input_fingerprint = input_fingerprint , keys = keys , view = view , snapshot = self , connection = connection , )","title":"write"},{"location":"reference/arti/#partitionkey","text":"class PartitionKey ( __pydantic_self__ , ** data : Any ) View Source class PartitionKey ( Model ) : _abstract_ = True _by_type_ : ClassVar [ dict[type[Type ] , type [ PartitionKey ] ]] = {} default_key_components : ClassVar [ frozendict[str, str ] ] matching_type : ClassVar [ type[Type ] ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return for attr in ( \"default_key_components\" , \"matching_type\" ) : if not hasattr ( cls , attr ) : raise TypeError ( f \"{cls.__name__} must set `{attr}`\" ) if unknown : = set ( cls . default_key_components ) - cls . key_components : raise TypeError ( f \"Unknown key_components in {cls.__name__}.default_key_components: {unknown}\" ) register ( cls . _by_type_ , cls . matching_type , cls ) @classproperty def key_components ( cls ) -> frozenset [ str ] : return frozenset ( cls . __fields__ ) | frozenset ( name for name in dir ( cls ) if isinstance ( getattr_static ( cls , name ), key_component ) ) @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : raise NotImplementedError ( f \"Unable to parse '{cls.__name__}' from: {key_components}\" ) @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ] @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"PartitionKey"},{"location":"reference/arti/#ancestors-in-mro_8","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants_4","text":"arti.partitions.DateKey arti.partitions._IntKey arti.partitions.NullKey","title":"Descendants"},{"location":"reference/arti/#class-variables_8","text":"Config key_components","title":"Class variables"},{"location":"reference/arti/#static-methods_8","text":"","title":"Static methods"},{"location":"reference/arti/#construct_8","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_key_components","text":"def from_key_components ( ** key_components : 'str' ) -> 'PartitionKey' View Source @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : raise NotImplementedError ( f \"Unable to parse '{cls.__name__}' from: {key_components}\" )","title":"from_key_components"},{"location":"reference/arti/#from_orm_8","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#get_class_for","text":"def get_class_for ( type_ : 'Type' ) -> 'type[PartitionKey]' View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ]","title":"get_class_for"},{"location":"reference/arti/#parse_file_8","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_8","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_8","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_8","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_8","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#types_from","text":"def types_from ( type_ : 'Type' ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"types_from"},{"location":"reference/arti/#update_forward_refs_8","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_8","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_8","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_9","text":"","title":"Methods"},{"location":"reference/arti/#copy_8","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_8","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_8","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#producer_1","text":"class Producer ( __pydantic_self__ , ** data : Any ) View Source class Producer ( Model ) : \"\"\"A Producer is a task that builds one or more Artifacts.\"\"\" # User fields / methods annotations : tuple [ Annotation, ... ] = () version : Version = SemVer ( major = 0 , minor = 0 , patch = 1 ) # The map / build / validate_outputs parameters are intended to be dynamic and set by subclasses , # however mypy doesn 't like the \"incompatible\" signature on subclasses if actually defined here # (nor support ParamSpec yet). `map` is generated during subclassing if not set, `build` is # required, and `validate_outputs` defaults to no-op checks (hence is the only one with a # provided method). # # These must be @classmethods or @staticmethods. map: ClassVar[MapSig] build: ClassVar[BuildSig] if TYPE_CHECKING: validate_outputs: ClassVar[ValidateSig] else: @staticmethod def validate_outputs(*outputs: Any) -> Union[bool, tuple[bool, str]]: \"\"\"Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\"\" return True, \"No validation performed.\" # Internal fields/methods _abstract_: ClassVar[bool] = True _fingerprint_excludes_ = frozenset([\"annotations\"]) # NOTE: The following are set in __init_subclass__ _input_artifact_classes_: ClassVar[frozendict[str, type[Artifact]]] _build_inputs_: ClassVar[BuildInputs] _build_sig_: ClassVar[Signature] _map_inputs_: ClassVar[MapInputs] _map_sig_: ClassVar[Signature] _outputs_: ClassVar[Outputs] @classmethod def __init_subclass__(cls, **kwargs: Any) -> None: super().__init_subclass__(**kwargs) if not cls._abstract_: with wrap_exc(ValueError, prefix=cls.__name__): cls._input_artifact_classes_ = cls._validate_fields() with wrap_exc(ValueError, prefix=\".build\"): ( cls._build_sig_, cls._build_inputs_, cls._outputs_, ) = cls._validate_build_sig() with wrap_exc(ValueError, prefix=\".validate_output\"): cls._validate_validate_output_sig() with wrap_exc(ValueError, prefix=\".map\"): cls._map_sig_, cls._map_inputs_ = cls._validate_map_sig() cls._validate_no_unused_fields() @classmethod def _validate_fields(cls) -> frozendict[str, type[Artifact]]: # NOTE: Aside from the base producer fields, all others should (currently) be Artifacts. # # Users can set additional class attributes, but they must be properly hinted as ClassVars. # These won' t interact with the \"framework\" and can 't be parameters to build/map. artifact_fields = {k: v for k, v in cls.__fields__.items() if k not in Producer.__fields__} for name, field in artifact_fields.items(): with wrap_exc(ValueError, prefix=f\".{name}\"): if not (field.default is None and field.default_factory is None and field.required): raise ValueError(\"field must not have a default nor be Optional.\") if not lenient_issubclass(field.outer_type_, Artifact): raise ValueError( f\"type hint must be an Artifact subclass, got: {field.outer_type_}\" ) return frozendict({name: field.outer_type_ for name, field in artifact_fields.items()}) @classmethod def _validate_parameters( cls, sig: Signature, *, validator: Callable[[str, Parameter], _T] ) -> Iterator[_T]: if undefined_params := set(sig.parameters) - set(cls._input_artifact_classes_): raise ValueError( f\"the following parameter(s) must be defined as a field: {undefined_params}\" ) for name, param in sig.parameters.items(): with wrap_exc(ValueError, prefix=f\" {name} param\"): if param.annotation is param.empty: raise ValueError(\"must have a type hint.\") if param.default is not param.empty: raise ValueError(\"must not have a default.\") if param.kind not in (param.POSITIONAL_OR_KEYWORD, param.KEYWORD_ONLY): raise ValueError(\"must be usable as a keyword argument.\") yield validator(name, param) @classmethod def _validate_build_param(cls, name: str, param: Parameter) -> tuple[str, View]: annotation = param.annotation field_artifact_class = cls._input_artifact_classes_[param.name] # If there is no Artifact hint, add in the field value as the default. if get_item_from_annotated(annotation, Artifact, is_subclass=True) is None: annotation = Annotated[annotation, field_artifact_class] view = View.from_annotation(annotation, mode=\"READ\") if view.artifact_class != field_artifact_class: raise ValueError( f\"annotation Artifact class ({view.artifact_class}) does not match that set on the field ({field_artifact_class}).\" ) return name, view @classmethod def _validate_build_sig_return(cls, annotation: Any, *, i: int) -> View: with wrap_exc(ValueError, prefix=f\" {ordinal(i+1)} return\"): return View.from_annotation(annotation, mode=\"WRITE\") @classmethod def _validate_build_sig(cls) -> tuple[Signature, BuildInputs, Outputs]: \"\"\"Validate the .build method\"\"\" if not hasattr(cls, \"build\"): raise ValueError(\"must be implemented\") if not isinstance(getattr_static(cls, \"build\"), (classmethod, staticmethod)): raise ValueError(\"must be a @classmethod or @staticmethod\") build_sig = signature(cls.build, force_tuple_return=True, remove_owner=True) # Validate the parameters build_inputs = BuildInputs( cls._validate_parameters(build_sig, validator=cls._validate_build_param) ) # Validate the return definition return_annotation = build_sig.return_annotation if return_annotation is build_sig.empty: # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError(\"a return value must be set with the output Artifact(s).\") if return_annotation == (NoneType,): raise ValueError(\"missing return signature\") outputs = Outputs( cls._validate_build_sig_return(annotation, i=i) for i, annotation in enumerate(return_annotation) ) # Validate all outputs have equivalent partitioning schemes. # # We currently require the partition key type *and* name to match, but in the future we # might be able to extend the dependency metadata to support heterogeneous names if # necessary. seen_key_types = {PartitionKey.types_from(view.type) for view in outputs} if len(seen_key_types) != 1: raise ValueError(\"all outputs must have the same partitioning scheme\") return build_sig, build_inputs, outputs @classmethod def _validate_validate_output_sig(cls) -> None: build_output_hints = [ get_args(hint)[0] if get_origin(hint) is Annotated else hint for hint in cls._build_sig_.return_annotation ] match_build_str = f\"match the `.build` return (`{build_output_hints}`)\" validate_parameters = signature(cls.validate_outputs).parameters def param_matches(param: Parameter, build_return: type) -> bool: # Skip checking non-hinted parameters to allow lambdas. # # NOTE: Parameter type hints are *contravariant* (you can' t pass a \"Manager\" into a # function expecting an \"Employee\" ), hence the lenient_issubclass has build_return as # the subtype and param . annotation as the supertype . return param . annotation is param . empty or lenient_issubclass ( build_return , param . annotation ) if ( # Allow ` * args : Any ` or ` * args : T ` for ` build (...) -> tuple [ T, ... ] ` len ( validate_parameters ) == 1 and ( param : = tuple ( validate_parameters . values ()) [ 0 ] ). kind == param . VAR_POSITIONAL ) : if not all ( param_matches ( param , output_hint ) for output_hint in build_output_hints ) : with wrap_exc ( ValueError , prefix = f \" {param.name} param\" ) : raise ValueError ( f \"type hint must be `Any` or {match_build_str}\" ) else : # Otherwise , check pairwise if len ( validate_parameters ) != len ( build_output_hints ) : raise ValueError ( f \"must {match_build_str}\" ) for i , ( name , param ) in enumerate ( validate_parameters . items ()) : with wrap_exc ( ValueError , prefix = f \" {name} param\" ) : if param . default is not param . empty : raise ValueError ( \"must not have a default.\" ) if param . kind not in ( param . POSITIONAL_ONLY , param . POSITIONAL_OR_KEYWORD ) : raise ValueError ( \"must be usable as a positional argument.\" ) if not param_matches ( param , ( expected : = build_output_hints [ i ] )) : raise ValueError ( f \"type hint must match the {ordinal(i+1)} `.build` return (`{expected}`)\" ) # TODO : Validate return signature ? @classmethod def _validate_map_param ( cls , name : str , param : Parameter ) -> str : # TODO : Should we add some ArtifactPartition [ MyArtifact ] type ? if param . annotation != StoragePartitions : raise ValueError ( \"type hint must be `StoragePartitions`\" ) return name @classmethod def _validate_map_sig ( cls ) -> tuple [ Signature, MapInputs ] : \"\"\"Validate partitioned Artifacts and the .map method\"\"\" if not hasattr ( cls , \"map\" ) : # TODO : Add runtime checking of ` map ` output ( ie : output aligns w / output # artifacts and such ). if any ( is_partitioned ( view . type ) for view in cls . _outputs_ ) : raise ValueError ( \"must be implemented when the `build` outputs are partitioned\" ) def map ( ** kwargs : StoragePartitions ) -> PartitionDependencies : return PartitionDependencies ( { NotPartitioned : frozendict ( kwargs ) } ) # Narrow the map signature , which is validated below and used at graph build time ( via # cls . _map_inputs_ ) to determine what arguments to pass to map . map . __signature__ = Signature ( # type : ignore [ attr-defined ] [ Parameter(name=name, annotation=StoragePartitions, kind=Parameter.KEYWORD_ONLY) for name, artifact in cls._input_artifact_classes_.items() if name in cls._build_inputs_ ] , return_annotation = PartitionDependencies , ) cls . map = cast ( MapSig , staticmethod ( map )) if not isinstance ( getattr_static ( cls , \"map\" ), ( classmethod , staticmethod )) : raise ValueError ( \"must be a @classmethod or @staticmethod\" ) map_sig = signature ( cls . map ) map_inputs = MapInputs ( cls . _validate_parameters ( map_sig , validator = cls . _validate_map_param )) return map_sig , map_inputs # TODO : Verify map output hint matches TBD spec @classmethod def _validate_no_unused_fields ( cls ) -> None : if unused_fields : = set ( cls . _input_artifact_classes_ ) - ( set ( cls . _build_sig_ . parameters ) | set ( cls . _map_sig_ . parameters ) ) : raise ValueError ( f \"the following fields aren't used in `.build` or `.map`: {unused_fields}\" ) @validator ( \"*\" ) @classmethod def _validate_instance_artifact_args ( cls , value : Artifact , field : ModelField ) -> Artifact : if ( view : = cls . _build_inputs_ . get ( field . name )) is not None : view . check_artifact_compatibility ( value ) return value # NOTE : pydantic defines . __iter__ to return ` self . __dict__ . items () ` to support ` dict ( model ) ` , # but we want to override to support easy expansion / assignment to a Graph without ` . out () ` ( eg : # ` g . artifacts . a , g . artifacts . b = MyProducer (...) ` ). def __iter__ ( self ) -> Iterator [ Artifact ] : # type : ignore [ override ] ret = self . out () if not isinstance ( ret , tuple ) : ret = ( ret ,) return iter ( ret ) def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str, StoragePartitions ] ) -> Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_inputs_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected {expected_names}, got {input_names}\" ) # We only care if the * code * or * input partition contents * changed , not if the input file # paths changed ( but have the same content as a prior run ). return Fingerprint . from_string ( self . _class_key_ ). combine ( self . version . fingerprint , * ( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), ) def compute_dependencies ( self , input_partitions : InputPartitions ) -> tuple [ PartitionDependencies, InputFingerprints ] : # TODO : Validate the partition_dependencies against the Producer ' s partitioning scheme and # such ( basically , check user error ). eg : if output is not partitioned , we expect only 1 # entry in partition_dependencies ( NotPartitioned ). partition_dependencies = self . map ( ** { name : partitions for name , partitions in input_partitions . items () if name in self . _map_inputs_ } ) partition_input_fingerprints = InputFingerprints ( { composite_key : self . compute_input_fingerprint ( dependency_partitions ) for composite_key , dependency_partitions in partition_dependencies . items () } ) return partition_dependencies , partition_input_fingerprints @property def inputs ( self ) -> dict [ str, Artifact ] : return { k : getattr ( self , k ) for k in self . _input_artifact_classes_ } def out ( self , * outputs : Artifact ) -> Union [ Artifact, tuple[Artifact, ... ] ]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : outputs = tuple ( view . artifact_class ( type = view . type ) for view in self . _outputs_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = \", \" . join ( [ str(v) for v in self._build_sig_.return_annotation ] ) raise ValueError ( f \"{self._class_key_}.out() - expected {expected_n} arguments of ({ret_str}), but got: {outputs}\" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : view = self . _outputs_ [ ord ] with wrap_exc ( ValueError , prefix = f \"{self._class_key_}.out() {ordinal(ord+1)} argument\" ) : view . check_artifact_compatibility ( artifact ) if artifact . producer_output is not None : raise ValueError ( f \"{artifact} is produced by {artifact.producer_output.producer}!\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord ) } ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs","title":"Producer"},{"location":"reference/arti/#ancestors-in-mro_9","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#class-variables_9","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_9","text":"","title":"Static methods"},{"location":"reference/arti/#construct_9","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_9","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_9","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_9","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_9","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_9","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_9","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_9","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_9","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#validate_outputs","text":"def validate_outputs ( * outputs : 'Any' ) -> 'Union[bool, tuple[bool, str]]' Validate the Producer.build outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of build will be passed in as it was returned, for example: def build(...): return 1, 2 will result in validate_outputs(1, 2) . NOTE: validate_outputs is a stopgap until Statistics and Thresholds are fully implemented. View Source @staticmethod def validate_outputs ( * outputs : Any ) -> Union [ bool , tuple [ bool , str ]] : \" \"\" Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\" \" return True , \"No validation performed.\"","title":"validate_outputs"},{"location":"reference/arti/#instance-variables_9","text":"fingerprint inputs","title":"Instance variables"},{"location":"reference/arti/#methods_10","text":"","title":"Methods"},{"location":"reference/arti/#compute_dependencies","text":"def compute_dependencies ( self , input_partitions : 'InputPartitions' ) -> 'tuple[PartitionDependencies, InputFingerprints]' View Source def compute_dependencies ( self , input_partitions : InputPartitions ) -> tuple [ PartitionDependencies , InputFingerprints ] : # TODO : Validate the partition_dependencies against the Producer 's partitioning scheme and # such (basically, check user error). eg: if output is not partitioned, we expect only 1 # entry in partition_dependencies (NotPartitioned). partition_dependencies = self.map( **{ name: partitions for name, partitions in input_partitions.items() if name in self._map_inputs_ } ) partition_input_fingerprints = InputFingerprints( { composite_key: self.compute_input_fingerprint(dependency_partitions) for composite_key, dependency_partitions in partition_dependencies.items() } ) return partition_dependencies, partition_input_fingerprints","title":"compute_dependencies"},{"location":"reference/arti/#compute_input_fingerprint","text":"def compute_input_fingerprint ( self , dependency_partitions : 'frozendict[str, StoragePartitions]' ) -> 'Fingerprint' View Source def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str , StoragePartitions ] ) - > Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_inputs_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected {expected_names}, got {input_names}\" ) # We only care if the * code * or * input partition contents * changed , not if the input file # paths changed ( but have the same content as a prior run ). return Fingerprint . from_string ( self . _class_key_ ) . combine ( self . version . fingerprint , *( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), )","title":"compute_input_fingerprint"},{"location":"reference/arti/#copy_9","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_9","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_9","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#out","text":"def out ( self , * outputs : 'Artifact' ) -> 'Union[Artifact, tuple[Artifact, ...]]' Configure the output Artifacts this Producer will build. The arguments are matched to the Producer.build return signature in order. View Source def out ( self , * outputs : Artifact ) -> Union [ Artifact, tuple[Artifact, ... ] ]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : outputs = tuple ( view . artifact_class ( type = view . type ) for view in self . _outputs_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = \", \" . join ( [ str(v) for v in self._build_sig_.return_annotation ] ) raise ValueError ( f \"{self._class_key_}.out() - expected {expected_n} arguments of ({ret_str}), but got: {outputs}\" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : view = self . _outputs_ [ ord ] with wrap_exc ( ValueError , prefix = f \"{self._class_key_}.out() {ordinal(ord+1)} argument\" ) : view . check_artifact_compatibility ( artifact ) if artifact . producer_output is not None : raise ValueError ( f \"{artifact} is produced by {artifact.producer_output.producer}!\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord ) } ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs","title":"out"},{"location":"reference/arti/#statistic","text":"class Statistic ( __pydantic_self__ , ** data : Any ) View Source class Statistic ( Model ): pass # TODO: Determine the interface for Statistics","title":"Statistic"},{"location":"reference/arti/#ancestors-in-mro_10","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#class-variables_10","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_10","text":"","title":"Static methods"},{"location":"reference/arti/#construct_10","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_10","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_10","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_10","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_10","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_10","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_10","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_10","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_10","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_10","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_11","text":"","title":"Methods"},{"location":"reference/arti/#copy_10","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_10","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_10","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#storage","text":"class Storage ( __pydantic_self__ , ** data : Any ) View Source class Storage ( Model , Generic [ StoragePartitionVar_co ] ) : \"\"\"Storage is a data reference identifying 1 or more partitions of data. Storage fields should have defaults set with placeholders for tags and partition keys. This allows automatic injection of the tags and partition keys for simple cases. \"\"\" _abstract_ = True storage_partition_type : ClassVar [ type[StoragePartitionVar_co ] ] # type : ignore [ misc ] # These separators are used in the default resolve_ * helpers to format metadata into # the storage fields . # # The defaults are tailored for \"path\" - like fields . key_value_sep : ClassVar [ str ] = \"=\" partition_name_component_sep : ClassVar [ str ] = \"_\" segment_sep : ClassVar [ str ] = os . sep _key_types : Optional [ CompositeKeyTypes ] = PrivateAttr ( None ) @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return cls . storage_partition_type = get_class_type_vars ( cls ) [ 0 ] expected_field_types = { name : info . outer_type_ for name , info in cls . storage_partition_type . __fields__ . items () if name not in StoragePartition . __fields__ } fields = { name : info . outer_type_ for name , info in cls . __fields__ . items () if name not in Storage . __fields__ } if fields != expected_field_types : raise TypeError ( f \"{cls.__name__} fields must match {cls.storage_partition_type.__name__} ({expected_field_types}), got: {fields}\" ) @classmethod def get_default ( cls ) -> Storage [ StoragePartition ] : from arti . storage . literal import StringLiteral return StringLiteral () # TODO : Support some sort of configurable defaults . def _visit_type ( self , type_ : Type ) -> Self : # TODO : Check support for the types and partitioning on the specified field ( s ). copy = self . copy () copy . _key_types = PartitionKey . types_from ( type_ ) assert copy . key_types is not None key_component_specs = { f \"{name}{self.partition_name_component_sep}{component_name}\" : f \"{{{name}.{component_spec}}}\" for name , pk in copy . key_types . items () for component_name , component_spec in pk . default_key_components . items () } return copy . resolve ( partition_key_spec = self . segment_sep . join ( f \"{name}{self.key_value_sep}{spec}\" for name , spec in key_component_specs . items () ) ) def _visit_format ( self , format_ : Format ) -> Self : return self . resolve ( extension = format_ . extension ) def _visit_graph ( self , graph : Graph ) -> Self : return self . resolve ( graph_name = graph . name , path_tags = self . segment_sep . join ( f \"{tag}{self.key_value_sep}{value}\" for tag , value in graph . path_tags . items () ), ) def _visit_input_fingerprint ( self , input_fingerprint : Fingerprint ) -> Self : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" return self . resolve ( input_fingerprint = input_fingerprint_key ) def _visit_names ( self , names : tuple [ str, ... ] ) -> Self : return self . resolve ( name = names [ -1 ] if names else \"\" , names = self . segment_sep . join ( names )) @property def includes_input_fingerprint_template ( self ) -> bool : return any ( \"{input_fingerprint}\" in val for val in self . _format_fields . values ()) @property def key_types ( self ) -> CompositeKeyTypes : if self . _key_types is None : raise ValueError ( \"`key_types` have not been set yet.\" ) return self . _key_types @property def _format_fields ( self ) -> frozendict [ str, str ] : return frozendict ( { name : value for name in self . __fields__ if lenient_issubclass ( type ( value : = getattr ( self , name )), str ) } ) @classmethod def _check_keys ( cls , key_types : CompositeKeyTypes , keys : CompositeKey ) -> None : # TODO : Confirm the key names and types align if key_types and not keys : raise ValueError ( f \"Expected partition keys {tuple(key_types)} but none were passed\" ) if keys and not key_types : raise ValueError ( f \"Expected no partition keys but got: {keys}\" ) @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartitionVar_co, ... ] : raise NotImplementedError () def generate_partition ( self , keys : CompositeKey = CompositeKey (), input_fingerprint : Fingerprint = Fingerprint . empty (), with_content_fingerprint : bool = True , ) -> StoragePartitionVar_co : self . _check_keys ( self . key_types , keys ) format_kwargs = dict [ Any, Any ] ( keys ) if input_fingerprint . is_empty : if self . includes_input_fingerprint_template : raise ValueError ( f \"{self} requires an input_fingerprint, but none was provided\" ) else : if not self . includes_input_fingerprint_template : raise ValueError ( f \"{self} does not specify a {{input_fingerprint}} template\" ) format_kwargs [ \"input_fingerprint\" ] = str ( input_fingerprint . key ) field_values = { name : ( strip_partition_indexes ( original ). format ( ** format_kwargs ) if lenient_issubclass ( type ( original : = getattr ( self , name )), str ) else original ) for name in self . __fields__ if name in self . storage_partition_type . __fields__ } partition = self . storage_partition_type ( input_fingerprint = input_fingerprint , keys = keys , ** field_values ) if with_content_fingerprint : partition = partition . with_content_fingerprint () return partition def _resolve_field ( self , name : str , spec : str , placeholder_values : dict [ str, str ] ) -> str : for placeholder , value in placeholder_values . items () : if not value : # Strip placeholder * and * any trailing self . segment_sep . trim = \"{\" + placeholder + \"}\" if f \"{trim}{self.segment_sep}\" in spec : trim = f \"{trim}{self.segment_sep}\" # Also strip any trailing separators , eg : if the placeholder was at the end . spec = spec . replace ( trim , \"\" ). rstrip ( self . segment_sep ) if not spec : raise ValueError ( f \"{self}.{name} was empty after removing unused templates\" ) return partial_format ( spec , ** placeholder_values ) def resolve ( self , ** values : str ) -> Self : return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ). if ( new : = self . _resolve_field ( name , original , values )) != original } )","title":"Storage"},{"location":"reference/arti/#ancestors-in-mro_11","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants_5","text":"arti.storage.google.cloud.storage.GCSFile arti.storage.local.LocalFile arti.storage.literal.StringLiteral","title":"Descendants"},{"location":"reference/arti/#class-variables_11","text":"Config key_value_sep partition_name_component_sep segment_sep","title":"Class variables"},{"location":"reference/arti/#static-methods_11","text":"","title":"Static methods"},{"location":"reference/arti/#construct_11","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_11","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#get_default_1","text":"def get_default ( ) -> 'Storage[StoragePartition]' View Source @classmethod def get_default ( cls ) -> Storage [ StoragePartition ]: from arti.storage.literal import StringLiteral return StringLiteral () # TODO: Support some sort of configurable defaults.","title":"get_default"},{"location":"reference/arti/#parse_file_11","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_11","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_11","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_11","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_11","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_11","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_11","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_11","text":"fingerprint includes_input_fingerprint_template key_types","title":"Instance variables"},{"location":"reference/arti/#methods_12","text":"","title":"Methods"},{"location":"reference/arti/#copy_11","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_11","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#discover_partitions","text":"def discover_partitions ( self , input_fingerprints : 'InputFingerprints' = {} ) -> 'tuple[StoragePartitionVar_co, ...]' View Source @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartitionVar_co, ... ] : raise NotImplementedError ()","title":"discover_partitions"},{"location":"reference/arti/#generate_partition","text":"def generate_partition ( self , keys : 'CompositeKey' = {}, input_fingerprint : 'Fingerprint' = Fingerprint ( key = None ), with_content_fingerprint : 'bool' = True ) -> 'StoragePartitionVar_co' View Source def generate_partition( self, keys: CompositeKey = CompositeKey(), input_fingerprint: Fingerprint = Fingerprint.empty(), with_content_fingerprint: bool = True, ) -> StoragePartitionVar_co: self._check_keys(self.key_types, keys) format_kwargs = dict[Any, Any](keys) if input_fingerprint.is_empty: if self.includes_input_fingerprint_template: raise ValueError(f\"{self} requires an input_fingerprint, but none was provided\") else: if not self.includes_input_fingerprint_template: raise ValueError(f\"{self} does not specify a {{ input_fingerprint }} template\") format_kwargs[\"input_fingerprint\"] = str(input_fingerprint.key) field_values = { name: ( strip_partition_indexes(original).format(**format_kwargs) if lenient_issubclass(type(original := getattr(self, name)), str) else original ) for name in self.__fields__ if name in self.storage_partition_type.__fields__ } partition = self.storage_partition_type( input_fingerprint=input_fingerprint, keys=keys, **field_values ) if with_content_fingerprint: partition = partition.with_content_fingerprint() return partition","title":"generate_partition"},{"location":"reference/arti/#json_11","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#resolve","text":"def resolve ( self , ** values : 'str' ) -> 'Self' View Source def resolve ( self , ** values : str ) -> Self : return self . copy ( update = { name : new for name , or iginal in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ) . if ( new := self . _resolve_field ( name , or iginal , values )) != or iginal } )","title":"resolve"},{"location":"reference/arti/#storagepartition","text":"class StoragePartition ( __pydantic_self__ , ** data : Any ) View Source class StoragePartition ( Model ) : keys : CompositeKey = CompositeKey () input_fingerprint : Fingerprint = Fingerprint . empty () content_fingerprint : Fingerprint = Fingerprint . empty () def with_content_fingerprint ( self , keep_existing : bool = True ) -> Self : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint () } ) @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" )","title":"StoragePartition"},{"location":"reference/arti/#ancestors-in-mro_12","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants_6","text":"arti.storage.google.cloud.storage.GCSFilePartition arti.storage.local.LocalFilePartition arti.storage.literal.StringLiteralPartition","title":"Descendants"},{"location":"reference/arti/#class-variables_12","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_12","text":"","title":"Static methods"},{"location":"reference/arti/#construct_12","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_12","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_12","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_12","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_12","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_12","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_12","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_12","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_12","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_12","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_13","text":"","title":"Methods"},{"location":"reference/arti/#compute_content_fingerprint","text":"def compute_content_fingerprint ( self ) -> 'Fingerprint' View Source @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" )","title":"compute_content_fingerprint"},{"location":"reference/arti/#copy_12","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_12","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_12","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#with_content_fingerprint","text":"def with_content_fingerprint ( self , keep_existing : 'bool' = True ) -> 'Self' View Source def with_content_fingerprint ( self , keep_existing : bool = True ) -> Self : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()})","title":"with_content_fingerprint"},{"location":"reference/arti/#threshold","text":"class Threshold ( __pydantic_self__ , ** data : Any ) View Source class Threshold ( Model ) : type : ClassVar [ type[Type ] ] def check ( self , value : Any ) -> bool : raise NotImplementedError ()","title":"Threshold"},{"location":"reference/arti/#ancestors-in-mro_13","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#class-variables_13","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_13","text":"","title":"Static methods"},{"location":"reference/arti/#construct_13","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_13","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_13","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_13","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_13","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_13","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_13","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_13","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_13","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_13","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_14","text":"","title":"Methods"},{"location":"reference/arti/#check","text":"def check ( self , value : 'Any' ) -> 'bool' View Source def check ( self , value : Any ) -> bool : raise NotImplementedError ()","title":"check"},{"location":"reference/arti/#copy_13","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_13","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_13","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#type","text":"class Type ( __pydantic_self__ , ** data : Any ) View Source class Type ( Model ) : \"\"\"Type represents a data type.\"\"\" _abstract_ = True # NOTE : Exclude the description to minimize fingerprint changes ( and thus rebuilds ). _fingerprint_excludes_ = frozenset ( [ \"description\" ] ) description : Optional [ str ] nullable : bool = False @property def friendly_key ( self ) -> str : \"\"\"A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. \"\"\" return self . _class_key_","title":"Type"},{"location":"reference/arti/#ancestors-in-mro_14","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants_7","text":"arti.types._Numeric arti.types.Binary arti.types.Boolean arti.types.Date arti.types.DateTime arti.types.Enum arti.types.Geography arti.types.List arti.types.Map arti.types.Null arti.types.Set arti.types.String arti.types.Struct arti.types.Time arti.types.Timestamp","title":"Descendants"},{"location":"reference/arti/#class-variables_14","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_14","text":"","title":"Static methods"},{"location":"reference/arti/#construct_14","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_14","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_14","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_14","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_14","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_14","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_14","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_14","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_14","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_14","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/#methods_15","text":"","title":"Methods"},{"location":"reference/arti/#copy_14","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_14","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_14","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#typeadapter","text":"class TypeAdapter ( / , * args , ** kwargs ) View Source class TypeAdapter : \"\"\"TypeAdapter maps between Artigraph types and a foreign type system.\"\"\" key : ClassVar [ str ] = class_name () artigraph : ClassVar [ type[Type ] ] # The internal Artigraph Type system : ClassVar [ Any ] # The external system ' s type priority : ClassVar [ int ] = 0 # Set the priority of this mapping . Higher is better . @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : raise NotImplementedError () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : raise NotImplementedError () @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : raise NotImplementedError ()","title":"TypeAdapter"},{"location":"reference/arti/#descendants_8","text":"arti.types._ScalarClassTypeAdapter arti.types.python.PyValueContainer arti.types.python.PyLiteral arti.types.python.PyMap arti.types.python.PyOptional arti.types.python.PyStruct arti.types.bigquery._BigQueryTypeAdapter arti.types.bigquery.ListFieldTypeAdapter arti.types.bigquery.TableTypeAdapter arti.types.numpy.ArrayAdapter arti.types.pandas.SeriesAdapter arti.types.pandas.DataFrameAdapter arti.types.pyarrow._PyarrowTypeAdapter arti.types.pydantic.BaseModelAdapter","title":"Descendants"},{"location":"reference/arti/#class-variables_15","text":"key priority","title":"Class variables"},{"location":"reference/arti/#static-methods_15","text":"","title":"Static methods"},{"location":"reference/arti/#matches_artigraph","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/#matches_system","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : raise NotImplementedError ()","title":"matches_system"},{"location":"reference/arti/#to_artigraph","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : raise NotImplementedError ()","title":"to_artigraph"},{"location":"reference/arti/#to_system","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : raise NotImplementedError ()","title":"to_system"},{"location":"reference/arti/#typesystem","text":"class TypeSystem ( __pydantic_self__ , ** data : Any ) View Source class TypeSystem ( Model ) : key : str extends : tuple [ TypeSystem, ... ] = () # NOTE : Use a NoCopyDict to avoid copies of the registry . Otherwise , TypeSystems that extend # this TypeSystem will only see the adapters registered * as of initialization * ( as pydantic # would deepcopy the TypeSystems in the ` extends ` argument ). _adapter_by_key : NoCopyDict [ str, type[TypeAdapter ] ] = PrivateAttr ( default_factory = NoCopyDict ) def register_adapter ( self , adapter : type [ TypeAdapter ] ) -> type [ TypeAdapter ] : return register ( self . _adapter_by_key , adapter . key , adapter ) @property def _priority_sorted_adapters ( self ) -> list [ type[TypeAdapter ] ]: return sorted ( self . _adapter_by_key . values (), key = attrgetter ( \"priority\" ), reverse = True ) def to_artigraph ( self , type_ : Any , * , hints : dict [ str, Any ] , root_type_system : Optional [ TypeSystem ] = None ) -> Type : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ) : return adapter . to_artigraph ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_artigraph ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No {root_type_system} adapter for system type: {type_}.\" ) def to_system ( self , type_ : Type , * , hints : dict [ str, Any ] , root_type_system : Optional [ TypeSystem ] = None ) -> Any : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ) : return adapter . to_system ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_system ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No {root_type_system} adapter for Artigraph type: {type_}.\" )","title":"TypeSystem"},{"location":"reference/arti/#ancestors-in-mro_15","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#class-variables_16","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_16","text":"","title":"Static methods"},{"location":"reference/arti/#construct_15","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_15","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_15","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_15","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_15","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_15","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_15","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_15","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_15","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_15","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_16","text":"","title":"Methods"},{"location":"reference/arti/#copy_15","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_15","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_15","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#register_adapter","text":"def register_adapter ( self , adapter : 'type[TypeAdapter]' ) -> 'type[TypeAdapter]' View Source def register_adapter ( self , adapter : type [ TypeAdapter ] ) -> type [ TypeAdapter ] : return register ( self . _adapter_by_key , adapter . key , adapter )","title":"register_adapter"},{"location":"reference/arti/#to_artigraph_1","text":"def to_artigraph ( self , type_ : 'Any' , * , hints : 'dict[str, Any]' , root_type_system : 'Optional[TypeSystem]' = None ) -> 'Type' View Source def to_artigraph ( self , type_ : Any , * , hints : dict [ str, Any ] , root_type_system : Optional [ TypeSystem ] = None ) -> Type : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ) : return adapter . to_artigraph ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_artigraph ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No {root_type_system} adapter for system type: {type_}.\" )","title":"to_artigraph"},{"location":"reference/arti/#to_system_1","text":"def to_system ( self , type_ : 'Type' , * , hints : 'dict[str, Any]' , root_type_system : 'Optional[TypeSystem]' = None ) -> 'Any' View Source def to_system ( self , type_ : Type , * , hints : dict [ str, Any ] , root_type_system : Optional [ TypeSystem ] = None ) -> Any : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ) : return adapter . to_system ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_system ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No {root_type_system} adapter for Artigraph type: {type_}.\" )","title":"to_system"},{"location":"reference/arti/#version","text":"class Version ( __pydantic_self__ , ** data : Any ) View Source class Version ( Model ): _abstract_ = True","title":"Version"},{"location":"reference/arti/#ancestors-in-mro_16","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants_9","text":"arti.versions.GitCommit arti.versions.SemVer arti.versions.String arti.versions.Timestamp","title":"Descendants"},{"location":"reference/arti/#class-variables_17","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_17","text":"","title":"Static methods"},{"location":"reference/arti/#construct_16","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_16","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_16","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_16","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_16","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_16","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_16","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_16","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_16","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_16","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_17","text":"","title":"Methods"},{"location":"reference/arti/#copy_16","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_16","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_16","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#view","text":"class View ( __pydantic_self__ , ** data : Any ) View Source class View ( Model ) : \"\"\"View represents the in-memory representation of the artifact. Examples include pandas.DataFrame, dask.DataFrame, a BigQuery table. \"\"\" _abstract_ = True _by_python_type_ : ClassVar [ dict[Optional[type ] , type [ View ] ]] = {} priority : ClassVar [ int ] = 0 # Set priority of this view for its python_type . Higher is better . python_type : ClassVar [ Optional[type ] ] type_system : ClassVar [ TypeSystem ] mode : MODE artifact_class : type [ Artifact ] = Artifact type : Type @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if not cls . _abstract_ : register ( cls . _by_python_type_ , cls . python_type , cls , lambda x : x . priority ) @classmethod def _check_type_compatibility ( cls , view_type : Type , artifact_type : Type ) -> None : # TODO : Consider supporting some form of \"reader schema\" where the View 's Type is a subset # of the Artifact' s Type ( and we filter the columns on read ). We could also consider # allowing the Producer 's Type to be a superset of the Artifact' s Type and we 'd filter the # columns on write. # # If implementing, we can leverage the `mode` to determine which should be the \"superset\". if view_type != artifact_type: raise ValueError( f\"the specified Type (`{view_type}`) is not compatible with the Artifact' s Type ( `{ artifact_type }` ). \" ) @validator(\" type \") @classmethod def _validate_type(cls, type_: Type, values: dict[str, Any]) -> Type: artifact_class: Optional[type[Artifact]] = values.get(\" artifact_class \") if artifact_class is None: return type_ # pragma: no cover artifact_type: Optional[Type] = get_field_default(artifact_class, \" type \") if artifact_type is not None: cls._check_type_compatibility(view_type=type_, artifact_type=artifact_type) return type_ @classmethod def _get_kwargs_from_annotation(cls, annotation: Any) -> dict[str, Any]: artifact_class = get_item_from_annotated( annotation, Artifact, is_subclass=True ) or get_field_default(cls, \" artifact_class \") assert artifact_class is not None assert issubclass(artifact_class, Artifact) # Try to extract or infer the Type. We prefer: an explicit Type in the annotation, followed # by an Artifact's default type, falling back to inferring a Type from the type hint. type_ = get_item_from_annotated(annotation, Type, is_subclass=False) if type_ is None: artifact_type: Optional[Type] = get_field_default(artifact_class, \" type \") if artifact_type is None: from arti.types.python import python_type_system type_ = python_type_system.to_artigraph(discard_Annotated(annotation), hints={}) else: type_ = artifact_type # NOTE: We validate that type_ and artifact_type (if set) are compatible in _validate_type, # which will run for *any* instance, not just those created with `.from_annotation`. return {\" artifact_class \": artifact_class, \" type \": type_} @classmethod # TODO: Use typing.Self for return, pending mypy support def get_class_for(cls, annotation: Any) -> builtins.type[View]: view_class = get_item_from_annotated(annotation, cls, is_subclass=True) if view_class is None: # We've already searched for a View instance in the original Annotated args, so just # extract the root annotation. annotation = discard_Annotated(annotation) # Import the View submodules to trigger registration. import_submodules(__path__, __name__) view_class = cls._by_python_type_.get(annotation) # If no match and the type is a subscripted Generic (eg: `list[int]`), try to unwrap any # extra type variables. if view_class is None and (origin := get_origin(annotation)) is not None: view_class = cls._by_python_type_.get(origin) if view_class is None: raise ValueError( f\" { annotation } cannot be matched to a View , try setting one explicitly ( eg : ` Annotated [ int, arti.views.python.Int ] ` ) \" ) return view_class @classmethod # TODO: Use typing.Self for return, pending mypy support def from_annotation(cls, annotation: Any, *, mode: MODE) -> View: view_class = cls.get_class_for(annotation) view = view_class(mode=mode, **cls._get_kwargs_from_annotation(annotation)) view.check_annotation_compatibility(annotation) return view def check_annotation_compatibility(self, annotation: Any) -> None: # We're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \") def check_artifact_compatibility(self, artifact: Artifact) -> None: if not isinstance(artifact, self.artifact_class): raise ValueError(f\" expected an instance of { self . artifact_class } , got { type ( artifact ) } \") self._check_type_compatibility(view_type=self.type, artifact_type=artifact.type) if self.mode in {\" READ \", \" READWRITE \"}: io._read.lookup( type(artifact.type), type(artifact.format), list[artifact.storage.storage_partition_type], # type: ignore[name-defined] type(self), ) if self.mode in {\" WRITE \", \" READWRITE \"}: io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), )","title":"View"},{"location":"reference/arti/#ancestors-in-mro_17","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants_10","text":"arti.views.python.PythonBuiltin","title":"Descendants"},{"location":"reference/arti/#class-variables_18","text":"Config priority","title":"Class variables"},{"location":"reference/arti/#static-methods_18","text":"","title":"Static methods"},{"location":"reference/arti/#construct_17","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_annotation","text":"def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view","title":"from_annotation"},{"location":"reference/arti/#from_orm_17","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#get_class_for_1","text":"def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class","title":"get_class_for"},{"location":"reference/arti/#parse_file_17","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_17","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_17","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_17","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_17","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_17","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_17","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_17","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_18","text":"","title":"Methods"},{"location":"reference/arti/#check_annotation_compatibility","text":"def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \")","title":"check_annotation_compatibility"},{"location":"reference/arti/#check_artifact_compatibility","text":"def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), )","title":"check_artifact_compatibility"},{"location":"reference/arti/#copy_17","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_17","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_17","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/annotations/","text":"Module arti.annotations None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from arti.internal.models import Model class Annotation ( Model ): \"\"\"An Annotation is a piece of human knowledge associated with an Artifact.\"\"\" _abstract_ = True Classes Annotation class Annotation ( __pydantic_self__ , ** data : Any ) View Source class Annotation ( Model ): \"\"\"An Annotation is a piece of human knowledge associated with an Artifact.\"\"\" _abstract_ = True Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Annotations"},{"location":"reference/arti/annotations/#module-artiannotations","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from arti.internal.models import Model class Annotation ( Model ): \"\"\"An Annotation is a piece of human knowledge associated with an Artifact.\"\"\" _abstract_ = True","title":"Module arti.annotations"},{"location":"reference/arti/annotations/#classes","text":"","title":"Classes"},{"location":"reference/arti/annotations/#annotation","text":"class Annotation ( __pydantic_self__ , ** data : Any ) View Source class Annotation ( Model ): \"\"\"An Annotation is a piece of human knowledge associated with an Artifact.\"\"\" _abstract_ = True","title":"Annotation"},{"location":"reference/arti/annotations/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/annotations/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/annotations/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/annotations/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/annotations/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/annotations/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/annotations/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/annotations/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/annotations/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/annotations/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/annotations/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/annotations/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/annotations/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/annotations/#methods","text":"","title":"Methods"},{"location":"reference/arti/annotations/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/annotations/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/annotations/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/artifacts/","text":"Module arti.artifacts None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import json from itertools import chain from typing import TYPE_CHECKING , Any , Optional from pydantic import Field , validator from pydantic.fields import ModelField from arti.annotations import Annotation from arti.formats import Format from arti.internal.models import Model , get_field_default from arti.internal.type_hints import get_annotation_from_value from arti.statistics import Statistic from arti.storage import Storage , StoragePartition from arti.types import Type if TYPE_CHECKING : from arti.producers import ProducerOutput class Artifact ( Model ): \"\"\"An Artifact is the base structure describing an existing or generated dataset. An Artifact is comprised of three key elements: - `type`: spec of the data's structure, such as data types, nullable, etc. - `format`: the data's serialized format, such as CSV, Parquet, database native, etc. - `storage`: the data's persistent storage system, such as blob storage, database native, etc. In addition to the core elements, an Artifact can be tagged with additional `annotations` (to associate it with human knowledge) and `statistics` (to track derived characteristics over time). \"\"\" type : Type format : Format = Field ( default_factory = Format . get_default ) storage : Storage [ StoragePartition ] = Field ( default_factory = Storage . get_default ) annotations : tuple [ Annotation , ... ] = () statistics : tuple [ Statistic , ... ] = () # Hide `producer_output` in repr to prevent showing the entire upstream graph. # # ProducerOutput is a ForwardRef/cyclic import. Quote the entire hint to force full resolution # during `.update_forward_refs`, rather than `Optional[ForwardRef(\"ProducerOutput\")]`. producer_output : Optional [ ProducerOutput ] = Field ( None , repr = False ) # NOTE: Narrow the fields that affect the fingerprint to minimize changes (which trigger # recompute). Importantly, avoid fingerprinting the `.producer_output` (ie: the *upstream* # producer) to prevent cascading fingerprint changes (Producer.fingerprint accesses the *input* # Artifact.fingerprints). Even so, this may still be quite sensitive. _fingerprint_includes_ = frozenset ([ \"type\" , \"format\" , \"storage\" ]) @validator ( \"format\" , always = True ) @classmethod def _validate_format ( cls , format : Format , values : dict [ str , Any ]) -> Format : if ( type_ := values . get ( \"type\" )) is not None : return format . _visit_type ( type_ ) return format @validator ( \"storage\" , always = True ) @classmethod def _validate_storage ( cls , storage : Storage [ StoragePartition ], values : dict [ str , Any ] ) -> Storage [ StoragePartition ]: if ( type_ := values . get ( \"type\" )) is not None : storage = storage . _visit_type ( type_ ) if ( format_ := values . get ( \"format\" )) is not None : storage = storage . _visit_format ( format_ ) return storage @validator ( \"annotations\" , \"statistics\" , always = True , pre = True ) @classmethod def _merge_class_defaults ( cls , value : tuple [ Any , ... ], field : ModelField ) -> tuple [ Any , ... ]: return tuple ( chain ( get_field_default ( cls , field . name ) or (), value )) @classmethod def cast ( cls , value : Any ) -> Artifact : \"\"\"Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `ArtifactBox` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, we attempt to map to a `Type` and return an Artifact instance with defaulted Format and Storage \"\"\" from arti.formats.json import JSON from arti.producers import Producer from arti.storage.literal import StringLiteral from arti.types.python import python_type_system if isinstance ( value , Artifact ): return value if isinstance ( value , Producer ): output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ): return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma: no cover # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( f \" { type ( value ) . __name__ } doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \" { type ( value ) . __name__ } produces { len ( output_artifacts ) } Artifacts. Try assigning each to a new name in the Graph!\" ) annotation = get_annotation_from_value ( value ) return cls ( type = python_type_system . to_artigraph ( annotation , hints = {}), format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), ) Variables TYPE_CHECKING Classes Artifact class Artifact ( __pydantic_self__ , ** data : Any ) View Source class Artifact ( Model ) : \" \"\" An Artifact is the base structure describing an existing or generated dataset. An Artifact is comprised of three key elements: - `type`: spec of the data's structure, such as data types, nullable, etc. - `format`: the data's serialized format, such as CSV, Parquet, database native, etc. - `storage`: the data's persistent storage system, such as blob storage, database native, etc. In addition to the core elements, an Artifact can be tagged with additional `annotations` (to associate it with human knowledge) and `statistics` (to track derived characteristics over time). \"\" \" type : Type format : Format = Field ( default_factory = Format . get_default ) storage : Storage [ StoragePartition ] = Field ( default_factory = Storage . get_default ) annotations : tuple [ Annotation , ... ] = () statistics : tuple [ Statistic , ... ] = () # Hide `producer_output` in repr to prevent showing the entire upstream graph. # # ProducerOutput is a ForwardRef/cyclic import. Quote the entire hint to force full resolution # during `.update_forward_refs`, rather than `Optional[ForwardRef(\"ProducerOutput\")]`. producer_output : Optional [ ProducerOutput ] = Field ( None , repr = False ) # NOTE: Narrow the fields that affect the fingerprint to minimize changes (which trigger # recompute). Importantly, avoid fingerprinting the `.producer_output` (ie: the *upstream* # producer) to prevent cascading fingerprint changes (Producer.fingerprint accesses the *input* # Artifact.fingerprints). Even so, this may still be quite sensitive. _fingerprint_includes_ = frozenset ( [ \"type\" , \"format\" , \"storage\" ] ) @validator ( \"format\" , always = True ) @classmethod def _validate_format ( cls , format : Format , values : dict [ str , Any ] ) -> Format : if ( type_ := values . get ( \"type\" )) is not None : return format . _visit_type ( type_ ) return format @validator ( \"storage\" , always = True ) @classmethod def _validate_storage ( cls , storage : Storage [ StoragePartition ] , values : dict [ str , Any ] ) -> Storage [ StoragePartition ] : if ( type_ := values . get ( \"type\" )) is not None : storage = storage . _visit_type ( type_ ) if ( format_ := values . get ( \"format\" )) is not None : storage = storage . _visit_format ( format_ ) return storage @validator ( \"annotations\" , \"statistics\" , always = True , pre = True ) @classmethod def _merge_class_defaults ( cls , value : tuple [ Any , ... ] , field : ModelField ) -> tuple [ Any , ... ] : return tuple ( chain ( get_field_default ( cls , field . name ) or (), value )) @classmethod def cast ( cls , value : Any ) -> Artifact : \" \"\" Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `ArtifactBox` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, we attempt to map to a `Type` and return an Artifact instance with defaulted Format and Storage \"\" \" from arti . formats . json import JSON from arti . producers import Producer from arti . storage . literal import StringLiteral from arti . types . python import python_type_system if isinstance ( value , Artifact ) : return value if isinstance ( value , Producer ) : output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ) : return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma: no cover # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( f \"{type(value).__name__} doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \"{type(value).__name__} produces {len(output_artifacts)} Artifacts. Try assigning each to a new name in the Graph!\" ) annotation = get_annotation_from_value ( value ) return cls ( type = python_type_system . to_artigraph ( annotation , hints = {} ), format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods cast def cast ( value : 'Any' ) -> 'Artifact' Attempt to convert an arbitrary value to an appropriate Artifact instance. Artifact.cast is used to convert values assigned to an ArtifactBox (such as Graph.artifacts ) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, we attempt to map to a Type and return an Artifact instance with defaulted Format and Storage View Source @classmethod def cast ( cls , value : Any ) -> Artifact : \"\"\"Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `ArtifactBox` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, we attempt to map to a `Type` and return an Artifact instance with defaulted Format and Storage \"\"\" from arti.formats.json import JSON from arti.producers import Producer from arti.storage.literal import StringLiteral from arti.types.python import python_type_system if isinstance ( value , Artifact ): return value if isinstance ( value , Producer ): output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ): return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma: no cover # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( f \" { type ( value ) . __name__ } doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \" { type ( value ) . __name__ } produces { len ( output_artifacts ) } Artifacts. Try assigning each to a new name in the Graph!\" ) annotation = get_annotation_from_value ( value ) return cls ( type = python_type_system . to_artigraph ( annotation , hints = {}), format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), ) construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Artifacts"},{"location":"reference/arti/artifacts/#module-artiartifacts","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import json from itertools import chain from typing import TYPE_CHECKING , Any , Optional from pydantic import Field , validator from pydantic.fields import ModelField from arti.annotations import Annotation from arti.formats import Format from arti.internal.models import Model , get_field_default from arti.internal.type_hints import get_annotation_from_value from arti.statistics import Statistic from arti.storage import Storage , StoragePartition from arti.types import Type if TYPE_CHECKING : from arti.producers import ProducerOutput class Artifact ( Model ): \"\"\"An Artifact is the base structure describing an existing or generated dataset. An Artifact is comprised of three key elements: - `type`: spec of the data's structure, such as data types, nullable, etc. - `format`: the data's serialized format, such as CSV, Parquet, database native, etc. - `storage`: the data's persistent storage system, such as blob storage, database native, etc. In addition to the core elements, an Artifact can be tagged with additional `annotations` (to associate it with human knowledge) and `statistics` (to track derived characteristics over time). \"\"\" type : Type format : Format = Field ( default_factory = Format . get_default ) storage : Storage [ StoragePartition ] = Field ( default_factory = Storage . get_default ) annotations : tuple [ Annotation , ... ] = () statistics : tuple [ Statistic , ... ] = () # Hide `producer_output` in repr to prevent showing the entire upstream graph. # # ProducerOutput is a ForwardRef/cyclic import. Quote the entire hint to force full resolution # during `.update_forward_refs`, rather than `Optional[ForwardRef(\"ProducerOutput\")]`. producer_output : Optional [ ProducerOutput ] = Field ( None , repr = False ) # NOTE: Narrow the fields that affect the fingerprint to minimize changes (which trigger # recompute). Importantly, avoid fingerprinting the `.producer_output` (ie: the *upstream* # producer) to prevent cascading fingerprint changes (Producer.fingerprint accesses the *input* # Artifact.fingerprints). Even so, this may still be quite sensitive. _fingerprint_includes_ = frozenset ([ \"type\" , \"format\" , \"storage\" ]) @validator ( \"format\" , always = True ) @classmethod def _validate_format ( cls , format : Format , values : dict [ str , Any ]) -> Format : if ( type_ := values . get ( \"type\" )) is not None : return format . _visit_type ( type_ ) return format @validator ( \"storage\" , always = True ) @classmethod def _validate_storage ( cls , storage : Storage [ StoragePartition ], values : dict [ str , Any ] ) -> Storage [ StoragePartition ]: if ( type_ := values . get ( \"type\" )) is not None : storage = storage . _visit_type ( type_ ) if ( format_ := values . get ( \"format\" )) is not None : storage = storage . _visit_format ( format_ ) return storage @validator ( \"annotations\" , \"statistics\" , always = True , pre = True ) @classmethod def _merge_class_defaults ( cls , value : tuple [ Any , ... ], field : ModelField ) -> tuple [ Any , ... ]: return tuple ( chain ( get_field_default ( cls , field . name ) or (), value )) @classmethod def cast ( cls , value : Any ) -> Artifact : \"\"\"Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `ArtifactBox` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, we attempt to map to a `Type` and return an Artifact instance with defaulted Format and Storage \"\"\" from arti.formats.json import JSON from arti.producers import Producer from arti.storage.literal import StringLiteral from arti.types.python import python_type_system if isinstance ( value , Artifact ): return value if isinstance ( value , Producer ): output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ): return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma: no cover # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( f \" { type ( value ) . __name__ } doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \" { type ( value ) . __name__ } produces { len ( output_artifacts ) } Artifacts. Try assigning each to a new name in the Graph!\" ) annotation = get_annotation_from_value ( value ) return cls ( type = python_type_system . to_artigraph ( annotation , hints = {}), format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), )","title":"Module arti.artifacts"},{"location":"reference/arti/artifacts/#variables","text":"TYPE_CHECKING","title":"Variables"},{"location":"reference/arti/artifacts/#classes","text":"","title":"Classes"},{"location":"reference/arti/artifacts/#artifact","text":"class Artifact ( __pydantic_self__ , ** data : Any ) View Source class Artifact ( Model ) : \" \"\" An Artifact is the base structure describing an existing or generated dataset. An Artifact is comprised of three key elements: - `type`: spec of the data's structure, such as data types, nullable, etc. - `format`: the data's serialized format, such as CSV, Parquet, database native, etc. - `storage`: the data's persistent storage system, such as blob storage, database native, etc. In addition to the core elements, an Artifact can be tagged with additional `annotations` (to associate it with human knowledge) and `statistics` (to track derived characteristics over time). \"\" \" type : Type format : Format = Field ( default_factory = Format . get_default ) storage : Storage [ StoragePartition ] = Field ( default_factory = Storage . get_default ) annotations : tuple [ Annotation , ... ] = () statistics : tuple [ Statistic , ... ] = () # Hide `producer_output` in repr to prevent showing the entire upstream graph. # # ProducerOutput is a ForwardRef/cyclic import. Quote the entire hint to force full resolution # during `.update_forward_refs`, rather than `Optional[ForwardRef(\"ProducerOutput\")]`. producer_output : Optional [ ProducerOutput ] = Field ( None , repr = False ) # NOTE: Narrow the fields that affect the fingerprint to minimize changes (which trigger # recompute). Importantly, avoid fingerprinting the `.producer_output` (ie: the *upstream* # producer) to prevent cascading fingerprint changes (Producer.fingerprint accesses the *input* # Artifact.fingerprints). Even so, this may still be quite sensitive. _fingerprint_includes_ = frozenset ( [ \"type\" , \"format\" , \"storage\" ] ) @validator ( \"format\" , always = True ) @classmethod def _validate_format ( cls , format : Format , values : dict [ str , Any ] ) -> Format : if ( type_ := values . get ( \"type\" )) is not None : return format . _visit_type ( type_ ) return format @validator ( \"storage\" , always = True ) @classmethod def _validate_storage ( cls , storage : Storage [ StoragePartition ] , values : dict [ str , Any ] ) -> Storage [ StoragePartition ] : if ( type_ := values . get ( \"type\" )) is not None : storage = storage . _visit_type ( type_ ) if ( format_ := values . get ( \"format\" )) is not None : storage = storage . _visit_format ( format_ ) return storage @validator ( \"annotations\" , \"statistics\" , always = True , pre = True ) @classmethod def _merge_class_defaults ( cls , value : tuple [ Any , ... ] , field : ModelField ) -> tuple [ Any , ... ] : return tuple ( chain ( get_field_default ( cls , field . name ) or (), value )) @classmethod def cast ( cls , value : Any ) -> Artifact : \" \"\" Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `ArtifactBox` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, we attempt to map to a `Type` and return an Artifact instance with defaulted Format and Storage \"\" \" from arti . formats . json import JSON from arti . producers import Producer from arti . storage . literal import StringLiteral from arti . types . python import python_type_system if isinstance ( value , Artifact ) : return value if isinstance ( value , Producer ) : output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ) : return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma: no cover # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( f \"{type(value).__name__} doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \"{type(value).__name__} produces {len(output_artifacts)} Artifacts. Try assigning each to a new name in the Graph!\" ) annotation = get_annotation_from_value ( value ) return cls ( type = python_type_system . to_artigraph ( annotation , hints = {} ), format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), )","title":"Artifact"},{"location":"reference/arti/artifacts/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/artifacts/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/artifacts/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/artifacts/#cast","text":"def cast ( value : 'Any' ) -> 'Artifact' Attempt to convert an arbitrary value to an appropriate Artifact instance. Artifact.cast is used to convert values assigned to an ArtifactBox (such as Graph.artifacts ) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, we attempt to map to a Type and return an Artifact instance with defaulted Format and Storage View Source @classmethod def cast ( cls , value : Any ) -> Artifact : \"\"\"Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `ArtifactBox` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, we attempt to map to a `Type` and return an Artifact instance with defaulted Format and Storage \"\"\" from arti.formats.json import JSON from arti.producers import Producer from arti.storage.literal import StringLiteral from arti.types.python import python_type_system if isinstance ( value , Artifact ): return value if isinstance ( value , Producer ): output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ): return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma: no cover # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( f \" { type ( value ) . __name__ } doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \" { type ( value ) . __name__ } produces { len ( output_artifacts ) } Artifacts. Try assigning each to a new name in the Graph!\" ) annotation = get_annotation_from_value ( value ) return cls ( type = python_type_system . to_artigraph ( annotation , hints = {}), format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), )","title":"cast"},{"location":"reference/arti/artifacts/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/artifacts/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/artifacts/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/artifacts/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/artifacts/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/artifacts/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/artifacts/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/artifacts/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/artifacts/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/artifacts/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/artifacts/#methods","text":"","title":"Methods"},{"location":"reference/arti/artifacts/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/artifacts/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/artifacts/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/fingerprints/","text":"Module arti.fingerprints None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import operator from collections.abc import Callable from functools import reduce from typing import Optional , Union import farmhash from arti.internal.models import Model from arti.internal.utils import int64 , uint64 def _gen_fingerprint_binop ( op : Callable [[ int , int ], int ] ) -> Callable [[ Fingerprint , Union [ int , Fingerprint ]], Fingerprint ]: def _fingerprint_binop ( self : Fingerprint , other : Union [ int , Fingerprint ]) -> Fingerprint : if isinstance ( other , int ): other = Fingerprint . from_int ( other ) if isinstance ( other , Fingerprint ): if self . key is None or other . key is None : return Fingerprint . empty () return Fingerprint ( key = op ( self . key , other . key )) return NotImplemented return _fingerprint_binop class Fingerprint ( Model ): \"\"\"Fingerprint represents a unique identity as an int64 value. Using an int(64) has a number of convenient properties: - can be combined independent of order with XOR - can be stored relatively cheaply - empty 0 values drop out when combined (5 ^ 0 = 5) - is relatively cross-platform (across databases, languages, etc) There are two \"special\" Fingerprints w/ factory functions that, when combined with other Fingerprints: - `empty()`: returns `empty()` - `identity()`: return the other Fingerprint \"\"\" key : Optional [ int64 ] def combine ( self , * others : Fingerprint ) -> Fingerprint : return reduce ( operator . xor , others , self ) @classmethod def empty ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None ) @classmethod def from_int ( cls , x : int , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x )) @classmethod def from_int64 ( cls , x : int64 , / ) -> Fingerprint : return cls ( key = x ) @classmethod def from_string ( cls , x : str , / ) -> Fingerprint : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x ))) @classmethod def from_uint64 ( cls , x : uint64 , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x )) @classmethod def identity ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 )) @property def is_empty ( self ) -> bool : return self . key is None @property def is_identity ( self ) -> bool : return self . key == 0 __and__ = _gen_fingerprint_binop ( operator . __and__ ) __lshift__ = _gen_fingerprint_binop ( operator . __lshift__ ) __or__ = _gen_fingerprint_binop ( operator . __or__ ) __rshift__ = _gen_fingerprint_binop ( operator . __rshift__ ) __xor__ = _gen_fingerprint_binop ( operator . __xor__ ) def __eq__ ( self , other : object ) -> bool : if isinstance ( other , int ): other = Fingerprint . from_int ( other ) if isinstance ( other , Fingerprint ): return self . key == other . key return NotImplemented Classes Fingerprint class Fingerprint ( __pydantic_self__ , ** data : Any ) View Source class Fingerprint ( Model ) : \"\"\"Fingerprint represents a unique identity as an int64 value. Using an int(64) has a number of convenient properties: - can be combined independent of order with XOR - can be stored relatively cheaply - empty 0 values drop out when combined (5 ^ 0 = 5) - is relatively cross-platform (across databases, languages, etc) There are two \" special \" Fingerprints w/ factory functions that, when combined with other Fingerprints: - `empty()`: returns `empty()` - `identity()`: return the other Fingerprint \"\"\" key : Optional [ int64 ] def combine ( self , * others : Fingerprint ) -> Fingerprint : return reduce ( operator . xor , others , self ) @classmethod def empty ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None ) @classmethod def from_int ( cls , x : int , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x )) @classmethod def from_int64 ( cls , x : int64 , / ) -> Fingerprint : return cls ( key = x ) @classmethod def from_string ( cls , x : str , / ) -> Fingerprint : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x ))) @classmethod def from_uint64 ( cls , x : uint64 , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x )) @classmethod def identity ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 )) @property def is_empty ( self ) -> bool : return self . key is None @property def is_identity ( self ) -> bool : return self . key == 0 __and__ = _gen_fingerprint_binop ( operator . __and__ ) __lshift__ = _gen_fingerprint_binop ( operator . __lshift__ ) __or__ = _gen_fingerprint_binop ( operator . __or__ ) __rshift__ = _gen_fingerprint_binop ( operator . __rshift__ ) __xor__ = _gen_fingerprint_binop ( operator . __xor__ ) def __eq__ ( self , other : object ) -> bool : if isinstance ( other , int ) : other = Fingerprint . from_int ( other ) if isinstance ( other , Fingerprint ) : return self . key == other . key return NotImplemented Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values empty def empty ( ) -> 'Fingerprint' Return a Fingerprint that, when combined, will return Fingerprint.empty() View Source @classmethod def empty ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None ) from_int def from_int ( x : 'int' , / ) -> 'Fingerprint' View Source @classmethod def from_int ( cls , x : int , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x )) from_int64 def from_int64 ( x : 'int64' , / ) -> 'Fingerprint' View Source @classmethod def from_int64 ( cls , x : int64 , / ) -> Fingerprint : return cls ( key = x ) from_orm def from_orm ( obj : Any ) -> 'Model' from_string def from_string ( x : 'str' , / ) -> 'Fingerprint' Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. View Source @classmethod def from_string ( cls , x : str , / ) -> Fingerprint : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x ))) from_uint64 def from_uint64 ( x : 'uint64' , / ) -> 'Fingerprint' View Source @classmethod def from_uint64 ( cls , x : uint64 , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x )) identity def identity ( ) -> 'Fingerprint' Return a Fingerprint that, when combined, will return the other Fingerprint. View Source @classmethod def identity ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 )) parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint is_empty is_identity Methods combine def combine ( self , * others : 'Fingerprint' ) -> 'Fingerprint' View Source def combine ( self , * others : Fingerprint ) -> Fingerprint : return reduce ( operator . xor , others , self ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Fingerprints"},{"location":"reference/arti/fingerprints/#module-artifingerprints","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import operator from collections.abc import Callable from functools import reduce from typing import Optional , Union import farmhash from arti.internal.models import Model from arti.internal.utils import int64 , uint64 def _gen_fingerprint_binop ( op : Callable [[ int , int ], int ] ) -> Callable [[ Fingerprint , Union [ int , Fingerprint ]], Fingerprint ]: def _fingerprint_binop ( self : Fingerprint , other : Union [ int , Fingerprint ]) -> Fingerprint : if isinstance ( other , int ): other = Fingerprint . from_int ( other ) if isinstance ( other , Fingerprint ): if self . key is None or other . key is None : return Fingerprint . empty () return Fingerprint ( key = op ( self . key , other . key )) return NotImplemented return _fingerprint_binop class Fingerprint ( Model ): \"\"\"Fingerprint represents a unique identity as an int64 value. Using an int(64) has a number of convenient properties: - can be combined independent of order with XOR - can be stored relatively cheaply - empty 0 values drop out when combined (5 ^ 0 = 5) - is relatively cross-platform (across databases, languages, etc) There are two \"special\" Fingerprints w/ factory functions that, when combined with other Fingerprints: - `empty()`: returns `empty()` - `identity()`: return the other Fingerprint \"\"\" key : Optional [ int64 ] def combine ( self , * others : Fingerprint ) -> Fingerprint : return reduce ( operator . xor , others , self ) @classmethod def empty ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None ) @classmethod def from_int ( cls , x : int , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x )) @classmethod def from_int64 ( cls , x : int64 , / ) -> Fingerprint : return cls ( key = x ) @classmethod def from_string ( cls , x : str , / ) -> Fingerprint : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x ))) @classmethod def from_uint64 ( cls , x : uint64 , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x )) @classmethod def identity ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 )) @property def is_empty ( self ) -> bool : return self . key is None @property def is_identity ( self ) -> bool : return self . key == 0 __and__ = _gen_fingerprint_binop ( operator . __and__ ) __lshift__ = _gen_fingerprint_binop ( operator . __lshift__ ) __or__ = _gen_fingerprint_binop ( operator . __or__ ) __rshift__ = _gen_fingerprint_binop ( operator . __rshift__ ) __xor__ = _gen_fingerprint_binop ( operator . __xor__ ) def __eq__ ( self , other : object ) -> bool : if isinstance ( other , int ): other = Fingerprint . from_int ( other ) if isinstance ( other , Fingerprint ): return self . key == other . key return NotImplemented","title":"Module arti.fingerprints"},{"location":"reference/arti/fingerprints/#classes","text":"","title":"Classes"},{"location":"reference/arti/fingerprints/#fingerprint","text":"class Fingerprint ( __pydantic_self__ , ** data : Any ) View Source class Fingerprint ( Model ) : \"\"\"Fingerprint represents a unique identity as an int64 value. Using an int(64) has a number of convenient properties: - can be combined independent of order with XOR - can be stored relatively cheaply - empty 0 values drop out when combined (5 ^ 0 = 5) - is relatively cross-platform (across databases, languages, etc) There are two \" special \" Fingerprints w/ factory functions that, when combined with other Fingerprints: - `empty()`: returns `empty()` - `identity()`: return the other Fingerprint \"\"\" key : Optional [ int64 ] def combine ( self , * others : Fingerprint ) -> Fingerprint : return reduce ( operator . xor , others , self ) @classmethod def empty ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None ) @classmethod def from_int ( cls , x : int , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x )) @classmethod def from_int64 ( cls , x : int64 , / ) -> Fingerprint : return cls ( key = x ) @classmethod def from_string ( cls , x : str , / ) -> Fingerprint : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x ))) @classmethod def from_uint64 ( cls , x : uint64 , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x )) @classmethod def identity ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 )) @property def is_empty ( self ) -> bool : return self . key is None @property def is_identity ( self ) -> bool : return self . key == 0 __and__ = _gen_fingerprint_binop ( operator . __and__ ) __lshift__ = _gen_fingerprint_binop ( operator . __lshift__ ) __or__ = _gen_fingerprint_binop ( operator . __or__ ) __rshift__ = _gen_fingerprint_binop ( operator . __rshift__ ) __xor__ = _gen_fingerprint_binop ( operator . __xor__ ) def __eq__ ( self , other : object ) -> bool : if isinstance ( other , int ) : other = Fingerprint . from_int ( other ) if isinstance ( other , Fingerprint ) : return self . key == other . key return NotImplemented","title":"Fingerprint"},{"location":"reference/arti/fingerprints/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/fingerprints/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/fingerprints/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/fingerprints/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/fingerprints/#empty","text":"def empty ( ) -> 'Fingerprint' Return a Fingerprint that, when combined, will return Fingerprint.empty() View Source @classmethod def empty ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None )","title":"empty"},{"location":"reference/arti/fingerprints/#from_int","text":"def from_int ( x : 'int' , / ) -> 'Fingerprint' View Source @classmethod def from_int ( cls , x : int , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x ))","title":"from_int"},{"location":"reference/arti/fingerprints/#from_int64","text":"def from_int64 ( x : 'int64' , / ) -> 'Fingerprint' View Source @classmethod def from_int64 ( cls , x : int64 , / ) -> Fingerprint : return cls ( key = x )","title":"from_int64"},{"location":"reference/arti/fingerprints/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/fingerprints/#from_string","text":"def from_string ( x : 'str' , / ) -> 'Fingerprint' Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. View Source @classmethod def from_string ( cls , x : str , / ) -> Fingerprint : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x )))","title":"from_string"},{"location":"reference/arti/fingerprints/#from_uint64","text":"def from_uint64 ( x : 'uint64' , / ) -> 'Fingerprint' View Source @classmethod def from_uint64 ( cls , x : uint64 , / ) -> Fingerprint : return cls . from_int64 ( int64 ( x ))","title":"from_uint64"},{"location":"reference/arti/fingerprints/#identity","text":"def identity ( ) -> 'Fingerprint' Return a Fingerprint that, when combined, will return the other Fingerprint. View Source @classmethod def identity ( cls ) -> Fingerprint : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 ))","title":"identity"},{"location":"reference/arti/fingerprints/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/fingerprints/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/fingerprints/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/fingerprints/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/fingerprints/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/fingerprints/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/fingerprints/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/fingerprints/#instance-variables","text":"fingerprint is_empty is_identity","title":"Instance variables"},{"location":"reference/arti/fingerprints/#methods","text":"","title":"Methods"},{"location":"reference/arti/fingerprints/#combine","text":"def combine ( self , * others : 'Fingerprint' ) -> 'Fingerprint' View Source def combine ( self , * others : Fingerprint ) -> Fingerprint : return reduce ( operator . xor , others , self )","title":"combine"},{"location":"reference/arti/fingerprints/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/fingerprints/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/fingerprints/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/graphs/","text":"Module arti.graphs None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from collections import defaultdict from collections.abc import Callable , Sequence from functools import cached_property , wraps from graphlib import TopologicalSorter from types import TracebackType from typing import TYPE_CHECKING , Any , Literal , Optional , TypeVar , Union from pydantic import Field , PrivateAttr , validator import arti from arti import io from arti.artifacts import Artifact from arti.backends import Backend , Connection from arti.fingerprints import Fingerprint from arti.internal.models import Model from arti.internal.utils import TypedBox , frozendict from arti.partitions import CompositeKey from arti.producers import Producer from arti.storage import StoragePartition , StoragePartitions from arti.types import is_partitioned from arti.views import View if TYPE_CHECKING : from arti.backends.memory import MemoryBackend from arti.executors import Executor else : from arti.internal.patches import patch_TopologicalSorter_class_getitem patch_TopologicalSorter_class_getitem () def _get_memory_backend () -> MemoryBackend : # Avoid importing non-root modules upon import from arti.backends.memory import MemoryBackend return MemoryBackend () SEALED : Literal [ True ] = True OPEN : Literal [ False ] = False BOX_KWARGS = { status : { \"box_dots\" : True , \"default_box\" : status is OPEN , \"frozen_box\" : status is SEALED , } for status in ( OPEN , SEALED ) } _Return = TypeVar ( \"_Return\" ) def requires_sealed ( fn : Callable [ ... , _Return ]) -> Callable [ ... , _Return ]: @wraps ( fn ) def check_if_sealed ( self : Graph , * args : Any , ** kwargs : Any ) -> _Return : if self . _status is not SEALED : raise ValueError ( f \" { fn . __name__ } cannot be used while the Graph is still being defined\" ) return fn ( self , * args , ** kwargs ) return check_if_sealed class ArtifactBox ( TypedBox [ Artifact ]): def _TypedBox__cast_value ( self , item : str , value : Any ) -> Artifact : artifact : Artifact = super () . _TypedBox__cast_value ( item , value ) # type: ignore[misc] storage = artifact . storage if ( graph := arti . context . graph ) is not None : storage = storage . _visit_graph ( graph ) . _visit_names ( ( * self . _box_config [ \"box_namespace\" ], item ) ) # Require an {input_fingerprint} template in the Storage if this Artifact is being generated # by a Producer. Otherwise, strip the {input_fingerprint} template (if set) for \"raw\" # Artifacts. # # We can't validate this at Artifact instantiation because the Producer is tracked later (by # copying the instance and setting the `producer_output` attribute). We won't know the # \"final\" instance until assignment here to the Graph. if artifact . producer_output is None : storage = storage . _visit_input_fingerprint ( Fingerprint . empty ()) elif not artifact . storage . includes_input_fingerprint_template : raise ValueError ( \"Produced Artifacts must have a ' {input_fingerprint} ' template in their Storage\" ) return artifact . copy ( update = { \"storage\" : storage }) Node = Union [ Artifact , Producer ] NodeDependencies = frozendict [ Node , frozenset [ Node ]] class Graph ( Model ): \"\"\"Graph stores a web of Artifacts connected by Producers.\"\"\" name : str artifacts : ArtifactBox = Field ( default_factory = lambda : ArtifactBox ( ** BOX_KWARGS [ SEALED ])) # The Backend *itself* should not affect the results of a Graph build, though the contents # certainly may (eg: stored annotations), so we avoid serializing it. This also prevent # embedding any credentials. backend : Backend [ Connection ] = Field ( default_factory = _get_memory_backend , exclude = True ) path_tags : frozendict [ str , str ] = frozendict () # Graph starts off sealed, but is opened within a `with Graph(...)` context _status : Optional [ bool ] = PrivateAttr ( None ) _artifact_to_key : frozendict [ Artifact , str ] = PrivateAttr ( frozendict ()) @validator ( \"artifacts\" ) @classmethod def _convert_artifacts ( cls , artifacts : ArtifactBox ) -> ArtifactBox : return ArtifactBox ( artifacts , ** BOX_KWARGS [ SEALED ]) def __enter__ ( self ) -> Graph : if arti . context . graph is not None : raise ValueError ( f \"Another graph is being defined: { arti . context . graph } \" ) arti . context . graph = self self . _toggle ( OPEN ) return self def __exit__ ( self , exc_type : Optional [ type [ BaseException ]], exc_value : Optional [ BaseException ], exc_traceback : Optional [ TracebackType ], ) -> None : arti . context . graph = None self . _toggle ( SEALED ) # Confirm the dependencies are acyclic TopologicalSorter ( self . dependencies ) . prepare () def _toggle ( self , status : bool ) -> None : # The Graph object is \"frozen\", so we must bypass the assignment checks. object . __setattr__ ( self , \"artifacts\" , ArtifactBox ( self . artifacts , ** BOX_KWARGS [ status ])) self . _status = status self . _artifact_to_key = frozendict ( { artifact : key for key , artifact in self . artifacts . walk ()} ) @property def artifact_to_key ( self ) -> frozendict [ Artifact , str ]: return self . _artifact_to_key @requires_sealed def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : return self . snapshot () . build ( executor ) @requires_sealed def snapshot ( self , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Identify a \"unique\" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" return GraphSnapshot . from_graph ( self , connection = connection ) @cached_property @requires_sealed def dependencies ( self ) -> NodeDependencies : artifact_deps = { artifact : ( frozenset ({ artifact . producer_output . producer }) if artifact . producer_output is not None else frozenset () ) for _ , artifact in self . artifacts . walk () } producer_deps = { # NOTE: multi-output Producers will appear multiple times (but be deduped) producer_output . producer : frozenset ( producer_output . producer . inputs . values ()) for artifact in artifact_deps if ( producer_output := artifact . producer_output ) is not None } return NodeDependencies ( artifact_deps | producer_deps ) @cached_property @requires_sealed def producers ( self ) -> frozenset [ Producer ]: return frozenset ( self . producer_outputs ) @cached_property @requires_sealed def producer_outputs ( self ) -> frozendict [ Producer , tuple [ Artifact , ... ]]: d = defaultdict [ Producer , dict [ int , Artifact ]]( dict ) for _ , artifact in self . artifacts . walk (): if artifact . producer_output is None : continue output = artifact . producer_output d [ output . producer ][ output . position ] = artifact return frozendict ( ( producer , tuple ( artifacts_by_position [ i ] for i in sorted ( artifacts_by_position ))) for producer , artifacts_by_position in d . items () ) # TODO: io.read/write probably need a bit of sanity checking (probably somewhere else), eg: type # ~= view. Doing validation on the data, etc. Should some of this live on the View? @requires_sealed def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence [ StoragePartition ]] = None , view : Optional [ View ] = None , snapshot : Optional [ GraphSnapshot ] = None , connection : Optional [ Connection ] = None , ) -> Any : key = self . artifact_to_key [ artifact ] if annotation is None and view is None : raise ValueError ( \"Either `annotation` or `view` must be passed\" ) if annotation is not None and view is not None : raise ValueError ( \"Only one of `annotation` or `view` may be passed\" ) if annotation is not None : view = View . get_class_for ( annotation )( artifact_class = type ( artifact ), type = artifact . type , mode = \"READ\" ) view . check_annotation_compatibility ( annotation ) view . check_artifact_compatibility ( artifact ) assert view is not None # mypy gets mixed up with ^ if storage_partitions is None : # We want to allow reading raw Artifacts even if other raw Artifacts are missing (which # prevents snapshotting). if snapshot is None and artifact . producer_output is None : # NOTE: We're not using read_artifact_partitions as the underlying data may have # changed. The backend may have pointers to old versions (which is expected), but we # only want to return the current values. storage_partitions = artifact . storage . discover_partitions () else : snapshot = snapshot or self . snapshot () with ( connection or self . backend ) . connect () as conn : storage_partitions = conn . read_snapshot_partitions ( snapshot , key , artifact ) return io . read ( type_ = artifact . type , format = artifact . format , storage_partitions = storage_partitions , view = view , ) @requires_sealed def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , snapshot : Optional [ GraphSnapshot ] = None , connection : Optional [ Connection ] = None , ) -> StoragePartition : key = self . artifact_to_key [ artifact ] if snapshot is not None and artifact . producer_output is None : raise ValueError ( f \"Writing to a raw Artifact (` { key } `) with a GraphSnapshot is not supported.\" ) if view is None : view = View . get_class_for ( type ( data ))( artifact_class = type ( artifact ), type = artifact . type , mode = \"WRITE\" ) view . check_annotation_compatibility ( type ( data )) view . check_artifact_compatibility ( artifact ) storage_partition = artifact . storage . generate_partition ( input_fingerprint = input_fingerprint , keys = keys , with_content_fingerprint = False ) storage_partition = io . write ( data , type_ = artifact . type , format = artifact . format , storage_partition = storage_partition , view = view , ) . with_content_fingerprint () # TODO: Should we only do this in bulk? We might want the backends to transparently batch # requests, but that's not so friendly with the transient \".connect\". with ( connection or self . backend ) . connect () as conn : conn . write_artifact_partitions ( artifact , ( storage_partition ,)) # Skip linking this partition to the snapshot if it affects raw Artifacts (which would # trigger an id change). if snapshot is not None and artifact . producer_output is not None : conn . write_snapshot_partitions ( snapshot , key , artifact , ( storage_partition ,)) return storage_partition class GraphSnapshot ( Model ): \"\"\"GraphSnapshot represents the state of a Graph and the referenced raw data at a point in time. GraphSnapshot encodes the structure of the Graph plus a snapshot of the raw Artifact data (partition kinds and contents) at a point in time. Any change that would affect data should prompt an ID change. \"\"\" id : Fingerprint graph : Graph @property def artifacts ( self ) -> ArtifactBox : return self . graph . artifacts @property def backend ( self ) -> Backend [ Connection ]: return self . graph . backend @property def name ( self ) -> str : return self . graph . name @classmethod # TODO: Should this use a (TTL) cache? Raw data changes (especially in tests) still need to be detected. def from_graph ( cls , graph : Graph , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Snapshot the Graph and all existing raw data. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifact data during Producer builds. \"\"\" # TODO: Resolve and statically set all available fingerprints. Specifically, we should pin # the Producer.fingerprint, which may by dynamic (eg: version is a Timestamp). Unbuilt # Artifact (partitions) won't be fully resolved yet. snapshot_id , known_artifact_partitions = graph . fingerprint , dict [ str , StoragePartitions ]() for node , _ in graph . dependencies . items (): snapshot_id = snapshot_id . combine ( node . fingerprint ) if isinstance ( node , Artifact ): key = graph . artifact_to_key [ node ] snapshot_id = snapshot_id . combine ( Fingerprint . from_string ( key )) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we'll have to handle things a bit # differently depending on if the external Artifacts are Produced (in an upstream # Graph) or not. if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . storage . discover_partitions () ) if not known_artifact_partitions [ key ]: content_str = \"partitions\" if is_partitioned ( node . type ) else \"data\" raise ValueError ( f \"No { content_str } found for ` { key } `: { node } \" ) snapshot_id = snapshot_id . combine ( * [ partition . fingerprint for partition in known_artifact_partitions [ key ]] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma: no cover # NOTE: This shouldn't happen unless the logic above is faulty. raise ValueError ( \"Fingerprint is empty!\" ) snapshot = cls ( graph = graph , id = snapshot_id ) # Write the discovered partitions (if not already known) and link to this new snapshot. with ( connection or snapshot . backend ) . connect () as conn : conn . write_graph ( graph ) conn . write_snapshot ( snapshot ) for key , partitions in known_artifact_partitions . items (): conn . write_artifact_and_graph_partitions ( snapshot , key , snapshot . artifacts [ key ], partitions ) return snapshot def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : if executor is None : from arti.executors.local import LocalExecutor executor = LocalExecutor () executor . build ( self ) return self def tag ( self , tag : str , * , overwrite : bool = False , connection : Optional [ Connection ] = None ) -> None : with ( connection or self . backend ) . connect () as conn : conn . write_snapshot_tag ( self , tag , overwrite ) @classmethod def from_tag ( cls , name : str , tag : str , * , connectable : Union [ Backend [ Connection ], Connection ] ) -> GraphSnapshot : with connectable . connect () as conn : return conn . read_snapshot_tag ( name , tag ) def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence [ StoragePartition ]] = None , view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> Any : return self . graph . read ( artifact , annotation = annotation , storage_partitions = storage_partitions , view = view , snapshot = self , connection = connection , ) def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> StoragePartition : return self . graph . write ( data , artifact = artifact , input_fingerprint = input_fingerprint , keys = keys , view = view , snapshot = self , connection = connection , ) Variables BOX_KWARGS Node NodeDependencies OPEN SEALED TYPE_CHECKING Functions requires_sealed def requires_sealed ( fn : 'Callable[..., _Return]' ) -> 'Callable[..., _Return]' View Source def requires_sealed ( fn : Callable [ ..., _Return ] ) -> Callable [ ..., _Return ] : @wraps ( fn ) def check_if_sealed ( self : Graph , * args : Any , ** kwargs : Any ) -> _Return : if self . _status is not SEALED : raise ValueError ( f \"{fn.__name__} cannot be used while the Graph is still being defined\" ) return fn ( self , * args , ** kwargs ) return check_if_sealed Classes ArtifactBox class ArtifactBox ( * args : Any , default_box : bool = False , default_box_attr : Any = < object object at 0x7f1a4d988080 > , default_box_none_transform : bool = True , default_box_create_on_get : bool = True , frozen_box : bool = False , camel_killer_box : bool = False , conversion_box : bool = True , modify_tuples_box : bool = False , box_safe_prefix : str = 'x' , box_duplicates : str = 'ignore' , box_intact_types : Union [ Tuple , List ] = (), box_recast : Optional [ Dict ] = None , box_dots : bool = False , box_class : Union [ Dict , Type [ ForwardRef ( 'Box' )], NoneType ] = None , box_namespace : Tuple [ str , ... ] = (), ** kwargs : Any ) View Source class ArtifactBox ( TypedBox [ Artifact ] ) : def _TypedBox__cast_value ( self , item : str , value : Any ) -> Artifact : artifact : Artifact = super (). _TypedBox__cast_value ( item , value ) # type : ignore [ misc ] storage = artifact . storage if ( graph : = arti . context . graph ) is not None : storage = storage . _visit_graph ( graph ). _visit_names ( ( * self . _box_config [ \"box_namespace\" ] , item ) ) # Require an { input_fingerprint } template in the Storage if this Artifact is being generated # by a Producer . Otherwise , strip the { input_fingerprint } template ( if set ) for \"raw\" # Artifacts . # # We can 't validate this at Artifact instantiation because the Producer is tracked later (by # copying the instance and setting the `producer_output` attribute). We won' t know the # \"final\" instance until assignment here to the Graph . if artifact . producer_output is None : storage = storage . _visit_input_fingerprint ( Fingerprint . empty ()) elif not artifact . storage . includes_input_fingerprint_template : raise ValueError ( \"Produced Artifacts must have a '{input_fingerprint}' template in their Storage\" ) return artifact . copy ( update = { \"storage\" : storage } ) Ancestors (in MRO) arti.graphs.TypedBox arti.internal.utils.TypedBox box.box.Box builtins.dict collections.abc.MutableMapping collections.abc.Mapping collections.abc.Collection collections.abc.Sized collections.abc.Iterable collections.abc.Container Static methods from_json def from_json ( json_string : Optional [ str ] = None , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transform a json object string into a Box object. If the incoming json is a list, you must use BoxList.from_json. Parameters: Name Type Description Default json_string None string to pass to json.loads None filename None filename to open and pass to json.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() or json.loads None Returns: Type Description None Box object from json data View Source @classmethod def from_json ( cls , json_string : Optional [ str ] = None , filename : Optional [ Union[str, PathLike ] ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transform a json object string into a Box object. If the incoming json is a list, you must use BoxList.from_json. :param json_string: string to pass to `json.loads` :param filename: filename to open and pass to `json.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` or `json.loads` :return: Box object from json data \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_json ( json_string , filename = filename , encoding = encoding , errors = errors , ** kwargs ) if not isinstance ( data , dict ) : raise BoxError ( f \"json data not returned as a dictionary, but rather a {type(data).__name__}\" ) return cls ( data , ** box_args ) from_msgpack def from_msgpack ( msgpack_bytes : Optional [ bytes ] = None , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' View Source @classmethod def from_msgpack ( cls , msgpack_bytes : Optional [ bytes ] = None , filename : Optional [ Union[str, PathLike ] ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : raise BoxError ( 'msgpack is unavailable on this system, please install the \"msgpack\" package' ) from_toml def from_toml ( toml_string : Optional [ str ] = None , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transforms a toml string or file into a Box object Parameters: Name Type Description Default toml_string None string to pass to toml.load None filename None filename to open and pass to toml.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() None Returns: Type Description None Box object View Source @classmethod def from_toml ( cls , toml_string : Optional [ str ] = None , filename : Optional [ Union[str, PathLike ] ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transforms a toml string or file into a Box object :param toml_string: string to pass to `toml.load` :param filename: filename to open and pass to `toml.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` :return: Box object \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_toml ( toml_string = toml_string , filename = filename , encoding = encoding , errors = errors ) return cls ( data , ** box_args ) from_yaml def from_yaml ( yaml_string : Optional [ str ] = None , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transform a yaml object string into a Box object. By default will use SafeLoader. Parameters: Name Type Description Default yaml_string None string to pass to yaml.load None filename None filename to open and pass to yaml.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() or yaml.load None Returns: Type Description None Box object from yaml data View Source @classmethod def from_yaml ( cls , yaml_string : Optional [ str ] = None , filename : Optional [ Union[str, PathLike ] ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transform a yaml object string into a Box object. By default will use SafeLoader. :param yaml_string: string to pass to `yaml.load` :param filename: filename to open and pass to `yaml.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` or `yaml.load` :return: Box object from yaml data \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_yaml ( yaml_string = yaml_string , filename = filename , encoding = encoding , errors = errors , ** kwargs ) if not data : return cls ( ** box_args ) if not isinstance ( data , dict ) : raise BoxError ( f \"yaml data not returned as a dictionary but rather a {type(data).__name__}\" ) return cls ( data , ** box_args ) Methods clear def clear ( self ) D.clear() -> None. Remove all items from D. View Source def clear ( self ) : if self . _box_config [ \"frozen_box\" ]: raise BoxError ( \"Box is frozen\" ) super () . clear () self . _box_config [ \"__safe_keys\" ]. clear () copy def copy ( self ) -> 'Box' D.copy() -> a shallow copy of D View Source def copy ( self ) -> \"Box\" : config = self . __box_config () config . pop ( \"box_namespace\" ) # Detach namespace ; it will be reassigned if we nest again return Box ( super (). copy (), ** config ) fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default =< object object at 0x7f1a4d988080 > ) Return the value for key if key is in the dictionary, else default. View Source def get ( self , key , default = NO_DEFAULT ) : if key not in self : if default is NO_DEFAULT : if self . _box_config [ \"default_box\" ] and self . _box_config [ \"default_box_none_transform\" ] : return self . __get_default ( key ) else : return None if isinstance ( default , dict ) and not isinstance ( default , Box ) : return Box ( default ) if isinstance ( default , list ) and not isinstance ( default , box . BoxList ) : return box . BoxList ( default ) return default return self [ key ] items def items ( self , dotted : bool = False ) D.items() -> a set-like object providing a view on D's items View Source def items ( self , dotted : Union [ bool ] = False ) : if not dotted : return super (). items () if not self . _box_config [ \"box_dots\" ] : raise BoxError ( \"Cannot return dotted keys as this Box does not have `box_dots` enabled\" ) return [ (k, self[k ] ) for k in self . keys ( dotted = True ) ] keys def keys ( self , dotted : bool = False ) D.keys() -> a set-like object providing a view on D's keys View Source def keys ( self , dotted : Union [ bool ] = False ) : if not dotted : return super (). keys () if not self . _box_config [ \"box_dots\" ] : raise BoxError ( \"Cannot return dotted keys as this Box does not have `box_dots` enabled\" ) keys = set () for key , value in self . items () : added = False if isinstance ( key , str ) : if isinstance ( value , Box ) : for sub_key in value . keys ( dotted = True ) : keys . add ( f \"{key}.{sub_key}\" ) added = True elif isinstance ( value , box . BoxList ) : for pos in value . _dotted_helper () : keys . add ( f \"{key}{pos}\" ) added = True if not added : keys . add ( key ) return sorted ( keys , key = lambda x : str ( x )) merge_update def merge_update ( self , * args , ** kwargs ) View Source def merge_update ( self , * args , ** kwargs ) : merge_type = None if \"box_merge_lists\" in kwargs : merge_type = kwargs . pop ( \"box_merge_lists\" ) def convert_and_set ( k , v ) : intact_type = self . _box_config [ \"box_intact_types\" ] and isinstance ( v , self . _box_config [ \"box_intact_types\" ] ) if isinstance ( v , dict ) and not intact_type : # Box objects must be created in case they are already # in the ` converted ` box_config set v = self . _box_config [ \"box_class\" ] ( v , ** self . __box_config ( extra_namespace = k )) if k in self and isinstance ( self [ k ] , dict ) : self [ k ] . merge_update ( v ) return if isinstance ( v , list ) and not intact_type : v = box . BoxList ( v , ** self . __box_config ( extra_namespace = k )) if merge_type == \"extend\" and k in self and isinstance ( self [ k ] , list ) : self [ k ] . extend ( v ) return if merge_type == \"unique\" and k in self and isinstance ( self [ k ] , list ) : for item in v : if item not in self [ k ] : self [ k ] . append ( item ) return self . __setitem__ ( k , v ) if ( len ( args ) + int ( bool ( kwargs ))) > 1 : raise BoxTypeError ( f \"merge_update expected at most 1 argument, got {len(args) + int(bool(kwargs))}\" ) single_arg = next ( iter ( args ), None ) if single_arg : if hasattr ( single_arg , \"keys\" ) : for k in single_arg : convert_and_set ( k , single_arg [ k ] ) else : for k , v in single_arg : convert_and_set ( k , v ) for key in kwargs : convert_and_set ( key , kwargs [ key ] ) pop def pop ( self , key , * args ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If the key is not found, return the default if given; otherwise, raise a KeyError. View Source def pop ( self , key , * args ) : if self . _box_config [ \"frozen_box\" ] : raise BoxError ( \"Box is frozen\" ) if args : if len ( args ) != 1 : raise BoxError ( 'pop() takes only one optional argument \"default\"' ) try : item = self [ key ] except KeyError : return args [ 0 ] else : del self [ key ] return item try : item = self [ key ] except KeyError : raise BoxKeyError ( f \"{key}\" ) from None else : del self [ key ] return item popitem def popitem ( self ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. View Source def popitem ( self ) : if self . _box_config [ \"frozen_box\" ]: raise BoxError ( \"Box is frozen\" ) try : key = next ( self . __iter__ ()) except StopIteration : raise BoxKeyError ( \"Empty box\" ) from None return key , self . pop ( key ) setdefault def setdefault ( self , item , default = None ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. View Source def setdefault ( self , item , default = None ) : if item in self : return self [ item ] if self . _box_config [ \"box_dots\" ] : if item in _get_dot_paths ( self ) : return self [ item ] if isinstance ( default , dict ) : default = self . _box_config [ \"box_class\" ] ( default , ** self . __box_config ( extra_namespace = item )) if isinstance ( default , list ) : default = box . BoxList ( default , ** self . __box_config ( extra_namespace = item )) self [ item ] = default return self [ item ] to_dict def to_dict ( self ) -> Dict Turn the Box and sub Boxes back into a native python dictionary. Returns: Type Description None python dictionary of this Box View Source def to_dict ( self ) -> Dict : \"\"\" Turn the Box and sub Boxes back into a native python dictionary. :return: python dictionary of this Box \"\"\" out_dict = dict ( self ) for k , v in out_dict . items () : if v is self : out_dict [ k ] = out_dict elif isinstance ( v , Box ) : out_dict [ k ] = v . to_dict () elif isinstance ( v , box . BoxList ) : out_dict [ k ] = v . to_list () return out_dict to_json def to_json ( self , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** json_kwargs ) Transform the Box object into a JSON string. Parameters: Name Type Description Default filename None If provided will save to file None encoding None File encoding None errors None How to handle encoding errors None json_kwargs None additional arguments to pass to json.dump(s) None Returns: Type Description None string of JSON (if no filename provided) View Source def to_json ( self , filename : Optional [ Union [ str , PathLike ]] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** json_kwargs , ) : \"\" \" Transform the Box object into a JSON string. :param filename: If provided will save to file :param encoding: File encoding :param errors: How to handle encoding errors :param json_kwargs: additional arguments to pass to json.dump(s) :return: string of JSON (if no filename provided) \"\" \" return _to_json(self.to_dict(), filename=filename, encoding=encoding, errors=errors, **json_kwargs) to_msgpack def to_msgpack ( self , filename : Union [ str , os . PathLike , NoneType ] = None , ** kwargs ) View Source def to_msgpack(self, filename: Optional[Union[str, PathLike]] = None, **kwargs): raise BoxError('msgpack is unavailable on this system, please install the \"msgpack\" package') to_toml def to_toml ( self , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' ) Transform the Box object into a toml string. Parameters: Name Type Description Default filename None File to write toml object too None encoding None File encoding None errors None How to handle encoding errors None Returns: Type Description None string of TOML (if no filename provided) View Source def to_toml ( self , filename : Optional [ Union [ str , PathLike ]] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" ) : \"\" \" Transform the Box object into a toml string. :param filename: File to write toml object too :param encoding: File encoding :param errors: How to handle encoding errors :return: string of TOML (if no filename provided) \"\" \" return _to_toml(self.to_dict(), filename=filename, encoding=encoding, errors=errors) to_yaml def to_yaml ( self , filename : Union [ str , os . PathLike , NoneType ] = None , default_flow_style : bool = False , encoding : str = 'utf-8' , errors : str = 'strict' , ** yaml_kwargs ) Transform the Box object into a YAML string. Parameters: Name Type Description Default filename None If provided will save to file None default_flow_style None False will recursively dump dicts None encoding None File encoding None errors None How to handle encoding errors None yaml_kwargs None additional arguments to pass to yaml.dump None Returns: Type Description None string of YAML (if no filename provided) View Source def to_yaml ( self , filename : Optional [ Union [ str , PathLike ]] = None , default_flow_style : bool = False , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** yaml_kwargs , ) : \"\" \" Transform the Box object into a YAML string. :param filename: If provided will save to file :param default_flow_style: False will recursively dump dicts :param encoding: File encoding :param errors: How to handle encoding errors :param yaml_kwargs: additional arguments to pass to yaml.dump :return: string of YAML (if no filename provided) \"\" \" return _to_yaml( self.to_dict(), filename=filename, default_flow_style=default_flow_style, encoding=encoding, errors=errors, **yaml_kwargs, ) update def update ( self , * args , ** kwargs ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] View Source def update ( self , * args , ** kwargs ) : if self . _box_config [ \"frozen_box\" ] : raise BoxError ( \"Box is frozen\" ) if ( len ( args ) + int ( bool ( kwargs ))) > 1 : raise BoxTypeError ( f \"update expected at most 1 argument, got {len(args) + int(bool(kwargs))}\" ) single_arg = next ( iter ( args ), None ) if single_arg : if hasattr ( single_arg , \"keys\" ) : for k in single_arg : self . __convert_and_store ( k , single_arg [ k ] ) else : for k , v in single_arg : self . __convert_and_store ( k , v ) for k in kwargs : self . __convert_and_store ( k , kwargs [ k ] ) values def values ( ... ) D.values() -> an object providing a view on D's values walk def walk ( self , root : 'tuple[str, ...]' = () ) -> 'Iterator[tuple[str, _V]]' View Source def walk ( self , root : tuple [ str, ... ] = ()) -> Iterator [ tuple[str, _V ] ]: for k , v in self . items () : subroot = ( * root , k ) if isinstance ( v , TypedBox ) : yield from v . walk ( root = subroot ) else : yield \".\" . join ( subroot ), v # type : ignore [ misc ] Graph class Graph ( __pydantic_self__ , ** data : Any ) View Source class Graph ( Model ) : \"\"\"Graph stores a web of Artifacts connected by Producers.\"\"\" name : str artifacts : ArtifactBox = Field ( default_factory = lambda : ArtifactBox ( ** BOX_KWARGS [ SEALED ] )) # The Backend * itself * should not affect the results of a Graph build , though the contents # certainly may ( eg : stored annotations ), so we avoid serializing it . This also prevent # embedding any credentials . backend : Backend [ Connection ] = Field ( default_factory = _get_memory_backend , exclude = True ) path_tags : frozendict [ str, str ] = frozendict () # Graph starts off sealed , but is opened within a ` with Graph (...) ` context _status : Optional [ bool ] = PrivateAttr ( None ) _artifact_to_key : frozendict [ Artifact, str ] = PrivateAttr ( frozendict ()) @validator ( \"artifacts\" ) @classmethod def _convert_artifacts ( cls , artifacts : ArtifactBox ) -> ArtifactBox : return ArtifactBox ( artifacts , ** BOX_KWARGS [ SEALED ] ) def __enter__ ( self ) -> Graph : if arti . context . graph is not None : raise ValueError ( f \"Another graph is being defined: {arti.context.graph}\" ) arti . context . graph = self self . _toggle ( OPEN ) return self def __exit__ ( self , exc_type : Optional [ type[BaseException ] ] , exc_value : Optional [ BaseException ] , exc_traceback : Optional [ TracebackType ] , ) -> None : arti . context . graph = None self . _toggle ( SEALED ) # Confirm the dependencies are acyclic TopologicalSorter ( self . dependencies ). prepare () def _toggle ( self , status : bool ) -> None : # The Graph object is \"frozen\" , so we must bypass the assignment checks . object . __setattr__ ( self , \"artifacts\" , ArtifactBox ( self . artifacts , ** BOX_KWARGS [ status ] )) self . _status = status self . _artifact_to_key = frozendict ( { artifact : key for key , artifact in self . artifacts . walk () } ) @property def artifact_to_key ( self ) -> frozendict [ Artifact, str ] : return self . _artifact_to_key @requires_sealed def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : return self . snapshot (). build ( executor ) @requires_sealed def snapshot ( self , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Identify a \" unique \" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" return GraphSnapshot . from_graph ( self , connection = connection ) @cached_property @requires_sealed def dependencies ( self ) -> NodeDependencies : artifact_deps = { artifact : ( frozenset ( { artifact . producer_output . producer } ) if artifact . producer_output is not None else frozenset () ) for _ , artifact in self . artifacts . walk () } producer_deps = { # NOTE : multi - output Producers will appear multiple times ( but be deduped ) producer_output . producer : frozenset ( producer_output . producer . inputs . values ()) for artifact in artifact_deps if ( producer_output : = artifact . producer_output ) is not None } return NodeDependencies ( artifact_deps | producer_deps ) @cached_property @requires_sealed def producers ( self ) -> frozenset [ Producer ] : return frozenset ( self . producer_outputs ) @cached_property @requires_sealed def producer_outputs ( self ) -> frozendict [ Producer, tuple[Artifact, ... ] ]: d = defaultdict [ Producer, dict[int, Artifact ] ] ( dict ) for _ , artifact in self . artifacts . walk () : if artifact . producer_output is None : continue output = artifact . producer_output d [ output.producer ][ output.position ] = artifact return frozendict ( ( producer , tuple ( artifacts_by_position [ i ] for i in sorted ( artifacts_by_position ))) for producer , artifacts_by_position in d . items () ) # TODO : io . read / write probably need a bit of sanity checking ( probably somewhere else ), eg : type # ~= view . Doing validation on the data , etc . Should some of this live on the View ? @requires_sealed def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , snapshot : Optional [ GraphSnapshot ] = None , connection : Optional [ Connection ] = None , ) -> Any : key = self . artifact_to_key [ artifact ] if annotation is None and view is None : raise ValueError ( \"Either `annotation` or `view` must be passed\" ) if annotation is not None and view is not None : raise ValueError ( \"Only one of `annotation` or `view` may be passed\" ) if annotation is not None : view = View . get_class_for ( annotation )( artifact_class = type ( artifact ), type = artifact . type , mode = \"READ\" ) view . check_annotation_compatibility ( annotation ) view . check_artifact_compatibility ( artifact ) assert view is not None # mypy gets mixed up with ^ if storage_partitions is None : # We want to allow reading raw Artifacts even if other raw Artifacts are missing ( which # prevents snapshotting ). if snapshot is None and artifact . producer_output is None : # NOTE : We 're not using read_artifact_partitions as the underlying data may have # changed. The backend may have pointers to old versions (which is expected), but we # only want to return the current values. storage_partitions = artifact.storage.discover_partitions() else: snapshot = snapshot or self.snapshot() with (connection or self.backend).connect() as conn: storage_partitions = conn.read_snapshot_partitions(snapshot, key, artifact) return io.read( type_=artifact.type, format=artifact.format, storage_partitions=storage_partitions, view=view, ) @requires_sealed def write( self, data: Any, *, artifact: Artifact, input_fingerprint: Fingerprint = Fingerprint.empty(), keys: CompositeKey = CompositeKey(), view: Optional[View] = None, snapshot: Optional[GraphSnapshot] = None, connection: Optional[Connection] = None, ) -> StoragePartition: key = self.artifact_to_key[artifact] if snapshot is not None and artifact.producer_output is None: raise ValueError( f\"Writing to a raw Artifact (`{key}`) with a GraphSnapshot is not supported.\" ) if view is None: view = View.get_class_for(type(data))( artifact_class=type(artifact), type=artifact.type, mode=\"WRITE\" ) view.check_annotation_compatibility(type(data)) view.check_artifact_compatibility(artifact) storage_partition = artifact.storage.generate_partition( input_fingerprint=input_fingerprint, keys=keys, with_content_fingerprint=False ) storage_partition = io.write( data, type_=artifact.type, format=artifact.format, storage_partition=storage_partition, view=view, ).with_content_fingerprint() # TODO: Should we only do this in bulk? We might want the backends to transparently batch # requests, but that' s not so friendly with the transient \".connect\" . with ( connection or self . backend ). connect () as conn : conn . write_artifact_partitions ( artifact , ( storage_partition ,)) # Skip linking this partition to the snapshot if it affects raw Artifacts ( which would # trigger an id change ). if snapshot is not None and artifact . producer_output is not None : conn . write_snapshot_partitions ( snapshot , key , artifact , ( storage_partition ,)) return storage_partition Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables artifact_to_key fingerprint Methods build def build ( self , executor : 'Optional[Executor]' = None ) -> 'GraphSnapshot' View Source @requires_sealed def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : return self . snapshot (). build ( executor ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dependencies def dependencies ( ... ) dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . producer_outputs def producer_outputs ( ... ) producers def producers ( ... ) read def read ( self , artifact : 'Artifact' , * , annotation : 'Optional[Any]' = None , storage_partitions : 'Optional[Sequence[StoragePartition]]' = None , view : 'Optional[View]' = None , snapshot : 'Optional[GraphSnapshot]' = None , connection : 'Optional[Connection]' = None ) -> 'Any' View Source @requires_sealed def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , snapshot : Optional [ GraphSnapshot ] = None , connection : Optional [ Connection ] = None , ) -> Any : key = self . artifact_to_key [ artifact ] if annotation is None and view is None : raise ValueError ( \"Either `annotation` or `view` must be passed\" ) if annotation is not None and view is not None : raise ValueError ( \"Only one of `annotation` or `view` may be passed\" ) if annotation is not None : view = View . get_class_for ( annotation )( artifact_class = type ( artifact ), type = artifact . type , mode = \"READ\" ) view . check_annotation_compatibility ( annotation ) view . check_artifact_compatibility ( artifact ) assert view is not None # mypy gets mixed up with ^ if storage_partitions is None : # We want to allow reading raw Artifacts even if other raw Artifacts are missing ( which # prevents snapshotting ). if snapshot is None and artifact . producer_output is None : # NOTE : We ' re not using read_artifact_partitions as the underlying data may have # changed . The backend may have pointers to old versions ( which is expected ), but we # only want to return the current values . storage_partitions = artifact . storage . discover_partitions () else : snapshot = snapshot or self . snapshot () with ( connection or self . backend ). connect () as conn : storage_partitions = conn . read_snapshot_partitions ( snapshot , key , artifact ) return io . read ( type_ = artifact . type , format = artifact . format , storage_partitions = storage_partitions , view = view , ) snapshot def snapshot ( self , * , connection : 'Optional[Connection]' = None ) -> 'GraphSnapshot' Identify a \"unique\" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a snapshot of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. View Source @requires_sealed def snapshot ( self , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Identify a \" unique \" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" return GraphSnapshot . from_graph ( self , connection = connection ) write def write ( self , data : 'Any' , * , artifact : 'Artifact' , input_fingerprint : 'Fingerprint' = Fingerprint ( key = None ), keys : 'CompositeKey' = {}, view : 'Optional[View]' = None , snapshot : 'Optional[GraphSnapshot]' = None , connection : 'Optional[Connection]' = None ) -> 'StoragePartition' View Source @requires_sealed def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , snapshot : Optional [ GraphSnapshot ] = None , connection : Optional [ Connection ] = None , ) -> StoragePartition : key = self . artifact_to_key [ artifact ] if snapshot is not None and artifact . producer_output is None : raise ValueError ( f \"Writing to a raw Artifact (`{key}`) with a GraphSnapshot is not supported.\" ) if view is None : view = View . get_class_for ( type ( data ))( artifact_class = type ( artifact ), type = artifact . type , mode = \"WRITE\" ) view . check_annotation_compatibility ( type ( data )) view . check_artifact_compatibility ( artifact ) storage_partition = artifact . storage . generate_partition ( input_fingerprint = input_fingerprint , keys = keys , with_content_fingerprint = False ) storage_partition = io . write ( data , type_ = artifact . type , format = artifact . format , storage_partition = storage_partition , view = view , ). with_content_fingerprint () # TODO : Should we only do this in bulk ? We might want the backends to transparently batch # requests , but that ' s not so friendly with the transient \".connect\" . with ( connection or self . backend ). connect () as conn : conn . write_artifact_partitions ( artifact , ( storage_partition ,)) # Skip linking this partition to the snapshot if it affects raw Artifacts ( which would # trigger an id change ). if snapshot is not None and artifact . producer_output is not None : conn . write_snapshot_partitions ( snapshot , key , artifact , ( storage_partition ,)) return storage_partition GraphSnapshot class GraphSnapshot ( __pydantic_self__ , ** data : Any ) View Source class GraphSnapshot ( Model ) : \"\"\"GraphSnapshot represents the state of a Graph and the referenced raw data at a point in time. GraphSnapshot encodes the structure of the Graph plus a snapshot of the raw Artifact data (partition kinds and contents) at a point in time. Any change that would affect data should prompt an ID change. \"\"\" id : Fingerprint graph : Graph @property def artifacts ( self ) -> ArtifactBox : return self . graph . artifacts @property def backend ( self ) -> Backend [ Connection ] : return self . graph . backend @property def name ( self ) -> str : return self . graph . name @classmethod # TODO : Should this use a ( TTL ) cache ? Raw data changes ( especially in tests ) still need to be detected . def from_graph ( cls , graph : Graph , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Snapshot the Graph and all existing raw data. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifact data during Producer builds. \"\"\" # TODO : Resolve and statically set all available fingerprints . Specifically , we should pin # the Producer . fingerprint , which may by dynamic ( eg : version is a Timestamp ). Unbuilt # Artifact ( partitions ) won 't be fully resolved yet. snapshot_id, known_artifact_partitions = graph.fingerprint, dict[str, StoragePartitions]() for node, _ in graph.dependencies.items(): snapshot_id = snapshot_id.combine(node.fingerprint) if isinstance(node, Artifact): key = graph.artifact_to_key[node] snapshot_id = snapshot_id.combine(Fingerprint.from_string(key)) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we' ll have to handle things a bit # differently depending on if the external Artifacts are Produced ( in an upstream # Graph ) or not . if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . storage . discover_partitions () ) if not known_artifact_partitions [ key ] : content_str = \"partitions\" if is_partitioned ( node . type ) else \"data\" raise ValueError ( f \"No {content_str} found for `{key}`: {node}\" ) snapshot_id = snapshot_id . combine ( *[ partition.fingerprint for partition in known_artifact_partitions[key ] ] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma : no cover # NOTE : This shouldn ' t happen unless the logic above is faulty . raise ValueError ( \"Fingerprint is empty!\" ) snapshot = cls ( graph = graph , id = snapshot_id ) # Write the discovered partitions ( if not already known ) and link to this new snapshot . with ( connection or snapshot . backend ). connect () as conn : conn . write_graph ( graph ) conn . write_snapshot ( snapshot ) for key , partitions in known_artifact_partitions . items () : conn . write_artifact_and_graph_partitions ( snapshot , key , snapshot . artifacts [ key ] , partitions ) return snapshot def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : if executor is None : from arti . executors . local import LocalExecutor executor = LocalExecutor () executor . build ( self ) return self def tag ( self , tag : str , * , overwrite : bool = False , connection : Optional [ Connection ] = None ) -> None : with ( connection or self . backend ). connect () as conn : conn . write_snapshot_tag ( self , tag , overwrite ) @classmethod def from_tag ( cls , name : str , tag : str , * , connectable : Union [ Backend[Connection ] , Connection ] ) -> GraphSnapshot : with connectable . connect () as conn : return conn . read_snapshot_tag ( name , tag ) def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> Any : return self . graph . read ( artifact , annotation = annotation , storage_partitions = storage_partitions , view = view , snapshot = self , connection = connection , ) def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> StoragePartition : return self . graph . write ( data , artifact = artifact , input_fingerprint = input_fingerprint , keys = keys , view = view , snapshot = self , connection = connection , ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_graph def from_graph ( graph : 'Graph' , * , connection : 'Optional[Connection]' = None ) -> 'GraphSnapshot' Snapshot the Graph and all existing raw data. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifact data during Producer builds. View Source @classmethod # TODO : Should this use a ( TTL ) cache ? Raw data changes ( especially in tests ) still need to be detected . def from_graph ( cls , graph : Graph , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Snapshot the Graph and all existing raw data. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifact data during Producer builds. \"\"\" # TODO : Resolve and statically set all available fingerprints . Specifically , we should pin # the Producer . fingerprint , which may by dynamic ( eg : version is a Timestamp ). Unbuilt # Artifact ( partitions ) won 't be fully resolved yet. snapshot_id, known_artifact_partitions = graph.fingerprint, dict[str, StoragePartitions]() for node, _ in graph.dependencies.items(): snapshot_id = snapshot_id.combine(node.fingerprint) if isinstance(node, Artifact): key = graph.artifact_to_key[node] snapshot_id = snapshot_id.combine(Fingerprint.from_string(key)) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we' ll have to handle things a bit # differently depending on if the external Artifacts are Produced ( in an upstream # Graph ) or not . if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . storage . discover_partitions () ) if not known_artifact_partitions [ key ] : content_str = \"partitions\" if is_partitioned ( node . type ) else \"data\" raise ValueError ( f \"No {content_str} found for `{key}`: {node}\" ) snapshot_id = snapshot_id . combine ( *[ partition.fingerprint for partition in known_artifact_partitions[key ] ] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma : no cover # NOTE : This shouldn ' t happen unless the logic above is faulty . raise ValueError ( \"Fingerprint is empty!\" ) snapshot = cls ( graph = graph , id = snapshot_id ) # Write the discovered partitions ( if not already known ) and link to this new snapshot . with ( connection or snapshot . backend ). connect () as conn : conn . write_graph ( graph ) conn . write_snapshot ( snapshot ) for key , partitions in known_artifact_partitions . items () : conn . write_artifact_and_graph_partitions ( snapshot , key , snapshot . artifacts [ key ] , partitions ) return snapshot from_orm def from_orm ( obj : Any ) -> 'Model' from_tag def from_tag ( name : 'str' , tag : 'str' , * , connectable : 'Union[Backend[Connection], Connection]' ) -> 'GraphSnapshot' View Source @classmethod def from_tag ( cls , name : str , tag : str , * , connectable : Union [ Backend[Connection ] , Connection ] ) -> GraphSnapshot : with connectable . connect () as conn : return conn . read_snapshot_tag ( name , tag ) parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables artifacts backend fingerprint name Methods build def build ( self , executor : 'Optional[Executor]' = None ) -> 'GraphSnapshot' View Source def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : if executor is None : from arti.executors.local import LocalExecutor executor = LocalExecutor () executor . build ( self ) return self copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . read def read ( self , artifact : 'Artifact' , * , annotation : 'Optional[Any]' = None , storage_partitions : 'Optional[Sequence[StoragePartition]]' = None , view : 'Optional[View]' = None , connection : 'Optional[Connection]' = None ) -> 'Any' View Source def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> Any : return self . graph . read ( artifact , annotation = annotation , storage_partitions = storage_partitions , view = view , snapshot = self , connection = connection , ) tag def tag ( self , tag : 'str' , * , overwrite : 'bool' = False , connection : 'Optional[Connection]' = None ) -> 'None' View Source def tag ( self , tag : str , * , overwrite : bool = False , connection : Optional [ Connection ] = None ) -> None : with ( connection or self . backend ). connect () as conn : conn . write_snapshot_tag ( self , tag , overwrite ) write def write ( self , data : 'Any' , * , artifact : 'Artifact' , input_fingerprint : 'Fingerprint' = Fingerprint ( key = None ), keys : 'CompositeKey' = {}, view : 'Optional[View]' = None , connection : 'Optional[Connection]' = None ) -> 'StoragePartition' View Source def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> StoragePartition : return self . graph . write ( data , artifact = artifact , input_fingerprint = input_fingerprint , keys = keys , view = view , snapshot = self , connection = connection , )","title":"Graphs"},{"location":"reference/arti/graphs/#module-artigraphs","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from collections import defaultdict from collections.abc import Callable , Sequence from functools import cached_property , wraps from graphlib import TopologicalSorter from types import TracebackType from typing import TYPE_CHECKING , Any , Literal , Optional , TypeVar , Union from pydantic import Field , PrivateAttr , validator import arti from arti import io from arti.artifacts import Artifact from arti.backends import Backend , Connection from arti.fingerprints import Fingerprint from arti.internal.models import Model from arti.internal.utils import TypedBox , frozendict from arti.partitions import CompositeKey from arti.producers import Producer from arti.storage import StoragePartition , StoragePartitions from arti.types import is_partitioned from arti.views import View if TYPE_CHECKING : from arti.backends.memory import MemoryBackend from arti.executors import Executor else : from arti.internal.patches import patch_TopologicalSorter_class_getitem patch_TopologicalSorter_class_getitem () def _get_memory_backend () -> MemoryBackend : # Avoid importing non-root modules upon import from arti.backends.memory import MemoryBackend return MemoryBackend () SEALED : Literal [ True ] = True OPEN : Literal [ False ] = False BOX_KWARGS = { status : { \"box_dots\" : True , \"default_box\" : status is OPEN , \"frozen_box\" : status is SEALED , } for status in ( OPEN , SEALED ) } _Return = TypeVar ( \"_Return\" ) def requires_sealed ( fn : Callable [ ... , _Return ]) -> Callable [ ... , _Return ]: @wraps ( fn ) def check_if_sealed ( self : Graph , * args : Any , ** kwargs : Any ) -> _Return : if self . _status is not SEALED : raise ValueError ( f \" { fn . __name__ } cannot be used while the Graph is still being defined\" ) return fn ( self , * args , ** kwargs ) return check_if_sealed class ArtifactBox ( TypedBox [ Artifact ]): def _TypedBox__cast_value ( self , item : str , value : Any ) -> Artifact : artifact : Artifact = super () . _TypedBox__cast_value ( item , value ) # type: ignore[misc] storage = artifact . storage if ( graph := arti . context . graph ) is not None : storage = storage . _visit_graph ( graph ) . _visit_names ( ( * self . _box_config [ \"box_namespace\" ], item ) ) # Require an {input_fingerprint} template in the Storage if this Artifact is being generated # by a Producer. Otherwise, strip the {input_fingerprint} template (if set) for \"raw\" # Artifacts. # # We can't validate this at Artifact instantiation because the Producer is tracked later (by # copying the instance and setting the `producer_output` attribute). We won't know the # \"final\" instance until assignment here to the Graph. if artifact . producer_output is None : storage = storage . _visit_input_fingerprint ( Fingerprint . empty ()) elif not artifact . storage . includes_input_fingerprint_template : raise ValueError ( \"Produced Artifacts must have a ' {input_fingerprint} ' template in their Storage\" ) return artifact . copy ( update = { \"storage\" : storage }) Node = Union [ Artifact , Producer ] NodeDependencies = frozendict [ Node , frozenset [ Node ]] class Graph ( Model ): \"\"\"Graph stores a web of Artifacts connected by Producers.\"\"\" name : str artifacts : ArtifactBox = Field ( default_factory = lambda : ArtifactBox ( ** BOX_KWARGS [ SEALED ])) # The Backend *itself* should not affect the results of a Graph build, though the contents # certainly may (eg: stored annotations), so we avoid serializing it. This also prevent # embedding any credentials. backend : Backend [ Connection ] = Field ( default_factory = _get_memory_backend , exclude = True ) path_tags : frozendict [ str , str ] = frozendict () # Graph starts off sealed, but is opened within a `with Graph(...)` context _status : Optional [ bool ] = PrivateAttr ( None ) _artifact_to_key : frozendict [ Artifact , str ] = PrivateAttr ( frozendict ()) @validator ( \"artifacts\" ) @classmethod def _convert_artifacts ( cls , artifacts : ArtifactBox ) -> ArtifactBox : return ArtifactBox ( artifacts , ** BOX_KWARGS [ SEALED ]) def __enter__ ( self ) -> Graph : if arti . context . graph is not None : raise ValueError ( f \"Another graph is being defined: { arti . context . graph } \" ) arti . context . graph = self self . _toggle ( OPEN ) return self def __exit__ ( self , exc_type : Optional [ type [ BaseException ]], exc_value : Optional [ BaseException ], exc_traceback : Optional [ TracebackType ], ) -> None : arti . context . graph = None self . _toggle ( SEALED ) # Confirm the dependencies are acyclic TopologicalSorter ( self . dependencies ) . prepare () def _toggle ( self , status : bool ) -> None : # The Graph object is \"frozen\", so we must bypass the assignment checks. object . __setattr__ ( self , \"artifacts\" , ArtifactBox ( self . artifacts , ** BOX_KWARGS [ status ])) self . _status = status self . _artifact_to_key = frozendict ( { artifact : key for key , artifact in self . artifacts . walk ()} ) @property def artifact_to_key ( self ) -> frozendict [ Artifact , str ]: return self . _artifact_to_key @requires_sealed def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : return self . snapshot () . build ( executor ) @requires_sealed def snapshot ( self , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Identify a \"unique\" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" return GraphSnapshot . from_graph ( self , connection = connection ) @cached_property @requires_sealed def dependencies ( self ) -> NodeDependencies : artifact_deps = { artifact : ( frozenset ({ artifact . producer_output . producer }) if artifact . producer_output is not None else frozenset () ) for _ , artifact in self . artifacts . walk () } producer_deps = { # NOTE: multi-output Producers will appear multiple times (but be deduped) producer_output . producer : frozenset ( producer_output . producer . inputs . values ()) for artifact in artifact_deps if ( producer_output := artifact . producer_output ) is not None } return NodeDependencies ( artifact_deps | producer_deps ) @cached_property @requires_sealed def producers ( self ) -> frozenset [ Producer ]: return frozenset ( self . producer_outputs ) @cached_property @requires_sealed def producer_outputs ( self ) -> frozendict [ Producer , tuple [ Artifact , ... ]]: d = defaultdict [ Producer , dict [ int , Artifact ]]( dict ) for _ , artifact in self . artifacts . walk (): if artifact . producer_output is None : continue output = artifact . producer_output d [ output . producer ][ output . position ] = artifact return frozendict ( ( producer , tuple ( artifacts_by_position [ i ] for i in sorted ( artifacts_by_position ))) for producer , artifacts_by_position in d . items () ) # TODO: io.read/write probably need a bit of sanity checking (probably somewhere else), eg: type # ~= view. Doing validation on the data, etc. Should some of this live on the View? @requires_sealed def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence [ StoragePartition ]] = None , view : Optional [ View ] = None , snapshot : Optional [ GraphSnapshot ] = None , connection : Optional [ Connection ] = None , ) -> Any : key = self . artifact_to_key [ artifact ] if annotation is None and view is None : raise ValueError ( \"Either `annotation` or `view` must be passed\" ) if annotation is not None and view is not None : raise ValueError ( \"Only one of `annotation` or `view` may be passed\" ) if annotation is not None : view = View . get_class_for ( annotation )( artifact_class = type ( artifact ), type = artifact . type , mode = \"READ\" ) view . check_annotation_compatibility ( annotation ) view . check_artifact_compatibility ( artifact ) assert view is not None # mypy gets mixed up with ^ if storage_partitions is None : # We want to allow reading raw Artifacts even if other raw Artifacts are missing (which # prevents snapshotting). if snapshot is None and artifact . producer_output is None : # NOTE: We're not using read_artifact_partitions as the underlying data may have # changed. The backend may have pointers to old versions (which is expected), but we # only want to return the current values. storage_partitions = artifact . storage . discover_partitions () else : snapshot = snapshot or self . snapshot () with ( connection or self . backend ) . connect () as conn : storage_partitions = conn . read_snapshot_partitions ( snapshot , key , artifact ) return io . read ( type_ = artifact . type , format = artifact . format , storage_partitions = storage_partitions , view = view , ) @requires_sealed def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , snapshot : Optional [ GraphSnapshot ] = None , connection : Optional [ Connection ] = None , ) -> StoragePartition : key = self . artifact_to_key [ artifact ] if snapshot is not None and artifact . producer_output is None : raise ValueError ( f \"Writing to a raw Artifact (` { key } `) with a GraphSnapshot is not supported.\" ) if view is None : view = View . get_class_for ( type ( data ))( artifact_class = type ( artifact ), type = artifact . type , mode = \"WRITE\" ) view . check_annotation_compatibility ( type ( data )) view . check_artifact_compatibility ( artifact ) storage_partition = artifact . storage . generate_partition ( input_fingerprint = input_fingerprint , keys = keys , with_content_fingerprint = False ) storage_partition = io . write ( data , type_ = artifact . type , format = artifact . format , storage_partition = storage_partition , view = view , ) . with_content_fingerprint () # TODO: Should we only do this in bulk? We might want the backends to transparently batch # requests, but that's not so friendly with the transient \".connect\". with ( connection or self . backend ) . connect () as conn : conn . write_artifact_partitions ( artifact , ( storage_partition ,)) # Skip linking this partition to the snapshot if it affects raw Artifacts (which would # trigger an id change). if snapshot is not None and artifact . producer_output is not None : conn . write_snapshot_partitions ( snapshot , key , artifact , ( storage_partition ,)) return storage_partition class GraphSnapshot ( Model ): \"\"\"GraphSnapshot represents the state of a Graph and the referenced raw data at a point in time. GraphSnapshot encodes the structure of the Graph plus a snapshot of the raw Artifact data (partition kinds and contents) at a point in time. Any change that would affect data should prompt an ID change. \"\"\" id : Fingerprint graph : Graph @property def artifacts ( self ) -> ArtifactBox : return self . graph . artifacts @property def backend ( self ) -> Backend [ Connection ]: return self . graph . backend @property def name ( self ) -> str : return self . graph . name @classmethod # TODO: Should this use a (TTL) cache? Raw data changes (especially in tests) still need to be detected. def from_graph ( cls , graph : Graph , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Snapshot the Graph and all existing raw data. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifact data during Producer builds. \"\"\" # TODO: Resolve and statically set all available fingerprints. Specifically, we should pin # the Producer.fingerprint, which may by dynamic (eg: version is a Timestamp). Unbuilt # Artifact (partitions) won't be fully resolved yet. snapshot_id , known_artifact_partitions = graph . fingerprint , dict [ str , StoragePartitions ]() for node , _ in graph . dependencies . items (): snapshot_id = snapshot_id . combine ( node . fingerprint ) if isinstance ( node , Artifact ): key = graph . artifact_to_key [ node ] snapshot_id = snapshot_id . combine ( Fingerprint . from_string ( key )) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we'll have to handle things a bit # differently depending on if the external Artifacts are Produced (in an upstream # Graph) or not. if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . storage . discover_partitions () ) if not known_artifact_partitions [ key ]: content_str = \"partitions\" if is_partitioned ( node . type ) else \"data\" raise ValueError ( f \"No { content_str } found for ` { key } `: { node } \" ) snapshot_id = snapshot_id . combine ( * [ partition . fingerprint for partition in known_artifact_partitions [ key ]] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma: no cover # NOTE: This shouldn't happen unless the logic above is faulty. raise ValueError ( \"Fingerprint is empty!\" ) snapshot = cls ( graph = graph , id = snapshot_id ) # Write the discovered partitions (if not already known) and link to this new snapshot. with ( connection or snapshot . backend ) . connect () as conn : conn . write_graph ( graph ) conn . write_snapshot ( snapshot ) for key , partitions in known_artifact_partitions . items (): conn . write_artifact_and_graph_partitions ( snapshot , key , snapshot . artifacts [ key ], partitions ) return snapshot def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : if executor is None : from arti.executors.local import LocalExecutor executor = LocalExecutor () executor . build ( self ) return self def tag ( self , tag : str , * , overwrite : bool = False , connection : Optional [ Connection ] = None ) -> None : with ( connection or self . backend ) . connect () as conn : conn . write_snapshot_tag ( self , tag , overwrite ) @classmethod def from_tag ( cls , name : str , tag : str , * , connectable : Union [ Backend [ Connection ], Connection ] ) -> GraphSnapshot : with connectable . connect () as conn : return conn . read_snapshot_tag ( name , tag ) def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence [ StoragePartition ]] = None , view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> Any : return self . graph . read ( artifact , annotation = annotation , storage_partitions = storage_partitions , view = view , snapshot = self , connection = connection , ) def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> StoragePartition : return self . graph . write ( data , artifact = artifact , input_fingerprint = input_fingerprint , keys = keys , view = view , snapshot = self , connection = connection , )","title":"Module arti.graphs"},{"location":"reference/arti/graphs/#variables","text":"BOX_KWARGS Node NodeDependencies OPEN SEALED TYPE_CHECKING","title":"Variables"},{"location":"reference/arti/graphs/#functions","text":"","title":"Functions"},{"location":"reference/arti/graphs/#requires_sealed","text":"def requires_sealed ( fn : 'Callable[..., _Return]' ) -> 'Callable[..., _Return]' View Source def requires_sealed ( fn : Callable [ ..., _Return ] ) -> Callable [ ..., _Return ] : @wraps ( fn ) def check_if_sealed ( self : Graph , * args : Any , ** kwargs : Any ) -> _Return : if self . _status is not SEALED : raise ValueError ( f \"{fn.__name__} cannot be used while the Graph is still being defined\" ) return fn ( self , * args , ** kwargs ) return check_if_sealed","title":"requires_sealed"},{"location":"reference/arti/graphs/#classes","text":"","title":"Classes"},{"location":"reference/arti/graphs/#artifactbox","text":"class ArtifactBox ( * args : Any , default_box : bool = False , default_box_attr : Any = < object object at 0x7f1a4d988080 > , default_box_none_transform : bool = True , default_box_create_on_get : bool = True , frozen_box : bool = False , camel_killer_box : bool = False , conversion_box : bool = True , modify_tuples_box : bool = False , box_safe_prefix : str = 'x' , box_duplicates : str = 'ignore' , box_intact_types : Union [ Tuple , List ] = (), box_recast : Optional [ Dict ] = None , box_dots : bool = False , box_class : Union [ Dict , Type [ ForwardRef ( 'Box' )], NoneType ] = None , box_namespace : Tuple [ str , ... ] = (), ** kwargs : Any ) View Source class ArtifactBox ( TypedBox [ Artifact ] ) : def _TypedBox__cast_value ( self , item : str , value : Any ) -> Artifact : artifact : Artifact = super (). _TypedBox__cast_value ( item , value ) # type : ignore [ misc ] storage = artifact . storage if ( graph : = arti . context . graph ) is not None : storage = storage . _visit_graph ( graph ). _visit_names ( ( * self . _box_config [ \"box_namespace\" ] , item ) ) # Require an { input_fingerprint } template in the Storage if this Artifact is being generated # by a Producer . Otherwise , strip the { input_fingerprint } template ( if set ) for \"raw\" # Artifacts . # # We can 't validate this at Artifact instantiation because the Producer is tracked later (by # copying the instance and setting the `producer_output` attribute). We won' t know the # \"final\" instance until assignment here to the Graph . if artifact . producer_output is None : storage = storage . _visit_input_fingerprint ( Fingerprint . empty ()) elif not artifact . storage . includes_input_fingerprint_template : raise ValueError ( \"Produced Artifacts must have a '{input_fingerprint}' template in their Storage\" ) return artifact . copy ( update = { \"storage\" : storage } )","title":"ArtifactBox"},{"location":"reference/arti/graphs/#ancestors-in-mro","text":"arti.graphs.TypedBox arti.internal.utils.TypedBox box.box.Box builtins.dict collections.abc.MutableMapping collections.abc.Mapping collections.abc.Collection collections.abc.Sized collections.abc.Iterable collections.abc.Container","title":"Ancestors (in MRO)"},{"location":"reference/arti/graphs/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/graphs/#from_json","text":"def from_json ( json_string : Optional [ str ] = None , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transform a json object string into a Box object. If the incoming json is a list, you must use BoxList.from_json. Parameters: Name Type Description Default json_string None string to pass to json.loads None filename None filename to open and pass to json.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() or json.loads None Returns: Type Description None Box object from json data View Source @classmethod def from_json ( cls , json_string : Optional [ str ] = None , filename : Optional [ Union[str, PathLike ] ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transform a json object string into a Box object. If the incoming json is a list, you must use BoxList.from_json. :param json_string: string to pass to `json.loads` :param filename: filename to open and pass to `json.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` or `json.loads` :return: Box object from json data \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_json ( json_string , filename = filename , encoding = encoding , errors = errors , ** kwargs ) if not isinstance ( data , dict ) : raise BoxError ( f \"json data not returned as a dictionary, but rather a {type(data).__name__}\" ) return cls ( data , ** box_args )","title":"from_json"},{"location":"reference/arti/graphs/#from_msgpack","text":"def from_msgpack ( msgpack_bytes : Optional [ bytes ] = None , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' View Source @classmethod def from_msgpack ( cls , msgpack_bytes : Optional [ bytes ] = None , filename : Optional [ Union[str, PathLike ] ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : raise BoxError ( 'msgpack is unavailable on this system, please install the \"msgpack\" package' )","title":"from_msgpack"},{"location":"reference/arti/graphs/#from_toml","text":"def from_toml ( toml_string : Optional [ str ] = None , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transforms a toml string or file into a Box object Parameters: Name Type Description Default toml_string None string to pass to toml.load None filename None filename to open and pass to toml.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() None Returns: Type Description None Box object View Source @classmethod def from_toml ( cls , toml_string : Optional [ str ] = None , filename : Optional [ Union[str, PathLike ] ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transforms a toml string or file into a Box object :param toml_string: string to pass to `toml.load` :param filename: filename to open and pass to `toml.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` :return: Box object \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_toml ( toml_string = toml_string , filename = filename , encoding = encoding , errors = errors ) return cls ( data , ** box_args )","title":"from_toml"},{"location":"reference/arti/graphs/#from_yaml","text":"def from_yaml ( yaml_string : Optional [ str ] = None , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transform a yaml object string into a Box object. By default will use SafeLoader. Parameters: Name Type Description Default yaml_string None string to pass to yaml.load None filename None filename to open and pass to yaml.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() or yaml.load None Returns: Type Description None Box object from yaml data View Source @classmethod def from_yaml ( cls , yaml_string : Optional [ str ] = None , filename : Optional [ Union[str, PathLike ] ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transform a yaml object string into a Box object. By default will use SafeLoader. :param yaml_string: string to pass to `yaml.load` :param filename: filename to open and pass to `yaml.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` or `yaml.load` :return: Box object from yaml data \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_yaml ( yaml_string = yaml_string , filename = filename , encoding = encoding , errors = errors , ** kwargs ) if not data : return cls ( ** box_args ) if not isinstance ( data , dict ) : raise BoxError ( f \"yaml data not returned as a dictionary but rather a {type(data).__name__}\" ) return cls ( data , ** box_args )","title":"from_yaml"},{"location":"reference/arti/graphs/#methods","text":"","title":"Methods"},{"location":"reference/arti/graphs/#clear","text":"def clear ( self ) D.clear() -> None. Remove all items from D. View Source def clear ( self ) : if self . _box_config [ \"frozen_box\" ]: raise BoxError ( \"Box is frozen\" ) super () . clear () self . _box_config [ \"__safe_keys\" ]. clear ()","title":"clear"},{"location":"reference/arti/graphs/#copy","text":"def copy ( self ) -> 'Box' D.copy() -> a shallow copy of D View Source def copy ( self ) -> \"Box\" : config = self . __box_config () config . pop ( \"box_namespace\" ) # Detach namespace ; it will be reassigned if we nest again return Box ( super (). copy (), ** config )","title":"copy"},{"location":"reference/arti/graphs/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/arti/graphs/#get","text":"def get ( self , key , default =< object object at 0x7f1a4d988080 > ) Return the value for key if key is in the dictionary, else default. View Source def get ( self , key , default = NO_DEFAULT ) : if key not in self : if default is NO_DEFAULT : if self . _box_config [ \"default_box\" ] and self . _box_config [ \"default_box_none_transform\" ] : return self . __get_default ( key ) else : return None if isinstance ( default , dict ) and not isinstance ( default , Box ) : return Box ( default ) if isinstance ( default , list ) and not isinstance ( default , box . BoxList ) : return box . BoxList ( default ) return default return self [ key ]","title":"get"},{"location":"reference/arti/graphs/#items","text":"def items ( self , dotted : bool = False ) D.items() -> a set-like object providing a view on D's items View Source def items ( self , dotted : Union [ bool ] = False ) : if not dotted : return super (). items () if not self . _box_config [ \"box_dots\" ] : raise BoxError ( \"Cannot return dotted keys as this Box does not have `box_dots` enabled\" ) return [ (k, self[k ] ) for k in self . keys ( dotted = True ) ]","title":"items"},{"location":"reference/arti/graphs/#keys","text":"def keys ( self , dotted : bool = False ) D.keys() -> a set-like object providing a view on D's keys View Source def keys ( self , dotted : Union [ bool ] = False ) : if not dotted : return super (). keys () if not self . _box_config [ \"box_dots\" ] : raise BoxError ( \"Cannot return dotted keys as this Box does not have `box_dots` enabled\" ) keys = set () for key , value in self . items () : added = False if isinstance ( key , str ) : if isinstance ( value , Box ) : for sub_key in value . keys ( dotted = True ) : keys . add ( f \"{key}.{sub_key}\" ) added = True elif isinstance ( value , box . BoxList ) : for pos in value . _dotted_helper () : keys . add ( f \"{key}{pos}\" ) added = True if not added : keys . add ( key ) return sorted ( keys , key = lambda x : str ( x ))","title":"keys"},{"location":"reference/arti/graphs/#merge_update","text":"def merge_update ( self , * args , ** kwargs ) View Source def merge_update ( self , * args , ** kwargs ) : merge_type = None if \"box_merge_lists\" in kwargs : merge_type = kwargs . pop ( \"box_merge_lists\" ) def convert_and_set ( k , v ) : intact_type = self . _box_config [ \"box_intact_types\" ] and isinstance ( v , self . _box_config [ \"box_intact_types\" ] ) if isinstance ( v , dict ) and not intact_type : # Box objects must be created in case they are already # in the ` converted ` box_config set v = self . _box_config [ \"box_class\" ] ( v , ** self . __box_config ( extra_namespace = k )) if k in self and isinstance ( self [ k ] , dict ) : self [ k ] . merge_update ( v ) return if isinstance ( v , list ) and not intact_type : v = box . BoxList ( v , ** self . __box_config ( extra_namespace = k )) if merge_type == \"extend\" and k in self and isinstance ( self [ k ] , list ) : self [ k ] . extend ( v ) return if merge_type == \"unique\" and k in self and isinstance ( self [ k ] , list ) : for item in v : if item not in self [ k ] : self [ k ] . append ( item ) return self . __setitem__ ( k , v ) if ( len ( args ) + int ( bool ( kwargs ))) > 1 : raise BoxTypeError ( f \"merge_update expected at most 1 argument, got {len(args) + int(bool(kwargs))}\" ) single_arg = next ( iter ( args ), None ) if single_arg : if hasattr ( single_arg , \"keys\" ) : for k in single_arg : convert_and_set ( k , single_arg [ k ] ) else : for k , v in single_arg : convert_and_set ( k , v ) for key in kwargs : convert_and_set ( key , kwargs [ key ] )","title":"merge_update"},{"location":"reference/arti/graphs/#pop","text":"def pop ( self , key , * args ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If the key is not found, return the default if given; otherwise, raise a KeyError. View Source def pop ( self , key , * args ) : if self . _box_config [ \"frozen_box\" ] : raise BoxError ( \"Box is frozen\" ) if args : if len ( args ) != 1 : raise BoxError ( 'pop() takes only one optional argument \"default\"' ) try : item = self [ key ] except KeyError : return args [ 0 ] else : del self [ key ] return item try : item = self [ key ] except KeyError : raise BoxKeyError ( f \"{key}\" ) from None else : del self [ key ] return item","title":"pop"},{"location":"reference/arti/graphs/#popitem","text":"def popitem ( self ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. View Source def popitem ( self ) : if self . _box_config [ \"frozen_box\" ]: raise BoxError ( \"Box is frozen\" ) try : key = next ( self . __iter__ ()) except StopIteration : raise BoxKeyError ( \"Empty box\" ) from None return key , self . pop ( key )","title":"popitem"},{"location":"reference/arti/graphs/#setdefault","text":"def setdefault ( self , item , default = None ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. View Source def setdefault ( self , item , default = None ) : if item in self : return self [ item ] if self . _box_config [ \"box_dots\" ] : if item in _get_dot_paths ( self ) : return self [ item ] if isinstance ( default , dict ) : default = self . _box_config [ \"box_class\" ] ( default , ** self . __box_config ( extra_namespace = item )) if isinstance ( default , list ) : default = box . BoxList ( default , ** self . __box_config ( extra_namespace = item )) self [ item ] = default return self [ item ]","title":"setdefault"},{"location":"reference/arti/graphs/#to_dict","text":"def to_dict ( self ) -> Dict Turn the Box and sub Boxes back into a native python dictionary. Returns: Type Description None python dictionary of this Box View Source def to_dict ( self ) -> Dict : \"\"\" Turn the Box and sub Boxes back into a native python dictionary. :return: python dictionary of this Box \"\"\" out_dict = dict ( self ) for k , v in out_dict . items () : if v is self : out_dict [ k ] = out_dict elif isinstance ( v , Box ) : out_dict [ k ] = v . to_dict () elif isinstance ( v , box . BoxList ) : out_dict [ k ] = v . to_list () return out_dict","title":"to_dict"},{"location":"reference/arti/graphs/#to_json","text":"def to_json ( self , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** json_kwargs ) Transform the Box object into a JSON string. Parameters: Name Type Description Default filename None If provided will save to file None encoding None File encoding None errors None How to handle encoding errors None json_kwargs None additional arguments to pass to json.dump(s) None Returns: Type Description None string of JSON (if no filename provided) View Source def to_json ( self , filename : Optional [ Union [ str , PathLike ]] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** json_kwargs , ) : \"\" \" Transform the Box object into a JSON string. :param filename: If provided will save to file :param encoding: File encoding :param errors: How to handle encoding errors :param json_kwargs: additional arguments to pass to json.dump(s) :return: string of JSON (if no filename provided) \"\" \" return _to_json(self.to_dict(), filename=filename, encoding=encoding, errors=errors, **json_kwargs)","title":"to_json"},{"location":"reference/arti/graphs/#to_msgpack","text":"def to_msgpack ( self , filename : Union [ str , os . PathLike , NoneType ] = None , ** kwargs ) View Source def to_msgpack(self, filename: Optional[Union[str, PathLike]] = None, **kwargs): raise BoxError('msgpack is unavailable on this system, please install the \"msgpack\" package')","title":"to_msgpack"},{"location":"reference/arti/graphs/#to_toml","text":"def to_toml ( self , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' ) Transform the Box object into a toml string. Parameters: Name Type Description Default filename None File to write toml object too None encoding None File encoding None errors None How to handle encoding errors None Returns: Type Description None string of TOML (if no filename provided) View Source def to_toml ( self , filename : Optional [ Union [ str , PathLike ]] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" ) : \"\" \" Transform the Box object into a toml string. :param filename: File to write toml object too :param encoding: File encoding :param errors: How to handle encoding errors :return: string of TOML (if no filename provided) \"\" \" return _to_toml(self.to_dict(), filename=filename, encoding=encoding, errors=errors)","title":"to_toml"},{"location":"reference/arti/graphs/#to_yaml","text":"def to_yaml ( self , filename : Union [ str , os . PathLike , NoneType ] = None , default_flow_style : bool = False , encoding : str = 'utf-8' , errors : str = 'strict' , ** yaml_kwargs ) Transform the Box object into a YAML string. Parameters: Name Type Description Default filename None If provided will save to file None default_flow_style None False will recursively dump dicts None encoding None File encoding None errors None How to handle encoding errors None yaml_kwargs None additional arguments to pass to yaml.dump None Returns: Type Description None string of YAML (if no filename provided) View Source def to_yaml ( self , filename : Optional [ Union [ str , PathLike ]] = None , default_flow_style : bool = False , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** yaml_kwargs , ) : \"\" \" Transform the Box object into a YAML string. :param filename: If provided will save to file :param default_flow_style: False will recursively dump dicts :param encoding: File encoding :param errors: How to handle encoding errors :param yaml_kwargs: additional arguments to pass to yaml.dump :return: string of YAML (if no filename provided) \"\" \" return _to_yaml( self.to_dict(), filename=filename, default_flow_style=default_flow_style, encoding=encoding, errors=errors, **yaml_kwargs, )","title":"to_yaml"},{"location":"reference/arti/graphs/#update","text":"def update ( self , * args , ** kwargs ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] View Source def update ( self , * args , ** kwargs ) : if self . _box_config [ \"frozen_box\" ] : raise BoxError ( \"Box is frozen\" ) if ( len ( args ) + int ( bool ( kwargs ))) > 1 : raise BoxTypeError ( f \"update expected at most 1 argument, got {len(args) + int(bool(kwargs))}\" ) single_arg = next ( iter ( args ), None ) if single_arg : if hasattr ( single_arg , \"keys\" ) : for k in single_arg : self . __convert_and_store ( k , single_arg [ k ] ) else : for k , v in single_arg : self . __convert_and_store ( k , v ) for k in kwargs : self . __convert_and_store ( k , kwargs [ k ] )","title":"update"},{"location":"reference/arti/graphs/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/arti/graphs/#walk","text":"def walk ( self , root : 'tuple[str, ...]' = () ) -> 'Iterator[tuple[str, _V]]' View Source def walk ( self , root : tuple [ str, ... ] = ()) -> Iterator [ tuple[str, _V ] ]: for k , v in self . items () : subroot = ( * root , k ) if isinstance ( v , TypedBox ) : yield from v . walk ( root = subroot ) else : yield \".\" . join ( subroot ), v # type : ignore [ misc ]","title":"walk"},{"location":"reference/arti/graphs/#graph","text":"class Graph ( __pydantic_self__ , ** data : Any ) View Source class Graph ( Model ) : \"\"\"Graph stores a web of Artifacts connected by Producers.\"\"\" name : str artifacts : ArtifactBox = Field ( default_factory = lambda : ArtifactBox ( ** BOX_KWARGS [ SEALED ] )) # The Backend * itself * should not affect the results of a Graph build , though the contents # certainly may ( eg : stored annotations ), so we avoid serializing it . This also prevent # embedding any credentials . backend : Backend [ Connection ] = Field ( default_factory = _get_memory_backend , exclude = True ) path_tags : frozendict [ str, str ] = frozendict () # Graph starts off sealed , but is opened within a ` with Graph (...) ` context _status : Optional [ bool ] = PrivateAttr ( None ) _artifact_to_key : frozendict [ Artifact, str ] = PrivateAttr ( frozendict ()) @validator ( \"artifacts\" ) @classmethod def _convert_artifacts ( cls , artifacts : ArtifactBox ) -> ArtifactBox : return ArtifactBox ( artifacts , ** BOX_KWARGS [ SEALED ] ) def __enter__ ( self ) -> Graph : if arti . context . graph is not None : raise ValueError ( f \"Another graph is being defined: {arti.context.graph}\" ) arti . context . graph = self self . _toggle ( OPEN ) return self def __exit__ ( self , exc_type : Optional [ type[BaseException ] ] , exc_value : Optional [ BaseException ] , exc_traceback : Optional [ TracebackType ] , ) -> None : arti . context . graph = None self . _toggle ( SEALED ) # Confirm the dependencies are acyclic TopologicalSorter ( self . dependencies ). prepare () def _toggle ( self , status : bool ) -> None : # The Graph object is \"frozen\" , so we must bypass the assignment checks . object . __setattr__ ( self , \"artifacts\" , ArtifactBox ( self . artifacts , ** BOX_KWARGS [ status ] )) self . _status = status self . _artifact_to_key = frozendict ( { artifact : key for key , artifact in self . artifacts . walk () } ) @property def artifact_to_key ( self ) -> frozendict [ Artifact, str ] : return self . _artifact_to_key @requires_sealed def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : return self . snapshot (). build ( executor ) @requires_sealed def snapshot ( self , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Identify a \" unique \" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" return GraphSnapshot . from_graph ( self , connection = connection ) @cached_property @requires_sealed def dependencies ( self ) -> NodeDependencies : artifact_deps = { artifact : ( frozenset ( { artifact . producer_output . producer } ) if artifact . producer_output is not None else frozenset () ) for _ , artifact in self . artifacts . walk () } producer_deps = { # NOTE : multi - output Producers will appear multiple times ( but be deduped ) producer_output . producer : frozenset ( producer_output . producer . inputs . values ()) for artifact in artifact_deps if ( producer_output : = artifact . producer_output ) is not None } return NodeDependencies ( artifact_deps | producer_deps ) @cached_property @requires_sealed def producers ( self ) -> frozenset [ Producer ] : return frozenset ( self . producer_outputs ) @cached_property @requires_sealed def producer_outputs ( self ) -> frozendict [ Producer, tuple[Artifact, ... ] ]: d = defaultdict [ Producer, dict[int, Artifact ] ] ( dict ) for _ , artifact in self . artifacts . walk () : if artifact . producer_output is None : continue output = artifact . producer_output d [ output.producer ][ output.position ] = artifact return frozendict ( ( producer , tuple ( artifacts_by_position [ i ] for i in sorted ( artifacts_by_position ))) for producer , artifacts_by_position in d . items () ) # TODO : io . read / write probably need a bit of sanity checking ( probably somewhere else ), eg : type # ~= view . Doing validation on the data , etc . Should some of this live on the View ? @requires_sealed def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , snapshot : Optional [ GraphSnapshot ] = None , connection : Optional [ Connection ] = None , ) -> Any : key = self . artifact_to_key [ artifact ] if annotation is None and view is None : raise ValueError ( \"Either `annotation` or `view` must be passed\" ) if annotation is not None and view is not None : raise ValueError ( \"Only one of `annotation` or `view` may be passed\" ) if annotation is not None : view = View . get_class_for ( annotation )( artifact_class = type ( artifact ), type = artifact . type , mode = \"READ\" ) view . check_annotation_compatibility ( annotation ) view . check_artifact_compatibility ( artifact ) assert view is not None # mypy gets mixed up with ^ if storage_partitions is None : # We want to allow reading raw Artifacts even if other raw Artifacts are missing ( which # prevents snapshotting ). if snapshot is None and artifact . producer_output is None : # NOTE : We 're not using read_artifact_partitions as the underlying data may have # changed. The backend may have pointers to old versions (which is expected), but we # only want to return the current values. storage_partitions = artifact.storage.discover_partitions() else: snapshot = snapshot or self.snapshot() with (connection or self.backend).connect() as conn: storage_partitions = conn.read_snapshot_partitions(snapshot, key, artifact) return io.read( type_=artifact.type, format=artifact.format, storage_partitions=storage_partitions, view=view, ) @requires_sealed def write( self, data: Any, *, artifact: Artifact, input_fingerprint: Fingerprint = Fingerprint.empty(), keys: CompositeKey = CompositeKey(), view: Optional[View] = None, snapshot: Optional[GraphSnapshot] = None, connection: Optional[Connection] = None, ) -> StoragePartition: key = self.artifact_to_key[artifact] if snapshot is not None and artifact.producer_output is None: raise ValueError( f\"Writing to a raw Artifact (`{key}`) with a GraphSnapshot is not supported.\" ) if view is None: view = View.get_class_for(type(data))( artifact_class=type(artifact), type=artifact.type, mode=\"WRITE\" ) view.check_annotation_compatibility(type(data)) view.check_artifact_compatibility(artifact) storage_partition = artifact.storage.generate_partition( input_fingerprint=input_fingerprint, keys=keys, with_content_fingerprint=False ) storage_partition = io.write( data, type_=artifact.type, format=artifact.format, storage_partition=storage_partition, view=view, ).with_content_fingerprint() # TODO: Should we only do this in bulk? We might want the backends to transparently batch # requests, but that' s not so friendly with the transient \".connect\" . with ( connection or self . backend ). connect () as conn : conn . write_artifact_partitions ( artifact , ( storage_partition ,)) # Skip linking this partition to the snapshot if it affects raw Artifacts ( which would # trigger an id change ). if snapshot is not None and artifact . producer_output is not None : conn . write_snapshot_partitions ( snapshot , key , artifact , ( storage_partition ,)) return storage_partition","title":"Graph"},{"location":"reference/arti/graphs/#ancestors-in-mro_1","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/graphs/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/graphs/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/graphs/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/graphs/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/graphs/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/graphs/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/graphs/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/graphs/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/graphs/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/graphs/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/graphs/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/graphs/#instance-variables","text":"artifact_to_key fingerprint","title":"Instance variables"},{"location":"reference/arti/graphs/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/graphs/#build","text":"def build ( self , executor : 'Optional[Executor]' = None ) -> 'GraphSnapshot' View Source @requires_sealed def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : return self . snapshot (). build ( executor )","title":"build"},{"location":"reference/arti/graphs/#copy_1","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/graphs/#dependencies","text":"def dependencies ( ... )","title":"dependencies"},{"location":"reference/arti/graphs/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/graphs/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/graphs/#producer_outputs","text":"def producer_outputs ( ... )","title":"producer_outputs"},{"location":"reference/arti/graphs/#producers","text":"def producers ( ... )","title":"producers"},{"location":"reference/arti/graphs/#read","text":"def read ( self , artifact : 'Artifact' , * , annotation : 'Optional[Any]' = None , storage_partitions : 'Optional[Sequence[StoragePartition]]' = None , view : 'Optional[View]' = None , snapshot : 'Optional[GraphSnapshot]' = None , connection : 'Optional[Connection]' = None ) -> 'Any' View Source @requires_sealed def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , snapshot : Optional [ GraphSnapshot ] = None , connection : Optional [ Connection ] = None , ) -> Any : key = self . artifact_to_key [ artifact ] if annotation is None and view is None : raise ValueError ( \"Either `annotation` or `view` must be passed\" ) if annotation is not None and view is not None : raise ValueError ( \"Only one of `annotation` or `view` may be passed\" ) if annotation is not None : view = View . get_class_for ( annotation )( artifact_class = type ( artifact ), type = artifact . type , mode = \"READ\" ) view . check_annotation_compatibility ( annotation ) view . check_artifact_compatibility ( artifact ) assert view is not None # mypy gets mixed up with ^ if storage_partitions is None : # We want to allow reading raw Artifacts even if other raw Artifacts are missing ( which # prevents snapshotting ). if snapshot is None and artifact . producer_output is None : # NOTE : We ' re not using read_artifact_partitions as the underlying data may have # changed . The backend may have pointers to old versions ( which is expected ), but we # only want to return the current values . storage_partitions = artifact . storage . discover_partitions () else : snapshot = snapshot or self . snapshot () with ( connection or self . backend ). connect () as conn : storage_partitions = conn . read_snapshot_partitions ( snapshot , key , artifact ) return io . read ( type_ = artifact . type , format = artifact . format , storage_partitions = storage_partitions , view = view , )","title":"read"},{"location":"reference/arti/graphs/#snapshot","text":"def snapshot ( self , * , connection : 'Optional[Connection]' = None ) -> 'GraphSnapshot' Identify a \"unique\" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a snapshot of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. View Source @requires_sealed def snapshot ( self , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Identify a \" unique \" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" return GraphSnapshot . from_graph ( self , connection = connection )","title":"snapshot"},{"location":"reference/arti/graphs/#write","text":"def write ( self , data : 'Any' , * , artifact : 'Artifact' , input_fingerprint : 'Fingerprint' = Fingerprint ( key = None ), keys : 'CompositeKey' = {}, view : 'Optional[View]' = None , snapshot : 'Optional[GraphSnapshot]' = None , connection : 'Optional[Connection]' = None ) -> 'StoragePartition' View Source @requires_sealed def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , snapshot : Optional [ GraphSnapshot ] = None , connection : Optional [ Connection ] = None , ) -> StoragePartition : key = self . artifact_to_key [ artifact ] if snapshot is not None and artifact . producer_output is None : raise ValueError ( f \"Writing to a raw Artifact (`{key}`) with a GraphSnapshot is not supported.\" ) if view is None : view = View . get_class_for ( type ( data ))( artifact_class = type ( artifact ), type = artifact . type , mode = \"WRITE\" ) view . check_annotation_compatibility ( type ( data )) view . check_artifact_compatibility ( artifact ) storage_partition = artifact . storage . generate_partition ( input_fingerprint = input_fingerprint , keys = keys , with_content_fingerprint = False ) storage_partition = io . write ( data , type_ = artifact . type , format = artifact . format , storage_partition = storage_partition , view = view , ). with_content_fingerprint () # TODO : Should we only do this in bulk ? We might want the backends to transparently batch # requests , but that ' s not so friendly with the transient \".connect\" . with ( connection or self . backend ). connect () as conn : conn . write_artifact_partitions ( artifact , ( storage_partition ,)) # Skip linking this partition to the snapshot if it affects raw Artifacts ( which would # trigger an id change ). if snapshot is not None and artifact . producer_output is not None : conn . write_snapshot_partitions ( snapshot , key , artifact , ( storage_partition ,)) return storage_partition","title":"write"},{"location":"reference/arti/graphs/#graphsnapshot","text":"class GraphSnapshot ( __pydantic_self__ , ** data : Any ) View Source class GraphSnapshot ( Model ) : \"\"\"GraphSnapshot represents the state of a Graph and the referenced raw data at a point in time. GraphSnapshot encodes the structure of the Graph plus a snapshot of the raw Artifact data (partition kinds and contents) at a point in time. Any change that would affect data should prompt an ID change. \"\"\" id : Fingerprint graph : Graph @property def artifacts ( self ) -> ArtifactBox : return self . graph . artifacts @property def backend ( self ) -> Backend [ Connection ] : return self . graph . backend @property def name ( self ) -> str : return self . graph . name @classmethod # TODO : Should this use a ( TTL ) cache ? Raw data changes ( especially in tests ) still need to be detected . def from_graph ( cls , graph : Graph , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Snapshot the Graph and all existing raw data. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifact data during Producer builds. \"\"\" # TODO : Resolve and statically set all available fingerprints . Specifically , we should pin # the Producer . fingerprint , which may by dynamic ( eg : version is a Timestamp ). Unbuilt # Artifact ( partitions ) won 't be fully resolved yet. snapshot_id, known_artifact_partitions = graph.fingerprint, dict[str, StoragePartitions]() for node, _ in graph.dependencies.items(): snapshot_id = snapshot_id.combine(node.fingerprint) if isinstance(node, Artifact): key = graph.artifact_to_key[node] snapshot_id = snapshot_id.combine(Fingerprint.from_string(key)) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we' ll have to handle things a bit # differently depending on if the external Artifacts are Produced ( in an upstream # Graph ) or not . if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . storage . discover_partitions () ) if not known_artifact_partitions [ key ] : content_str = \"partitions\" if is_partitioned ( node . type ) else \"data\" raise ValueError ( f \"No {content_str} found for `{key}`: {node}\" ) snapshot_id = snapshot_id . combine ( *[ partition.fingerprint for partition in known_artifact_partitions[key ] ] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma : no cover # NOTE : This shouldn ' t happen unless the logic above is faulty . raise ValueError ( \"Fingerprint is empty!\" ) snapshot = cls ( graph = graph , id = snapshot_id ) # Write the discovered partitions ( if not already known ) and link to this new snapshot . with ( connection or snapshot . backend ). connect () as conn : conn . write_graph ( graph ) conn . write_snapshot ( snapshot ) for key , partitions in known_artifact_partitions . items () : conn . write_artifact_and_graph_partitions ( snapshot , key , snapshot . artifacts [ key ] , partitions ) return snapshot def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : if executor is None : from arti . executors . local import LocalExecutor executor = LocalExecutor () executor . build ( self ) return self def tag ( self , tag : str , * , overwrite : bool = False , connection : Optional [ Connection ] = None ) -> None : with ( connection or self . backend ). connect () as conn : conn . write_snapshot_tag ( self , tag , overwrite ) @classmethod def from_tag ( cls , name : str , tag : str , * , connectable : Union [ Backend[Connection ] , Connection ] ) -> GraphSnapshot : with connectable . connect () as conn : return conn . read_snapshot_tag ( name , tag ) def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> Any : return self . graph . read ( artifact , annotation = annotation , storage_partitions = storage_partitions , view = view , snapshot = self , connection = connection , ) def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> StoragePartition : return self . graph . write ( data , artifact = artifact , input_fingerprint = input_fingerprint , keys = keys , view = view , snapshot = self , connection = connection , )","title":"GraphSnapshot"},{"location":"reference/arti/graphs/#ancestors-in-mro_2","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/graphs/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/arti/graphs/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/arti/graphs/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/graphs/#from_graph","text":"def from_graph ( graph : 'Graph' , * , connection : 'Optional[Connection]' = None ) -> 'GraphSnapshot' Snapshot the Graph and all existing raw data. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifact data during Producer builds. View Source @classmethod # TODO : Should this use a ( TTL ) cache ? Raw data changes ( especially in tests ) still need to be detected . def from_graph ( cls , graph : Graph , * , connection : Optional [ Connection ] = None ) -> GraphSnapshot : \"\"\"Snapshot the Graph and all existing raw data. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifact data during Producer builds. \"\"\" # TODO : Resolve and statically set all available fingerprints . Specifically , we should pin # the Producer . fingerprint , which may by dynamic ( eg : version is a Timestamp ). Unbuilt # Artifact ( partitions ) won 't be fully resolved yet. snapshot_id, known_artifact_partitions = graph.fingerprint, dict[str, StoragePartitions]() for node, _ in graph.dependencies.items(): snapshot_id = snapshot_id.combine(node.fingerprint) if isinstance(node, Artifact): key = graph.artifact_to_key[node] snapshot_id = snapshot_id.combine(Fingerprint.from_string(key)) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we' ll have to handle things a bit # differently depending on if the external Artifacts are Produced ( in an upstream # Graph ) or not . if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . storage . discover_partitions () ) if not known_artifact_partitions [ key ] : content_str = \"partitions\" if is_partitioned ( node . type ) else \"data\" raise ValueError ( f \"No {content_str} found for `{key}`: {node}\" ) snapshot_id = snapshot_id . combine ( *[ partition.fingerprint for partition in known_artifact_partitions[key ] ] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma : no cover # NOTE : This shouldn ' t happen unless the logic above is faulty . raise ValueError ( \"Fingerprint is empty!\" ) snapshot = cls ( graph = graph , id = snapshot_id ) # Write the discovered partitions ( if not already known ) and link to this new snapshot . with ( connection or snapshot . backend ). connect () as conn : conn . write_graph ( graph ) conn . write_snapshot ( snapshot ) for key , partitions in known_artifact_partitions . items () : conn . write_artifact_and_graph_partitions ( snapshot , key , snapshot . artifacts [ key ] , partitions ) return snapshot","title":"from_graph"},{"location":"reference/arti/graphs/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/graphs/#from_tag","text":"def from_tag ( name : 'str' , tag : 'str' , * , connectable : 'Union[Backend[Connection], Connection]' ) -> 'GraphSnapshot' View Source @classmethod def from_tag ( cls , name : str , tag : str , * , connectable : Union [ Backend[Connection ] , Connection ] ) -> GraphSnapshot : with connectable . connect () as conn : return conn . read_snapshot_tag ( name , tag )","title":"from_tag"},{"location":"reference/arti/graphs/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/graphs/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/graphs/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/graphs/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/graphs/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/graphs/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/graphs/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/graphs/#instance-variables_1","text":"artifacts backend fingerprint name","title":"Instance variables"},{"location":"reference/arti/graphs/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/graphs/#build_1","text":"def build ( self , executor : 'Optional[Executor]' = None ) -> 'GraphSnapshot' View Source def build ( self , executor : Optional [ Executor ] = None ) -> GraphSnapshot : if executor is None : from arti.executors.local import LocalExecutor executor = LocalExecutor () executor . build ( self ) return self","title":"build"},{"location":"reference/arti/graphs/#copy_2","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/graphs/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/graphs/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/graphs/#read_1","text":"def read ( self , artifact : 'Artifact' , * , annotation : 'Optional[Any]' = None , storage_partitions : 'Optional[Sequence[StoragePartition]]' = None , view : 'Optional[View]' = None , connection : 'Optional[Connection]' = None ) -> 'Any' View Source def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> Any : return self . graph . read ( artifact , annotation = annotation , storage_partitions = storage_partitions , view = view , snapshot = self , connection = connection , )","title":"read"},{"location":"reference/arti/graphs/#tag","text":"def tag ( self , tag : 'str' , * , overwrite : 'bool' = False , connection : 'Optional[Connection]' = None ) -> 'None' View Source def tag ( self , tag : str , * , overwrite : bool = False , connection : Optional [ Connection ] = None ) -> None : with ( connection or self . backend ). connect () as conn : conn . write_snapshot_tag ( self , tag , overwrite )","title":"tag"},{"location":"reference/arti/graphs/#write_1","text":"def write ( self , data : 'Any' , * , artifact : 'Artifact' , input_fingerprint : 'Fingerprint' = Fingerprint ( key = None ), keys : 'CompositeKey' = {}, view : 'Optional[View]' = None , connection : 'Optional[Connection]' = None ) -> 'StoragePartition' View Source def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , connection : Optional [ Connection ] = None , ) -> StoragePartition : return self . graph . write ( data , artifact = artifact , input_fingerprint = input_fingerprint , keys = keys , view = view , snapshot = self , connection = connection , )","title":"write"},{"location":"reference/arti/partitions/","text":"Module arti.partitions None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import abc from datetime import date from inspect import getattr_static from typing import Any , ClassVar from arti.fingerprints import Fingerprint from arti.internal.models import Model from arti.internal.utils import classproperty , frozendict , register from arti.types import Collection , Date , Int8 , Int16 , Int32 , Int64 , Null , Type class key_component ( property ): pass class PartitionKey ( Model ): _abstract_ = True _by_type_ : ClassVar [ dict [ type [ Type ], type [ PartitionKey ]]] = {} default_key_components : ClassVar [ frozendict [ str , str ]] matching_type : ClassVar [ type [ Type ]] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return for attr in ( \"default_key_components\" , \"matching_type\" ): if not hasattr ( cls , attr ): raise TypeError ( f \" { cls . __name__ } must set ` { attr } `\" ) if unknown := set ( cls . default_key_components ) - cls . key_components : raise TypeError ( f \"Unknown key_components in { cls . __name__ } .default_key_components: { unknown } \" ) register ( cls . _by_type_ , cls . matching_type , cls ) @classproperty def key_components ( cls ) -> frozenset [ str ]: return frozenset ( cls . __fields__ ) | frozenset ( name for name in dir ( cls ) if isinstance ( getattr_static ( cls , name ), key_component ) ) @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : raise NotImplementedError ( f \"Unable to parse ' { cls . __name__ } ' from: { key_components } \" ) @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ]: return cls . _by_type_ [ type ( type_ )] @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ): return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items ()} ) # CompositeKey is the set of named PartitionKeys that uniquely identify a single partition. CompositeKey = frozendict [ str , PartitionKey ] CompositeKeyTypes = frozendict [ str , type [ PartitionKey ]] NotPartitioned = CompositeKey () class DateKey ( PartitionKey ): default_key_components : ClassVar [ frozendict [ str , str ]] = frozendict ( Y = \"Y\" , m = \"m:02\" , d = \"d:02\" ) matching_type = Date key : date @key_component def Y ( self ) -> int : return self . key . year @key_component def m ( self ) -> int : return self . key . month @key_component def d ( self ) -> int : return self . key . day @key_component def iso ( self ) -> str : return self . key . isoformat () @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = date . fromisoformat ( key_components [ \"key\" ])) if names == { \"iso\" }: return cls ( key = date . fromisoformat ( key_components [ \"iso\" ])) if names == { \"Y\" , \"m\" , \"d\" }: return cls ( key = date ( * [ int ( key_components [ k ]) for k in ( \"Y\" , \"m\" , \"d\" )])) return super () . from_key_components ( ** key_components ) class _IntKey ( PartitionKey ): _abstract_ = True default_key_components : ClassVar [ frozendict [ str , str ]] = frozendict ( key = \"key\" ) key : int @key_component def hex ( self ) -> str : return hex ( self . key ) @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ])) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ], base = 16 )) return super () . from_key_components ( ** key_components ) class Int8Key ( _IntKey ): matching_type = Int8 class Int16Key ( _IntKey ): matching_type = Int16 class Int32Key ( _IntKey ): matching_type = Int32 class Int64Key ( _IntKey ): matching_type = Int64 class NullKey ( PartitionKey ): default_key_components : ClassVar [ frozendict [ str , str ]] = frozendict ( key = \"key\" ) matching_type = Null key : None = None @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : if set ( key_components ) == { \"key\" }: if key_components [ \"key\" ] != \"None\" : raise ValueError ( f \"' { cls . __name__ } ' can only be used with 'None'!\" ) return cls () return super () . from_key_components ( ** key_components ) InputFingerprints = frozendict [ CompositeKey , Fingerprint ] Variables CompositeKey CompositeKeyTypes InputFingerprints NotPartitioned Classes DateKey class DateKey ( __pydantic_self__ , ** data : Any ) View Source class DateKey ( PartitionKey ) : default_key_components : ClassVar [ frozendict[str, str ] ] = frozendict ( Y = \"Y\" , m = \"m:02\" , d = \"d:02\" ) matching_type = Date key : date @key_component def Y ( self ) -> int : return self . key . year @key_component def m ( self ) -> int : return self . key . month @key_component def d ( self ) -> int : return self . key . day @key_component def iso ( self ) -> str : return self . key . isoformat () @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = date . fromisoformat ( key_components [ \"key\" ] )) if names == { \"iso\" }: return cls ( key = date . fromisoformat ( key_components [ \"iso\" ] )) if names == { \"Y\" , \"m\" , \"d\" }: return cls ( key = date ( *[ int(key_components[k ] ) for k in ( \"Y\" , \"m\" , \"d\" ) ] )) return super (). from_key_components ( ** key_components ) Ancestors (in MRO) arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config default_key_components key_components matching_type Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_key_components def from_key_components ( ** key_components : 'str' ) -> 'PartitionKey' View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = date . fromisoformat ( key_components [ \"key\" ] )) if names == { \"iso\" }: return cls ( key = date . fromisoformat ( key_components [ \"iso\" ] )) if names == { \"Y\" , \"m\" , \"d\" }: return cls ( key = date ( *[ int(key_components[k ] ) for k in ( \"Y\" , \"m\" , \"d\" ) ] )) return super (). from_key_components ( ** key_components ) from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( type_ : 'Type' ) -> 'type[PartitionKey]' View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' types_from def types_from ( type_ : 'Type' ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables Y d fingerprint iso m Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int16Key class Int16Key ( __pydantic_self__ , ** data : Any ) View Source class Int16Key ( _IntKey ): matching_type = Int16 Ancestors (in MRO) arti.partitions._IntKey arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config default_key_components key_components matching_type Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_key_components def from_key_components ( ** key_components : 'str' ) -> 'PartitionKey' View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ] )) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ] , base = 16 )) return super (). from_key_components ( ** key_components ) from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( type_ : 'Type' ) -> 'type[PartitionKey]' View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' types_from def types_from ( type_ : 'Type' ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint hex Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int32Key class Int32Key ( __pydantic_self__ , ** data : Any ) View Source class Int32Key ( _IntKey ): matching_type = Int32 Ancestors (in MRO) arti.partitions._IntKey arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config default_key_components key_components matching_type Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_key_components def from_key_components ( ** key_components : 'str' ) -> 'PartitionKey' View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ] )) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ] , base = 16 )) return super (). from_key_components ( ** key_components ) from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( type_ : 'Type' ) -> 'type[PartitionKey]' View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' types_from def types_from ( type_ : 'Type' ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint hex Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int64Key class Int64Key ( __pydantic_self__ , ** data : Any ) View Source class Int64Key ( _IntKey ): matching_type = Int64 Ancestors (in MRO) arti.partitions._IntKey arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config default_key_components key_components matching_type Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_key_components def from_key_components ( ** key_components : 'str' ) -> 'PartitionKey' View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ] )) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ] , base = 16 )) return super (). from_key_components ( ** key_components ) from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( type_ : 'Type' ) -> 'type[PartitionKey]' View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' types_from def types_from ( type_ : 'Type' ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint hex Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int8Key class Int8Key ( __pydantic_self__ , ** data : Any ) View Source class Int8Key ( _IntKey ): matching_type = Int8 Ancestors (in MRO) arti.partitions._IntKey arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config default_key_components key_components matching_type Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_key_components def from_key_components ( ** key_components : 'str' ) -> 'PartitionKey' View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ] )) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ] , base = 16 )) return super (). from_key_components ( ** key_components ) from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( type_ : 'Type' ) -> 'type[PartitionKey]' View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' types_from def types_from ( type_ : 'Type' ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint hex Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . NullKey class NullKey ( __pydantic_self__ , ** data : Any ) View Source class NullKey ( PartitionKey ) : default_key_components : ClassVar [ frozendict[str, str ] ] = frozendict ( key = \"key\" ) matching_type = Null key : None = None @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : if set ( key_components ) == { \"key\" }: if key_components [ \"key\" ] != \"None\" : raise ValueError ( f \"'{cls.__name__}' can only be used with 'None'!\" ) return cls () return super (). from_key_components ( ** key_components ) Ancestors (in MRO) arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config default_key_components key_components matching_type Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_key_components def from_key_components ( ** key_components : 'str' ) -> 'PartitionKey' View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : if set ( key_components ) == { \"key\" }: if key_components [ \"key\" ] != \"None\" : raise ValueError ( f \"'{cls.__name__}' can only be used with 'None'!\" ) return cls () return super (). from_key_components ( ** key_components ) from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( type_ : 'Type' ) -> 'type[PartitionKey]' View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' types_from def types_from ( type_ : 'Type' ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . PartitionKey class PartitionKey ( __pydantic_self__ , ** data : Any ) View Source class PartitionKey ( Model ) : _abstract_ = True _by_type_ : ClassVar [ dict[type[Type ] , type [ PartitionKey ] ]] = {} default_key_components : ClassVar [ frozendict[str, str ] ] matching_type : ClassVar [ type[Type ] ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return for attr in ( \"default_key_components\" , \"matching_type\" ) : if not hasattr ( cls , attr ) : raise TypeError ( f \"{cls.__name__} must set `{attr}`\" ) if unknown : = set ( cls . default_key_components ) - cls . key_components : raise TypeError ( f \"Unknown key_components in {cls.__name__}.default_key_components: {unknown}\" ) register ( cls . _by_type_ , cls . matching_type , cls ) @classproperty def key_components ( cls ) -> frozenset [ str ] : return frozenset ( cls . __fields__ ) | frozenset ( name for name in dir ( cls ) if isinstance ( getattr_static ( cls , name ), key_component ) ) @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : raise NotImplementedError ( f \"Unable to parse '{cls.__name__}' from: {key_components}\" ) @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ] @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.partitions.DateKey arti.partitions._IntKey arti.partitions.NullKey Class variables Config key_components Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_key_components def from_key_components ( ** key_components : 'str' ) -> 'PartitionKey' View Source @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : raise NotImplementedError ( f \"Unable to parse '{cls.__name__}' from: {key_components}\" ) from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( type_ : 'Type' ) -> 'type[PartitionKey]' View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' types_from def types_from ( type_ : 'Type' ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . key_component class key_component ( / , * args , ** kwargs ) View Source class key_component ( property ): pass Ancestors (in MRO) builtins.property Class variables fdel fget fset Methods deleter def deleter ( ... ) Descriptor to obtain a copy of the property with a different deleter. getter def getter ( ... ) Descriptor to obtain a copy of the property with a different getter. setter def setter ( ... ) Descriptor to obtain a copy of the property with a different setter.","title":"Partitions"},{"location":"reference/arti/partitions/#module-artipartitions","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import abc from datetime import date from inspect import getattr_static from typing import Any , ClassVar from arti.fingerprints import Fingerprint from arti.internal.models import Model from arti.internal.utils import classproperty , frozendict , register from arti.types import Collection , Date , Int8 , Int16 , Int32 , Int64 , Null , Type class key_component ( property ): pass class PartitionKey ( Model ): _abstract_ = True _by_type_ : ClassVar [ dict [ type [ Type ], type [ PartitionKey ]]] = {} default_key_components : ClassVar [ frozendict [ str , str ]] matching_type : ClassVar [ type [ Type ]] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return for attr in ( \"default_key_components\" , \"matching_type\" ): if not hasattr ( cls , attr ): raise TypeError ( f \" { cls . __name__ } must set ` { attr } `\" ) if unknown := set ( cls . default_key_components ) - cls . key_components : raise TypeError ( f \"Unknown key_components in { cls . __name__ } .default_key_components: { unknown } \" ) register ( cls . _by_type_ , cls . matching_type , cls ) @classproperty def key_components ( cls ) -> frozenset [ str ]: return frozenset ( cls . __fields__ ) | frozenset ( name for name in dir ( cls ) if isinstance ( getattr_static ( cls , name ), key_component ) ) @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : raise NotImplementedError ( f \"Unable to parse ' { cls . __name__ } ' from: { key_components } \" ) @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ]: return cls . _by_type_ [ type ( type_ )] @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ): return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items ()} ) # CompositeKey is the set of named PartitionKeys that uniquely identify a single partition. CompositeKey = frozendict [ str , PartitionKey ] CompositeKeyTypes = frozendict [ str , type [ PartitionKey ]] NotPartitioned = CompositeKey () class DateKey ( PartitionKey ): default_key_components : ClassVar [ frozendict [ str , str ]] = frozendict ( Y = \"Y\" , m = \"m:02\" , d = \"d:02\" ) matching_type = Date key : date @key_component def Y ( self ) -> int : return self . key . year @key_component def m ( self ) -> int : return self . key . month @key_component def d ( self ) -> int : return self . key . day @key_component def iso ( self ) -> str : return self . key . isoformat () @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = date . fromisoformat ( key_components [ \"key\" ])) if names == { \"iso\" }: return cls ( key = date . fromisoformat ( key_components [ \"iso\" ])) if names == { \"Y\" , \"m\" , \"d\" }: return cls ( key = date ( * [ int ( key_components [ k ]) for k in ( \"Y\" , \"m\" , \"d\" )])) return super () . from_key_components ( ** key_components ) class _IntKey ( PartitionKey ): _abstract_ = True default_key_components : ClassVar [ frozendict [ str , str ]] = frozendict ( key = \"key\" ) key : int @key_component def hex ( self ) -> str : return hex ( self . key ) @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ])) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ], base = 16 )) return super () . from_key_components ( ** key_components ) class Int8Key ( _IntKey ): matching_type = Int8 class Int16Key ( _IntKey ): matching_type = Int16 class Int32Key ( _IntKey ): matching_type = Int32 class Int64Key ( _IntKey ): matching_type = Int64 class NullKey ( PartitionKey ): default_key_components : ClassVar [ frozendict [ str , str ]] = frozendict ( key = \"key\" ) matching_type = Null key : None = None @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : if set ( key_components ) == { \"key\" }: if key_components [ \"key\" ] != \"None\" : raise ValueError ( f \"' { cls . __name__ } ' can only be used with 'None'!\" ) return cls () return super () . from_key_components ( ** key_components ) InputFingerprints = frozendict [ CompositeKey , Fingerprint ]","title":"Module arti.partitions"},{"location":"reference/arti/partitions/#variables","text":"CompositeKey CompositeKeyTypes InputFingerprints NotPartitioned","title":"Variables"},{"location":"reference/arti/partitions/#classes","text":"","title":"Classes"},{"location":"reference/arti/partitions/#datekey","text":"class DateKey ( __pydantic_self__ , ** data : Any ) View Source class DateKey ( PartitionKey ) : default_key_components : ClassVar [ frozendict[str, str ] ] = frozendict ( Y = \"Y\" , m = \"m:02\" , d = \"d:02\" ) matching_type = Date key : date @key_component def Y ( self ) -> int : return self . key . year @key_component def m ( self ) -> int : return self . key . month @key_component def d ( self ) -> int : return self . key . day @key_component def iso ( self ) -> str : return self . key . isoformat () @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = date . fromisoformat ( key_components [ \"key\" ] )) if names == { \"iso\" }: return cls ( key = date . fromisoformat ( key_components [ \"iso\" ] )) if names == { \"Y\" , \"m\" , \"d\" }: return cls ( key = date ( *[ int(key_components[k ] ) for k in ( \"Y\" , \"m\" , \"d\" ) ] )) return super (). from_key_components ( ** key_components )","title":"DateKey"},{"location":"reference/arti/partitions/#ancestors-in-mro","text":"arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/partitions/#class-variables","text":"Config default_key_components key_components matching_type","title":"Class variables"},{"location":"reference/arti/partitions/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/partitions/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/partitions/#from_key_components","text":"def from_key_components ( ** key_components : 'str' ) -> 'PartitionKey' View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = date . fromisoformat ( key_components [ \"key\" ] )) if names == { \"iso\" }: return cls ( key = date . fromisoformat ( key_components [ \"iso\" ] )) if names == { \"Y\" , \"m\" , \"d\" }: return cls ( key = date ( *[ int(key_components[k ] ) for k in ( \"Y\" , \"m\" , \"d\" ) ] )) return super (). from_key_components ( ** key_components )","title":"from_key_components"},{"location":"reference/arti/partitions/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/partitions/#get_class_for","text":"def get_class_for ( type_ : 'Type' ) -> 'type[PartitionKey]' View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ]","title":"get_class_for"},{"location":"reference/arti/partitions/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/partitions/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/partitions/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/partitions/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/partitions/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/partitions/#types_from","text":"def types_from ( type_ : 'Type' ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"types_from"},{"location":"reference/arti/partitions/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/partitions/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/partitions/#instance-variables","text":"Y d fingerprint iso m","title":"Instance variables"},{"location":"reference/arti/partitions/#methods","text":"","title":"Methods"},{"location":"reference/arti/partitions/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/partitions/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/partitions/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/partitions/#int16key","text":"class Int16Key ( __pydantic_self__ , ** data : Any ) View Source class Int16Key ( _IntKey ): matching_type = Int16","title":"Int16Key"},{"location":"reference/arti/partitions/#ancestors-in-mro_1","text":"arti.partitions._IntKey arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/partitions/#class-variables_1","text":"Config default_key_components key_components matching_type","title":"Class variables"},{"location":"reference/arti/partitions/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/partitions/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/partitions/#from_key_components_1","text":"def from_key_components ( ** key_components : 'str' ) -> 'PartitionKey' View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ] )) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ] , base = 16 )) return super (). from_key_components ( ** key_components )","title":"from_key_components"},{"location":"reference/arti/partitions/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/partitions/#get_class_for_1","text":"def get_class_for ( type_ : 'Type' ) -> 'type[PartitionKey]' View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ]","title":"get_class_for"},{"location":"reference/arti/partitions/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/partitions/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/partitions/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/partitions/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/partitions/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/partitions/#types_from_1","text":"def types_from ( type_ : 'Type' ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"types_from"},{"location":"reference/arti/partitions/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/partitions/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/partitions/#instance-variables_1","text":"fingerprint hex","title":"Instance variables"},{"location":"reference/arti/partitions/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/partitions/#copy_1","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/partitions/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/partitions/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/partitions/#int32key","text":"class Int32Key ( __pydantic_self__ , ** data : Any ) View Source class Int32Key ( _IntKey ): matching_type = Int32","title":"Int32Key"},{"location":"reference/arti/partitions/#ancestors-in-mro_2","text":"arti.partitions._IntKey arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/partitions/#class-variables_2","text":"Config default_key_components key_components matching_type","title":"Class variables"},{"location":"reference/arti/partitions/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/arti/partitions/#construct_2","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/partitions/#from_key_components_2","text":"def from_key_components ( ** key_components : 'str' ) -> 'PartitionKey' View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ] )) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ] , base = 16 )) return super (). from_key_components ( ** key_components )","title":"from_key_components"},{"location":"reference/arti/partitions/#from_orm_2","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/partitions/#get_class_for_2","text":"def get_class_for ( type_ : 'Type' ) -> 'type[PartitionKey]' View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ]","title":"get_class_for"},{"location":"reference/arti/partitions/#parse_file_2","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/partitions/#parse_obj_2","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/partitions/#parse_raw_2","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/partitions/#schema_2","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/partitions/#schema_json_2","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/partitions/#types_from_2","text":"def types_from ( type_ : 'Type' ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"types_from"},{"location":"reference/arti/partitions/#update_forward_refs_2","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/partitions/#validate_2","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/partitions/#instance-variables_2","text":"fingerprint hex","title":"Instance variables"},{"location":"reference/arti/partitions/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/partitions/#copy_2","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/partitions/#dict_2","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/partitions/#json_2","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/partitions/#int64key","text":"class Int64Key ( __pydantic_self__ , ** data : Any ) View Source class Int64Key ( _IntKey ): matching_type = Int64","title":"Int64Key"},{"location":"reference/arti/partitions/#ancestors-in-mro_3","text":"arti.partitions._IntKey arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/partitions/#class-variables_3","text":"Config default_key_components key_components matching_type","title":"Class variables"},{"location":"reference/arti/partitions/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/arti/partitions/#construct_3","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/partitions/#from_key_components_3","text":"def from_key_components ( ** key_components : 'str' ) -> 'PartitionKey' View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ] )) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ] , base = 16 )) return super (). from_key_components ( ** key_components )","title":"from_key_components"},{"location":"reference/arti/partitions/#from_orm_3","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/partitions/#get_class_for_3","text":"def get_class_for ( type_ : 'Type' ) -> 'type[PartitionKey]' View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ]","title":"get_class_for"},{"location":"reference/arti/partitions/#parse_file_3","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/partitions/#parse_obj_3","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/partitions/#parse_raw_3","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/partitions/#schema_3","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/partitions/#schema_json_3","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/partitions/#types_from_3","text":"def types_from ( type_ : 'Type' ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"types_from"},{"location":"reference/arti/partitions/#update_forward_refs_3","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/partitions/#validate_3","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/partitions/#instance-variables_3","text":"fingerprint hex","title":"Instance variables"},{"location":"reference/arti/partitions/#methods_3","text":"","title":"Methods"},{"location":"reference/arti/partitions/#copy_3","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/partitions/#dict_3","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/partitions/#json_3","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/partitions/#int8key","text":"class Int8Key ( __pydantic_self__ , ** data : Any ) View Source class Int8Key ( _IntKey ): matching_type = Int8","title":"Int8Key"},{"location":"reference/arti/partitions/#ancestors-in-mro_4","text":"arti.partitions._IntKey arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/partitions/#class-variables_4","text":"Config default_key_components key_components matching_type","title":"Class variables"},{"location":"reference/arti/partitions/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/arti/partitions/#construct_4","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/partitions/#from_key_components_4","text":"def from_key_components ( ** key_components : 'str' ) -> 'PartitionKey' View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ] )) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ] , base = 16 )) return super (). from_key_components ( ** key_components )","title":"from_key_components"},{"location":"reference/arti/partitions/#from_orm_4","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/partitions/#get_class_for_4","text":"def get_class_for ( type_ : 'Type' ) -> 'type[PartitionKey]' View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ]","title":"get_class_for"},{"location":"reference/arti/partitions/#parse_file_4","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/partitions/#parse_obj_4","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/partitions/#parse_raw_4","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/partitions/#schema_4","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/partitions/#schema_json_4","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/partitions/#types_from_4","text":"def types_from ( type_ : 'Type' ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"types_from"},{"location":"reference/arti/partitions/#update_forward_refs_4","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/partitions/#validate_4","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/partitions/#instance-variables_4","text":"fingerprint hex","title":"Instance variables"},{"location":"reference/arti/partitions/#methods_4","text":"","title":"Methods"},{"location":"reference/arti/partitions/#copy_4","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/partitions/#dict_4","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/partitions/#json_4","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/partitions/#nullkey","text":"class NullKey ( __pydantic_self__ , ** data : Any ) View Source class NullKey ( PartitionKey ) : default_key_components : ClassVar [ frozendict[str, str ] ] = frozendict ( key = \"key\" ) matching_type = Null key : None = None @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : if set ( key_components ) == { \"key\" }: if key_components [ \"key\" ] != \"None\" : raise ValueError ( f \"'{cls.__name__}' can only be used with 'None'!\" ) return cls () return super (). from_key_components ( ** key_components )","title":"NullKey"},{"location":"reference/arti/partitions/#ancestors-in-mro_5","text":"arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/partitions/#class-variables_5","text":"Config default_key_components key_components matching_type","title":"Class variables"},{"location":"reference/arti/partitions/#static-methods_5","text":"","title":"Static methods"},{"location":"reference/arti/partitions/#construct_5","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/partitions/#from_key_components_5","text":"def from_key_components ( ** key_components : 'str' ) -> 'PartitionKey' View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : if set ( key_components ) == { \"key\" }: if key_components [ \"key\" ] != \"None\" : raise ValueError ( f \"'{cls.__name__}' can only be used with 'None'!\" ) return cls () return super (). from_key_components ( ** key_components )","title":"from_key_components"},{"location":"reference/arti/partitions/#from_orm_5","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/partitions/#get_class_for_5","text":"def get_class_for ( type_ : 'Type' ) -> 'type[PartitionKey]' View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ]","title":"get_class_for"},{"location":"reference/arti/partitions/#parse_file_5","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/partitions/#parse_obj_5","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/partitions/#parse_raw_5","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/partitions/#schema_5","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/partitions/#schema_json_5","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/partitions/#types_from_5","text":"def types_from ( type_ : 'Type' ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"types_from"},{"location":"reference/arti/partitions/#update_forward_refs_5","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/partitions/#validate_5","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/partitions/#instance-variables_5","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/partitions/#methods_5","text":"","title":"Methods"},{"location":"reference/arti/partitions/#copy_5","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/partitions/#dict_5","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/partitions/#json_5","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/partitions/#partitionkey","text":"class PartitionKey ( __pydantic_self__ , ** data : Any ) View Source class PartitionKey ( Model ) : _abstract_ = True _by_type_ : ClassVar [ dict[type[Type ] , type [ PartitionKey ] ]] = {} default_key_components : ClassVar [ frozendict[str, str ] ] matching_type : ClassVar [ type[Type ] ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return for attr in ( \"default_key_components\" , \"matching_type\" ) : if not hasattr ( cls , attr ) : raise TypeError ( f \"{cls.__name__} must set `{attr}`\" ) if unknown : = set ( cls . default_key_components ) - cls . key_components : raise TypeError ( f \"Unknown key_components in {cls.__name__}.default_key_components: {unknown}\" ) register ( cls . _by_type_ , cls . matching_type , cls ) @classproperty def key_components ( cls ) -> frozenset [ str ] : return frozenset ( cls . __fields__ ) | frozenset ( name for name in dir ( cls ) if isinstance ( getattr_static ( cls , name ), key_component ) ) @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : raise NotImplementedError ( f \"Unable to parse '{cls.__name__}' from: {key_components}\" ) @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ] @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"PartitionKey"},{"location":"reference/arti/partitions/#ancestors-in-mro_6","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/partitions/#descendants","text":"arti.partitions.DateKey arti.partitions._IntKey arti.partitions.NullKey","title":"Descendants"},{"location":"reference/arti/partitions/#class-variables_6","text":"Config key_components","title":"Class variables"},{"location":"reference/arti/partitions/#static-methods_6","text":"","title":"Static methods"},{"location":"reference/arti/partitions/#construct_6","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/partitions/#from_key_components_6","text":"def from_key_components ( ** key_components : 'str' ) -> 'PartitionKey' View Source @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : raise NotImplementedError ( f \"Unable to parse '{cls.__name__}' from: {key_components}\" )","title":"from_key_components"},{"location":"reference/arti/partitions/#from_orm_6","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/partitions/#get_class_for_6","text":"def get_class_for ( type_ : 'Type' ) -> 'type[PartitionKey]' View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ PartitionKey ] : return cls . _by_type_ [ type(type_) ]","title":"get_class_for"},{"location":"reference/arti/partitions/#parse_file_6","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/partitions/#parse_obj_6","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/partitions/#parse_raw_6","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/partitions/#schema_6","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/partitions/#schema_json_6","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/partitions/#types_from_6","text":"def types_from ( type_ : 'Type' ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> CompositeKeyTypes : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"types_from"},{"location":"reference/arti/partitions/#update_forward_refs_6","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/partitions/#validate_6","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/partitions/#instance-variables_6","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/partitions/#methods_6","text":"","title":"Methods"},{"location":"reference/arti/partitions/#copy_6","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/partitions/#dict_6","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/partitions/#json_6","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/partitions/#key_component","text":"class key_component ( / , * args , ** kwargs ) View Source class key_component ( property ): pass","title":"key_component"},{"location":"reference/arti/partitions/#ancestors-in-mro_7","text":"builtins.property","title":"Ancestors (in MRO)"},{"location":"reference/arti/partitions/#class-variables_7","text":"fdel fget fset","title":"Class variables"},{"location":"reference/arti/partitions/#methods_7","text":"","title":"Methods"},{"location":"reference/arti/partitions/#deleter","text":"def deleter ( ... ) Descriptor to obtain a copy of the property with a different deleter.","title":"deleter"},{"location":"reference/arti/partitions/#getter","text":"def getter ( ... ) Descriptor to obtain a copy of the property with a different getter.","title":"getter"},{"location":"reference/arti/partitions/#setter","text":"def setter ( ... ) Descriptor to obtain a copy of the property with a different setter.","title":"setter"},{"location":"reference/arti/producers/","text":"Module arti.producers None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from collections.abc import Callable , Iterator from inspect import Parameter , Signature , getattr_static from typing import ( TYPE_CHECKING , Annotated , Any , ClassVar , Optional , TypeVar , Union , cast , get_args , get_origin , ) from pydantic import validator from pydantic.fields import ModelField from arti.annotations import Annotation from arti.artifacts import Artifact from arti.fingerprints import Fingerprint from arti.internal import wrap_exc from arti.internal.models import Model from arti.internal.type_hints import ( NoneType , get_item_from_annotated , lenient_issubclass , signature , ) from arti.internal.utils import frozendict , get_module_name , ordinal from arti.partitions import CompositeKey , InputFingerprints , NotPartitioned , PartitionKey from arti.storage import StoragePartitions from arti.types import is_partitioned from arti.versions import SemVer , Version from arti.views import View _T = TypeVar ( \"_T\" ) MapInputs = set [ str ] BuildInputs = frozendict [ str , View ] Outputs = tuple [ View , ... ] InputPartitions = frozendict [ str , StoragePartitions ] PartitionDependencies = frozendict [ CompositeKey , InputPartitions ] MapSig = Callable [ ... , PartitionDependencies ] BuildSig = Callable [ ... , Any ] ValidateSig = Callable [ ... , tuple [ bool , str ]] class Producer ( Model ): \"\"\"A Producer is a task that builds one or more Artifacts.\"\"\" # User fields/methods annotations : tuple [ Annotation , ... ] = () version : Version = SemVer ( major = 0 , minor = 0 , patch = 1 ) # The map/build/validate_outputs parameters are intended to be dynamic and set by subclasses, # however mypy doesn't like the \"incompatible\" signature on subclasses if actually defined here # (nor support ParamSpec yet). `map` is generated during subclassing if not set, `build` is # required, and `validate_outputs` defaults to no-op checks (hence is the only one with a # provided method). # # These must be @classmethods or @staticmethods. map : ClassVar [ MapSig ] build : ClassVar [ BuildSig ] if TYPE_CHECKING : validate_outputs : ClassVar [ ValidateSig ] else : @staticmethod def validate_outputs ( * outputs : Any ) -> Union [ bool , tuple [ bool , str ]]: \"\"\"Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\"\" return True , \"No validation performed.\" # Internal fields/methods _abstract_ : ClassVar [ bool ] = True _fingerprint_excludes_ = frozenset ([ \"annotations\" ]) # NOTE: The following are set in __init_subclass__ _input_artifact_classes_ : ClassVar [ frozendict [ str , type [ Artifact ]]] _build_inputs_ : ClassVar [ BuildInputs ] _build_sig_ : ClassVar [ Signature ] _map_inputs_ : ClassVar [ MapInputs ] _map_sig_ : ClassVar [ Signature ] _outputs_ : ClassVar [ Outputs ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if not cls . _abstract_ : with wrap_exc ( ValueError , prefix = cls . __name__ ): cls . _input_artifact_classes_ = cls . _validate_fields () with wrap_exc ( ValueError , prefix = \".build\" ): ( cls . _build_sig_ , cls . _build_inputs_ , cls . _outputs_ , ) = cls . _validate_build_sig () with wrap_exc ( ValueError , prefix = \".validate_output\" ): cls . _validate_validate_output_sig () with wrap_exc ( ValueError , prefix = \".map\" ): cls . _map_sig_ , cls . _map_inputs_ = cls . _validate_map_sig () cls . _validate_no_unused_fields () @classmethod def _validate_fields ( cls ) -> frozendict [ str , type [ Artifact ]]: # NOTE: Aside from the base producer fields, all others should (currently) be Artifacts. # # Users can set additional class attributes, but they must be properly hinted as ClassVars. # These won't interact with the \"framework\" and can't be parameters to build/map. artifact_fields = { k : v for k , v in cls . __fields__ . items () if k not in Producer . __fields__ } for name , field in artifact_fields . items (): with wrap_exc ( ValueError , prefix = f \". { name } \" ): if not ( field . default is None and field . default_factory is None and field . required ): raise ValueError ( \"field must not have a default nor be Optional.\" ) if not lenient_issubclass ( field . outer_type_ , Artifact ): raise ValueError ( f \"type hint must be an Artifact subclass, got: { field . outer_type_ } \" ) return frozendict ({ name : field . outer_type_ for name , field in artifact_fields . items ()}) @classmethod def _validate_parameters ( cls , sig : Signature , * , validator : Callable [[ str , Parameter ], _T ] ) -> Iterator [ _T ]: if undefined_params := set ( sig . parameters ) - set ( cls . _input_artifact_classes_ ): raise ValueError ( f \"the following parameter(s) must be defined as a field: { undefined_params } \" ) for name , param in sig . parameters . items (): with wrap_exc ( ValueError , prefix = f \" { name } param\" ): if param . annotation is param . empty : raise ValueError ( \"must have a type hint.\" ) if param . default is not param . empty : raise ValueError ( \"must not have a default.\" ) if param . kind not in ( param . POSITIONAL_OR_KEYWORD , param . KEYWORD_ONLY ): raise ValueError ( \"must be usable as a keyword argument.\" ) yield validator ( name , param ) @classmethod def _validate_build_param ( cls , name : str , param : Parameter ) -> tuple [ str , View ]: annotation = param . annotation field_artifact_class = cls . _input_artifact_classes_ [ param . name ] # If there is no Artifact hint, add in the field value as the default. if get_item_from_annotated ( annotation , Artifact , is_subclass = True ) is None : annotation = Annotated [ annotation , field_artifact_class ] view = View . from_annotation ( annotation , mode = \"READ\" ) if view . artifact_class != field_artifact_class : raise ValueError ( f \"annotation Artifact class ( { view . artifact_class } ) does not match that set on the field ( { field_artifact_class } ).\" ) return name , view @classmethod def _validate_build_sig_return ( cls , annotation : Any , * , i : int ) -> View : with wrap_exc ( ValueError , prefix = f \" { ordinal ( i + 1 ) } return\" ): return View . from_annotation ( annotation , mode = \"WRITE\" ) @classmethod def _validate_build_sig ( cls ) -> tuple [ Signature , BuildInputs , Outputs ]: \"\"\"Validate the .build method\"\"\" if not hasattr ( cls , \"build\" ): raise ValueError ( \"must be implemented\" ) if not isinstance ( getattr_static ( cls , \"build\" ), ( classmethod , staticmethod )): raise ValueError ( \"must be a @classmethod or @staticmethod\" ) build_sig = signature ( cls . build , force_tuple_return = True , remove_owner = True ) # Validate the parameters build_inputs = BuildInputs ( cls . _validate_parameters ( build_sig , validator = cls . _validate_build_param ) ) # Validate the return definition return_annotation = build_sig . return_annotation if return_annotation is build_sig . empty : # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( \"a return value must be set with the output Artifact(s).\" ) if return_annotation == ( NoneType ,): raise ValueError ( \"missing return signature\" ) outputs = Outputs ( cls . _validate_build_sig_return ( annotation , i = i ) for i , annotation in enumerate ( return_annotation ) ) # Validate all outputs have equivalent partitioning schemes. # # We currently require the partition key type *and* name to match, but in the future we # might be able to extend the dependency metadata to support heterogeneous names if # necessary. seen_key_types = { PartitionKey . types_from ( view . type ) for view in outputs } if len ( seen_key_types ) != 1 : raise ValueError ( \"all outputs must have the same partitioning scheme\" ) return build_sig , build_inputs , outputs @classmethod def _validate_validate_output_sig ( cls ) -> None : build_output_hints = [ get_args ( hint )[ 0 ] if get_origin ( hint ) is Annotated else hint for hint in cls . _build_sig_ . return_annotation ] match_build_str = f \"match the `.build` return (` { build_output_hints } `)\" validate_parameters = signature ( cls . validate_outputs ) . parameters def param_matches ( param : Parameter , build_return : type ) -> bool : # Skip checking non-hinted parameters to allow lambdas. # # NOTE: Parameter type hints are *contravariant* (you can't pass a \"Manager\" into a # function expecting an \"Employee\"), hence the lenient_issubclass has build_return as # the subtype and param.annotation as the supertype. return param . annotation is param . empty or lenient_issubclass ( build_return , param . annotation ) if ( # Allow `*args: Any` or `*args: T` for `build(...) -> tuple[T, ...]` len ( validate_parameters ) == 1 and ( param := tuple ( validate_parameters . values ())[ 0 ]) . kind == param . VAR_POSITIONAL ): if not all ( param_matches ( param , output_hint ) for output_hint in build_output_hints ): with wrap_exc ( ValueError , prefix = f \" { param . name } param\" ): raise ValueError ( f \"type hint must be `Any` or { match_build_str } \" ) else : # Otherwise, check pairwise if len ( validate_parameters ) != len ( build_output_hints ): raise ValueError ( f \"must { match_build_str } \" ) for i , ( name , param ) in enumerate ( validate_parameters . items ()): with wrap_exc ( ValueError , prefix = f \" { name } param\" ): if param . default is not param . empty : raise ValueError ( \"must not have a default.\" ) if param . kind not in ( param . POSITIONAL_ONLY , param . POSITIONAL_OR_KEYWORD ): raise ValueError ( \"must be usable as a positional argument.\" ) if not param_matches ( param , ( expected := build_output_hints [ i ])): raise ValueError ( f \"type hint must match the { ordinal ( i + 1 ) } `.build` return (` { expected } `)\" ) # TODO: Validate return signature? @classmethod def _validate_map_param ( cls , name : str , param : Parameter ) -> str : # TODO: Should we add some ArtifactPartition[MyArtifact] type? if param . annotation != StoragePartitions : raise ValueError ( \"type hint must be `StoragePartitions`\" ) return name @classmethod def _validate_map_sig ( cls ) -> tuple [ Signature , MapInputs ]: \"\"\"Validate partitioned Artifacts and the .map method\"\"\" if not hasattr ( cls , \"map\" ): # TODO: Add runtime checking of `map` output (ie: output aligns w/ output # artifacts and such). if any ( is_partitioned ( view . type ) for view in cls . _outputs_ ): raise ValueError ( \"must be implemented when the `build` outputs are partitioned\" ) def map ( ** kwargs : StoragePartitions ) -> PartitionDependencies : return PartitionDependencies ({ NotPartitioned : frozendict ( kwargs )}) # Narrow the map signature, which is validated below and used at graph build time (via # cls._map_inputs_) to determine what arguments to pass to map. map . __signature__ = Signature ( # type: ignore[attr-defined] [ Parameter ( name = name , annotation = StoragePartitions , kind = Parameter . KEYWORD_ONLY ) for name , artifact in cls . _input_artifact_classes_ . items () if name in cls . _build_inputs_ ], return_annotation = PartitionDependencies , ) cls . map = cast ( MapSig , staticmethod ( map )) if not isinstance ( getattr_static ( cls , \"map\" ), ( classmethod , staticmethod )): raise ValueError ( \"must be a @classmethod or @staticmethod\" ) map_sig = signature ( cls . map ) map_inputs = MapInputs ( cls . _validate_parameters ( map_sig , validator = cls . _validate_map_param )) return map_sig , map_inputs # TODO: Verify map output hint matches TBD spec @classmethod def _validate_no_unused_fields ( cls ) -> None : if unused_fields := set ( cls . _input_artifact_classes_ ) - ( set ( cls . _build_sig_ . parameters ) | set ( cls . _map_sig_ . parameters ) ): raise ValueError ( f \"the following fields aren't used in `.build` or `.map`: { unused_fields } \" ) @validator ( \"*\" ) @classmethod def _validate_instance_artifact_args ( cls , value : Artifact , field : ModelField ) -> Artifact : if ( view := cls . _build_inputs_ . get ( field . name )) is not None : view . check_artifact_compatibility ( value ) return value # NOTE: pydantic defines .__iter__ to return `self.__dict__.items()` to support `dict(model)`, # but we want to override to support easy expansion/assignment to a Graph without `.out()` (eg: # `g.artifacts.a, g.artifacts.b = MyProducer(...)`). def __iter__ ( self ) -> Iterator [ Artifact ]: # type: ignore[override] ret = self . out () if not isinstance ( ret , tuple ): ret = ( ret ,) return iter ( ret ) def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str , StoragePartitions ] ) -> Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_inputs_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected { expected_names } , got { input_names } \" ) # We only care if the *code* or *input partition contents* changed, not if the input file # paths changed (but have the same content as a prior run). return Fingerprint . from_string ( self . _class_key_ ) . combine ( self . version . fingerprint , * ( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), ) def compute_dependencies ( self , input_partitions : InputPartitions ) -> tuple [ PartitionDependencies , InputFingerprints ]: # TODO: Validate the partition_dependencies against the Producer's partitioning scheme and # such (basically, check user error). eg: if output is not partitioned, we expect only 1 # entry in partition_dependencies (NotPartitioned). partition_dependencies = self . map ( ** { name : partitions for name , partitions in input_partitions . items () if name in self . _map_inputs_ } ) partition_input_fingerprints = InputFingerprints ( { composite_key : self . compute_input_fingerprint ( dependency_partitions ) for composite_key , dependency_partitions in partition_dependencies . items () } ) return partition_dependencies , partition_input_fingerprints @property def inputs ( self ) -> dict [ str , Artifact ]: return { k : getattr ( self , k ) for k in self . _input_artifact_classes_ } def out ( self , * outputs : Artifact ) -> Union [ Artifact , tuple [ Artifact , ... ]]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : outputs = tuple ( view . artifact_class ( type = view . type ) for view in self . _outputs_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = \", \" . join ([ str ( v ) for v in self . _build_sig_ . return_annotation ]) raise ValueError ( f \" { self . _class_key_ } .out() - expected { expected_n } arguments of ( { ret_str } ), but got: { outputs } \" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : view = self . _outputs_ [ ord ] with wrap_exc ( ValueError , prefix = f \" { self . _class_key_ } .out() { ordinal ( ord + 1 ) } argument\" ): view . check_artifact_compatibility ( artifact ) if artifact . producer_output is not None : raise ValueError ( f \" { artifact } is produced by { artifact . producer_output . producer } !\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord )} ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs def producer ( * , annotations : Optional [ tuple [ Annotation , ... ]] = None , map : Optional [ MapSig ] = None , name : Optional [ str ] = None , validate_outputs : Optional [ ValidateSig ] = None , version : Optional [ Version ] = None , ) -> Callable [[ BuildSig ], type [ Producer ]]: def decorate ( build : BuildSig ) -> type [ Producer ]: nonlocal name name = build . __name__ if name is None else name __annotations__ : dict [ str , Any ] = {} for param in signature ( build ) . parameters . values (): with wrap_exc ( ValueError , prefix = f \" { name } { param . name } param\" ): view = View . from_annotation ( param . annotation , mode = \"READ\" ) __annotations__ [ param . name ] = view . artifact_class # If overriding, set an explicit \"annotations\" hint until [1] is released. # # 1: https://github.com/samuelcolvin/pydantic/pull/3018 if annotations : __annotations__ [ \"annotations\" ] = tuple [ Annotation , ... ] if version : __annotations__ [ \"version\" ] = Version return type ( name , ( Producer ,), { k : v for k , v in { \"__annotations__\" : __annotations__ , \"__module__\" : get_module_name ( depth = 2 ), # Not our module, but our caller's. \"annotations\" : annotations , \"build\" : staticmethod ( build ), \"map\" : None if map is None else staticmethod ( map ), \"validate_outputs\" : ( None if validate_outputs is None else staticmethod ( validate_outputs ) ), \"version\" : version , } . items () if v is not None }, ) return decorate class ProducerOutput ( Model ): producer : Producer position : int # TODO: Support named output (defaulting to artifact classname?) Artifact . update_forward_refs ( ProducerOutput = ProducerOutput ) Variables BuildInputs BuildSig InputPartitions MapInputs MapSig Outputs PartitionDependencies TYPE_CHECKING ValidateSig Functions producer def producer ( * , annotations : 'Optional[tuple[Annotation, ...]]' = None , map : 'Optional[MapSig]' = None , name : 'Optional[str]' = None , validate_outputs : 'Optional[ValidateSig]' = None , version : 'Optional[Version]' = None ) -> 'Callable[[BuildSig], type[Producer]]' View Source def producer ( * , annotations : Optional [ tuple[Annotation, ... ] ] = None , map : Optional [ MapSig ] = None , name : Optional [ str ] = None , validate_outputs : Optional [ ValidateSig ] = None , version : Optional [ Version ] = None , ) -> Callable [ [BuildSig ] , type [ Producer ] ]: def decorate ( build : BuildSig ) -> type [ Producer ] : nonlocal name name = build . __name__ if name is None else name __annotations__ : dict [ str, Any ] = {} for param in signature ( build ). parameters . values () : with wrap_exc ( ValueError , prefix = f \"{name} {param.name} param\" ) : view = View . from_annotation ( param . annotation , mode = \"READ\" ) __annotations__ [ param.name ] = view . artifact_class # If overriding , set an explicit \"annotations\" hint until [ 1 ] is released . # # 1 : https : // github . com / samuelcolvin / pydantic / pull / 3018 if annotations : __annotations__ [ \"annotations\" ] = tuple [ Annotation, ... ] if version : __annotations__ [ \"version\" ] = Version return type ( name , ( Producer ,), { k : v for k , v in { \"__annotations__\" : __annotations__ , \"__module__\" : get_module_name ( depth = 2 ), # Not our module , but our caller ' s . \"annotations\" : annotations , \"build\" : staticmethod ( build ), \"map\" : None if map is None else staticmethod ( map ), \"validate_outputs\" : ( None if validate_outputs is None else staticmethod ( validate_outputs ) ), \"version\" : version , } . items () if v is not None } , ) return decorate Classes Producer class Producer ( __pydantic_self__ , ** data : Any ) View Source class Producer ( Model ) : \"\"\"A Producer is a task that builds one or more Artifacts.\"\"\" # User fields / methods annotations : tuple [ Annotation, ... ] = () version : Version = SemVer ( major = 0 , minor = 0 , patch = 1 ) # The map / build / validate_outputs parameters are intended to be dynamic and set by subclasses , # however mypy doesn 't like the \"incompatible\" signature on subclasses if actually defined here # (nor support ParamSpec yet). `map` is generated during subclassing if not set, `build` is # required, and `validate_outputs` defaults to no-op checks (hence is the only one with a # provided method). # # These must be @classmethods or @staticmethods. map: ClassVar[MapSig] build: ClassVar[BuildSig] if TYPE_CHECKING: validate_outputs: ClassVar[ValidateSig] else: @staticmethod def validate_outputs(*outputs: Any) -> Union[bool, tuple[bool, str]]: \"\"\"Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\"\" return True, \"No validation performed.\" # Internal fields/methods _abstract_: ClassVar[bool] = True _fingerprint_excludes_ = frozenset([\"annotations\"]) # NOTE: The following are set in __init_subclass__ _input_artifact_classes_: ClassVar[frozendict[str, type[Artifact]]] _build_inputs_: ClassVar[BuildInputs] _build_sig_: ClassVar[Signature] _map_inputs_: ClassVar[MapInputs] _map_sig_: ClassVar[Signature] _outputs_: ClassVar[Outputs] @classmethod def __init_subclass__(cls, **kwargs: Any) -> None: super().__init_subclass__(**kwargs) if not cls._abstract_: with wrap_exc(ValueError, prefix=cls.__name__): cls._input_artifact_classes_ = cls._validate_fields() with wrap_exc(ValueError, prefix=\".build\"): ( cls._build_sig_, cls._build_inputs_, cls._outputs_, ) = cls._validate_build_sig() with wrap_exc(ValueError, prefix=\".validate_output\"): cls._validate_validate_output_sig() with wrap_exc(ValueError, prefix=\".map\"): cls._map_sig_, cls._map_inputs_ = cls._validate_map_sig() cls._validate_no_unused_fields() @classmethod def _validate_fields(cls) -> frozendict[str, type[Artifact]]: # NOTE: Aside from the base producer fields, all others should (currently) be Artifacts. # # Users can set additional class attributes, but they must be properly hinted as ClassVars. # These won' t interact with the \"framework\" and can 't be parameters to build/map. artifact_fields = {k: v for k, v in cls.__fields__.items() if k not in Producer.__fields__} for name, field in artifact_fields.items(): with wrap_exc(ValueError, prefix=f\".{name}\"): if not (field.default is None and field.default_factory is None and field.required): raise ValueError(\"field must not have a default nor be Optional.\") if not lenient_issubclass(field.outer_type_, Artifact): raise ValueError( f\"type hint must be an Artifact subclass, got: {field.outer_type_}\" ) return frozendict({name: field.outer_type_ for name, field in artifact_fields.items()}) @classmethod def _validate_parameters( cls, sig: Signature, *, validator: Callable[[str, Parameter], _T] ) -> Iterator[_T]: if undefined_params := set(sig.parameters) - set(cls._input_artifact_classes_): raise ValueError( f\"the following parameter(s) must be defined as a field: {undefined_params}\" ) for name, param in sig.parameters.items(): with wrap_exc(ValueError, prefix=f\" {name} param\"): if param.annotation is param.empty: raise ValueError(\"must have a type hint.\") if param.default is not param.empty: raise ValueError(\"must not have a default.\") if param.kind not in (param.POSITIONAL_OR_KEYWORD, param.KEYWORD_ONLY): raise ValueError(\"must be usable as a keyword argument.\") yield validator(name, param) @classmethod def _validate_build_param(cls, name: str, param: Parameter) -> tuple[str, View]: annotation = param.annotation field_artifact_class = cls._input_artifact_classes_[param.name] # If there is no Artifact hint, add in the field value as the default. if get_item_from_annotated(annotation, Artifact, is_subclass=True) is None: annotation = Annotated[annotation, field_artifact_class] view = View.from_annotation(annotation, mode=\"READ\") if view.artifact_class != field_artifact_class: raise ValueError( f\"annotation Artifact class ({view.artifact_class}) does not match that set on the field ({field_artifact_class}).\" ) return name, view @classmethod def _validate_build_sig_return(cls, annotation: Any, *, i: int) -> View: with wrap_exc(ValueError, prefix=f\" {ordinal(i+1)} return\"): return View.from_annotation(annotation, mode=\"WRITE\") @classmethod def _validate_build_sig(cls) -> tuple[Signature, BuildInputs, Outputs]: \"\"\"Validate the .build method\"\"\" if not hasattr(cls, \"build\"): raise ValueError(\"must be implemented\") if not isinstance(getattr_static(cls, \"build\"), (classmethod, staticmethod)): raise ValueError(\"must be a @classmethod or @staticmethod\") build_sig = signature(cls.build, force_tuple_return=True, remove_owner=True) # Validate the parameters build_inputs = BuildInputs( cls._validate_parameters(build_sig, validator=cls._validate_build_param) ) # Validate the return definition return_annotation = build_sig.return_annotation if return_annotation is build_sig.empty: # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError(\"a return value must be set with the output Artifact(s).\") if return_annotation == (NoneType,): raise ValueError(\"missing return signature\") outputs = Outputs( cls._validate_build_sig_return(annotation, i=i) for i, annotation in enumerate(return_annotation) ) # Validate all outputs have equivalent partitioning schemes. # # We currently require the partition key type *and* name to match, but in the future we # might be able to extend the dependency metadata to support heterogeneous names if # necessary. seen_key_types = {PartitionKey.types_from(view.type) for view in outputs} if len(seen_key_types) != 1: raise ValueError(\"all outputs must have the same partitioning scheme\") return build_sig, build_inputs, outputs @classmethod def _validate_validate_output_sig(cls) -> None: build_output_hints = [ get_args(hint)[0] if get_origin(hint) is Annotated else hint for hint in cls._build_sig_.return_annotation ] match_build_str = f\"match the `.build` return (`{build_output_hints}`)\" validate_parameters = signature(cls.validate_outputs).parameters def param_matches(param: Parameter, build_return: type) -> bool: # Skip checking non-hinted parameters to allow lambdas. # # NOTE: Parameter type hints are *contravariant* (you can' t pass a \"Manager\" into a # function expecting an \"Employee\" ), hence the lenient_issubclass has build_return as # the subtype and param . annotation as the supertype . return param . annotation is param . empty or lenient_issubclass ( build_return , param . annotation ) if ( # Allow ` * args : Any ` or ` * args : T ` for ` build (...) -> tuple [ T, ... ] ` len ( validate_parameters ) == 1 and ( param : = tuple ( validate_parameters . values ()) [ 0 ] ). kind == param . VAR_POSITIONAL ) : if not all ( param_matches ( param , output_hint ) for output_hint in build_output_hints ) : with wrap_exc ( ValueError , prefix = f \" {param.name} param\" ) : raise ValueError ( f \"type hint must be `Any` or {match_build_str}\" ) else : # Otherwise , check pairwise if len ( validate_parameters ) != len ( build_output_hints ) : raise ValueError ( f \"must {match_build_str}\" ) for i , ( name , param ) in enumerate ( validate_parameters . items ()) : with wrap_exc ( ValueError , prefix = f \" {name} param\" ) : if param . default is not param . empty : raise ValueError ( \"must not have a default.\" ) if param . kind not in ( param . POSITIONAL_ONLY , param . POSITIONAL_OR_KEYWORD ) : raise ValueError ( \"must be usable as a positional argument.\" ) if not param_matches ( param , ( expected : = build_output_hints [ i ] )) : raise ValueError ( f \"type hint must match the {ordinal(i+1)} `.build` return (`{expected}`)\" ) # TODO : Validate return signature ? @classmethod def _validate_map_param ( cls , name : str , param : Parameter ) -> str : # TODO : Should we add some ArtifactPartition [ MyArtifact ] type ? if param . annotation != StoragePartitions : raise ValueError ( \"type hint must be `StoragePartitions`\" ) return name @classmethod def _validate_map_sig ( cls ) -> tuple [ Signature, MapInputs ] : \"\"\"Validate partitioned Artifacts and the .map method\"\"\" if not hasattr ( cls , \"map\" ) : # TODO : Add runtime checking of ` map ` output ( ie : output aligns w / output # artifacts and such ). if any ( is_partitioned ( view . type ) for view in cls . _outputs_ ) : raise ValueError ( \"must be implemented when the `build` outputs are partitioned\" ) def map ( ** kwargs : StoragePartitions ) -> PartitionDependencies : return PartitionDependencies ( { NotPartitioned : frozendict ( kwargs ) } ) # Narrow the map signature , which is validated below and used at graph build time ( via # cls . _map_inputs_ ) to determine what arguments to pass to map . map . __signature__ = Signature ( # type : ignore [ attr-defined ] [ Parameter(name=name, annotation=StoragePartitions, kind=Parameter.KEYWORD_ONLY) for name, artifact in cls._input_artifact_classes_.items() if name in cls._build_inputs_ ] , return_annotation = PartitionDependencies , ) cls . map = cast ( MapSig , staticmethod ( map )) if not isinstance ( getattr_static ( cls , \"map\" ), ( classmethod , staticmethod )) : raise ValueError ( \"must be a @classmethod or @staticmethod\" ) map_sig = signature ( cls . map ) map_inputs = MapInputs ( cls . _validate_parameters ( map_sig , validator = cls . _validate_map_param )) return map_sig , map_inputs # TODO : Verify map output hint matches TBD spec @classmethod def _validate_no_unused_fields ( cls ) -> None : if unused_fields : = set ( cls . _input_artifact_classes_ ) - ( set ( cls . _build_sig_ . parameters ) | set ( cls . _map_sig_ . parameters ) ) : raise ValueError ( f \"the following fields aren't used in `.build` or `.map`: {unused_fields}\" ) @validator ( \"*\" ) @classmethod def _validate_instance_artifact_args ( cls , value : Artifact , field : ModelField ) -> Artifact : if ( view : = cls . _build_inputs_ . get ( field . name )) is not None : view . check_artifact_compatibility ( value ) return value # NOTE : pydantic defines . __iter__ to return ` self . __dict__ . items () ` to support ` dict ( model ) ` , # but we want to override to support easy expansion / assignment to a Graph without ` . out () ` ( eg : # ` g . artifacts . a , g . artifacts . b = MyProducer (...) ` ). def __iter__ ( self ) -> Iterator [ Artifact ] : # type : ignore [ override ] ret = self . out () if not isinstance ( ret , tuple ) : ret = ( ret ,) return iter ( ret ) def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str, StoragePartitions ] ) -> Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_inputs_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected {expected_names}, got {input_names}\" ) # We only care if the * code * or * input partition contents * changed , not if the input file # paths changed ( but have the same content as a prior run ). return Fingerprint . from_string ( self . _class_key_ ). combine ( self . version . fingerprint , * ( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), ) def compute_dependencies ( self , input_partitions : InputPartitions ) -> tuple [ PartitionDependencies, InputFingerprints ] : # TODO : Validate the partition_dependencies against the Producer ' s partitioning scheme and # such ( basically , check user error ). eg : if output is not partitioned , we expect only 1 # entry in partition_dependencies ( NotPartitioned ). partition_dependencies = self . map ( ** { name : partitions for name , partitions in input_partitions . items () if name in self . _map_inputs_ } ) partition_input_fingerprints = InputFingerprints ( { composite_key : self . compute_input_fingerprint ( dependency_partitions ) for composite_key , dependency_partitions in partition_dependencies . items () } ) return partition_dependencies , partition_input_fingerprints @property def inputs ( self ) -> dict [ str, Artifact ] : return { k : getattr ( self , k ) for k in self . _input_artifact_classes_ } def out ( self , * outputs : Artifact ) -> Union [ Artifact, tuple[Artifact, ... ] ]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : outputs = tuple ( view . artifact_class ( type = view . type ) for view in self . _outputs_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = \", \" . join ( [ str(v) for v in self._build_sig_.return_annotation ] ) raise ValueError ( f \"{self._class_key_}.out() - expected {expected_n} arguments of ({ret_str}), but got: {outputs}\" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : view = self . _outputs_ [ ord ] with wrap_exc ( ValueError , prefix = f \"{self._class_key_}.out() {ordinal(ord+1)} argument\" ) : view . check_artifact_compatibility ( artifact ) if artifact . producer_output is not None : raise ValueError ( f \"{artifact} is produced by {artifact.producer_output.producer}!\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord ) } ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_outputs def validate_outputs ( * outputs : 'Any' ) -> 'Union[bool, tuple[bool, str]]' Validate the Producer.build outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of build will be passed in as it was returned, for example: def build(...): return 1, 2 will result in validate_outputs(1, 2) . NOTE: validate_outputs is a stopgap until Statistics and Thresholds are fully implemented. View Source @staticmethod def validate_outputs ( * outputs : Any ) -> Union [ bool , tuple [ bool , str ]] : \" \"\" Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\" \" return True , \"No validation performed.\" Instance variables fingerprint inputs Methods compute_dependencies def compute_dependencies ( self , input_partitions : 'InputPartitions' ) -> 'tuple[PartitionDependencies, InputFingerprints]' View Source def compute_dependencies ( self , input_partitions : InputPartitions ) -> tuple [ PartitionDependencies , InputFingerprints ] : # TODO : Validate the partition_dependencies against the Producer 's partitioning scheme and # such (basically, check user error). eg: if output is not partitioned, we expect only 1 # entry in partition_dependencies (NotPartitioned). partition_dependencies = self.map( **{ name: partitions for name, partitions in input_partitions.items() if name in self._map_inputs_ } ) partition_input_fingerprints = InputFingerprints( { composite_key: self.compute_input_fingerprint(dependency_partitions) for composite_key, dependency_partitions in partition_dependencies.items() } ) return partition_dependencies, partition_input_fingerprints compute_input_fingerprint def compute_input_fingerprint ( self , dependency_partitions : 'frozendict[str, StoragePartitions]' ) -> 'Fingerprint' View Source def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str , StoragePartitions ] ) - > Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_inputs_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected {expected_names}, got {input_names}\" ) # We only care if the * code * or * input partition contents * changed , not if the input file # paths changed ( but have the same content as a prior run ). return Fingerprint . from_string ( self . _class_key_ ) . combine ( self . version . fingerprint , *( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . out def out ( self , * outputs : 'Artifact' ) -> 'Union[Artifact, tuple[Artifact, ...]]' Configure the output Artifacts this Producer will build. The arguments are matched to the Producer.build return signature in order. View Source def out ( self , * outputs : Artifact ) -> Union [ Artifact, tuple[Artifact, ... ] ]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : outputs = tuple ( view . artifact_class ( type = view . type ) for view in self . _outputs_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = \", \" . join ( [ str(v) for v in self._build_sig_.return_annotation ] ) raise ValueError ( f \"{self._class_key_}.out() - expected {expected_n} arguments of ({ret_str}), but got: {outputs}\" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : view = self . _outputs_ [ ord ] with wrap_exc ( ValueError , prefix = f \"{self._class_key_}.out() {ordinal(ord+1)} argument\" ) : view . check_artifact_compatibility ( artifact ) if artifact . producer_output is not None : raise ValueError ( f \"{artifact} is produced by {artifact.producer_output.producer}!\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord ) } ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs ProducerOutput class ProducerOutput ( __pydantic_self__ , ** data : Any ) View Source class ProducerOutput ( Model ): producer: Producer position: int # TODO: Support named output (defaulting to artifact classname?) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Producers"},{"location":"reference/arti/producers/#module-artiproducers","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from collections.abc import Callable , Iterator from inspect import Parameter , Signature , getattr_static from typing import ( TYPE_CHECKING , Annotated , Any , ClassVar , Optional , TypeVar , Union , cast , get_args , get_origin , ) from pydantic import validator from pydantic.fields import ModelField from arti.annotations import Annotation from arti.artifacts import Artifact from arti.fingerprints import Fingerprint from arti.internal import wrap_exc from arti.internal.models import Model from arti.internal.type_hints import ( NoneType , get_item_from_annotated , lenient_issubclass , signature , ) from arti.internal.utils import frozendict , get_module_name , ordinal from arti.partitions import CompositeKey , InputFingerprints , NotPartitioned , PartitionKey from arti.storage import StoragePartitions from arti.types import is_partitioned from arti.versions import SemVer , Version from arti.views import View _T = TypeVar ( \"_T\" ) MapInputs = set [ str ] BuildInputs = frozendict [ str , View ] Outputs = tuple [ View , ... ] InputPartitions = frozendict [ str , StoragePartitions ] PartitionDependencies = frozendict [ CompositeKey , InputPartitions ] MapSig = Callable [ ... , PartitionDependencies ] BuildSig = Callable [ ... , Any ] ValidateSig = Callable [ ... , tuple [ bool , str ]] class Producer ( Model ): \"\"\"A Producer is a task that builds one or more Artifacts.\"\"\" # User fields/methods annotations : tuple [ Annotation , ... ] = () version : Version = SemVer ( major = 0 , minor = 0 , patch = 1 ) # The map/build/validate_outputs parameters are intended to be dynamic and set by subclasses, # however mypy doesn't like the \"incompatible\" signature on subclasses if actually defined here # (nor support ParamSpec yet). `map` is generated during subclassing if not set, `build` is # required, and `validate_outputs` defaults to no-op checks (hence is the only one with a # provided method). # # These must be @classmethods or @staticmethods. map : ClassVar [ MapSig ] build : ClassVar [ BuildSig ] if TYPE_CHECKING : validate_outputs : ClassVar [ ValidateSig ] else : @staticmethod def validate_outputs ( * outputs : Any ) -> Union [ bool , tuple [ bool , str ]]: \"\"\"Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\"\" return True , \"No validation performed.\" # Internal fields/methods _abstract_ : ClassVar [ bool ] = True _fingerprint_excludes_ = frozenset ([ \"annotations\" ]) # NOTE: The following are set in __init_subclass__ _input_artifact_classes_ : ClassVar [ frozendict [ str , type [ Artifact ]]] _build_inputs_ : ClassVar [ BuildInputs ] _build_sig_ : ClassVar [ Signature ] _map_inputs_ : ClassVar [ MapInputs ] _map_sig_ : ClassVar [ Signature ] _outputs_ : ClassVar [ Outputs ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if not cls . _abstract_ : with wrap_exc ( ValueError , prefix = cls . __name__ ): cls . _input_artifact_classes_ = cls . _validate_fields () with wrap_exc ( ValueError , prefix = \".build\" ): ( cls . _build_sig_ , cls . _build_inputs_ , cls . _outputs_ , ) = cls . _validate_build_sig () with wrap_exc ( ValueError , prefix = \".validate_output\" ): cls . _validate_validate_output_sig () with wrap_exc ( ValueError , prefix = \".map\" ): cls . _map_sig_ , cls . _map_inputs_ = cls . _validate_map_sig () cls . _validate_no_unused_fields () @classmethod def _validate_fields ( cls ) -> frozendict [ str , type [ Artifact ]]: # NOTE: Aside from the base producer fields, all others should (currently) be Artifacts. # # Users can set additional class attributes, but they must be properly hinted as ClassVars. # These won't interact with the \"framework\" and can't be parameters to build/map. artifact_fields = { k : v for k , v in cls . __fields__ . items () if k not in Producer . __fields__ } for name , field in artifact_fields . items (): with wrap_exc ( ValueError , prefix = f \". { name } \" ): if not ( field . default is None and field . default_factory is None and field . required ): raise ValueError ( \"field must not have a default nor be Optional.\" ) if not lenient_issubclass ( field . outer_type_ , Artifact ): raise ValueError ( f \"type hint must be an Artifact subclass, got: { field . outer_type_ } \" ) return frozendict ({ name : field . outer_type_ for name , field in artifact_fields . items ()}) @classmethod def _validate_parameters ( cls , sig : Signature , * , validator : Callable [[ str , Parameter ], _T ] ) -> Iterator [ _T ]: if undefined_params := set ( sig . parameters ) - set ( cls . _input_artifact_classes_ ): raise ValueError ( f \"the following parameter(s) must be defined as a field: { undefined_params } \" ) for name , param in sig . parameters . items (): with wrap_exc ( ValueError , prefix = f \" { name } param\" ): if param . annotation is param . empty : raise ValueError ( \"must have a type hint.\" ) if param . default is not param . empty : raise ValueError ( \"must not have a default.\" ) if param . kind not in ( param . POSITIONAL_OR_KEYWORD , param . KEYWORD_ONLY ): raise ValueError ( \"must be usable as a keyword argument.\" ) yield validator ( name , param ) @classmethod def _validate_build_param ( cls , name : str , param : Parameter ) -> tuple [ str , View ]: annotation = param . annotation field_artifact_class = cls . _input_artifact_classes_ [ param . name ] # If there is no Artifact hint, add in the field value as the default. if get_item_from_annotated ( annotation , Artifact , is_subclass = True ) is None : annotation = Annotated [ annotation , field_artifact_class ] view = View . from_annotation ( annotation , mode = \"READ\" ) if view . artifact_class != field_artifact_class : raise ValueError ( f \"annotation Artifact class ( { view . artifact_class } ) does not match that set on the field ( { field_artifact_class } ).\" ) return name , view @classmethod def _validate_build_sig_return ( cls , annotation : Any , * , i : int ) -> View : with wrap_exc ( ValueError , prefix = f \" { ordinal ( i + 1 ) } return\" ): return View . from_annotation ( annotation , mode = \"WRITE\" ) @classmethod def _validate_build_sig ( cls ) -> tuple [ Signature , BuildInputs , Outputs ]: \"\"\"Validate the .build method\"\"\" if not hasattr ( cls , \"build\" ): raise ValueError ( \"must be implemented\" ) if not isinstance ( getattr_static ( cls , \"build\" ), ( classmethod , staticmethod )): raise ValueError ( \"must be a @classmethod or @staticmethod\" ) build_sig = signature ( cls . build , force_tuple_return = True , remove_owner = True ) # Validate the parameters build_inputs = BuildInputs ( cls . _validate_parameters ( build_sig , validator = cls . _validate_build_param ) ) # Validate the return definition return_annotation = build_sig . return_annotation if return_annotation is build_sig . empty : # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( \"a return value must be set with the output Artifact(s).\" ) if return_annotation == ( NoneType ,): raise ValueError ( \"missing return signature\" ) outputs = Outputs ( cls . _validate_build_sig_return ( annotation , i = i ) for i , annotation in enumerate ( return_annotation ) ) # Validate all outputs have equivalent partitioning schemes. # # We currently require the partition key type *and* name to match, but in the future we # might be able to extend the dependency metadata to support heterogeneous names if # necessary. seen_key_types = { PartitionKey . types_from ( view . type ) for view in outputs } if len ( seen_key_types ) != 1 : raise ValueError ( \"all outputs must have the same partitioning scheme\" ) return build_sig , build_inputs , outputs @classmethod def _validate_validate_output_sig ( cls ) -> None : build_output_hints = [ get_args ( hint )[ 0 ] if get_origin ( hint ) is Annotated else hint for hint in cls . _build_sig_ . return_annotation ] match_build_str = f \"match the `.build` return (` { build_output_hints } `)\" validate_parameters = signature ( cls . validate_outputs ) . parameters def param_matches ( param : Parameter , build_return : type ) -> bool : # Skip checking non-hinted parameters to allow lambdas. # # NOTE: Parameter type hints are *contravariant* (you can't pass a \"Manager\" into a # function expecting an \"Employee\"), hence the lenient_issubclass has build_return as # the subtype and param.annotation as the supertype. return param . annotation is param . empty or lenient_issubclass ( build_return , param . annotation ) if ( # Allow `*args: Any` or `*args: T` for `build(...) -> tuple[T, ...]` len ( validate_parameters ) == 1 and ( param := tuple ( validate_parameters . values ())[ 0 ]) . kind == param . VAR_POSITIONAL ): if not all ( param_matches ( param , output_hint ) for output_hint in build_output_hints ): with wrap_exc ( ValueError , prefix = f \" { param . name } param\" ): raise ValueError ( f \"type hint must be `Any` or { match_build_str } \" ) else : # Otherwise, check pairwise if len ( validate_parameters ) != len ( build_output_hints ): raise ValueError ( f \"must { match_build_str } \" ) for i , ( name , param ) in enumerate ( validate_parameters . items ()): with wrap_exc ( ValueError , prefix = f \" { name } param\" ): if param . default is not param . empty : raise ValueError ( \"must not have a default.\" ) if param . kind not in ( param . POSITIONAL_ONLY , param . POSITIONAL_OR_KEYWORD ): raise ValueError ( \"must be usable as a positional argument.\" ) if not param_matches ( param , ( expected := build_output_hints [ i ])): raise ValueError ( f \"type hint must match the { ordinal ( i + 1 ) } `.build` return (` { expected } `)\" ) # TODO: Validate return signature? @classmethod def _validate_map_param ( cls , name : str , param : Parameter ) -> str : # TODO: Should we add some ArtifactPartition[MyArtifact] type? if param . annotation != StoragePartitions : raise ValueError ( \"type hint must be `StoragePartitions`\" ) return name @classmethod def _validate_map_sig ( cls ) -> tuple [ Signature , MapInputs ]: \"\"\"Validate partitioned Artifacts and the .map method\"\"\" if not hasattr ( cls , \"map\" ): # TODO: Add runtime checking of `map` output (ie: output aligns w/ output # artifacts and such). if any ( is_partitioned ( view . type ) for view in cls . _outputs_ ): raise ValueError ( \"must be implemented when the `build` outputs are partitioned\" ) def map ( ** kwargs : StoragePartitions ) -> PartitionDependencies : return PartitionDependencies ({ NotPartitioned : frozendict ( kwargs )}) # Narrow the map signature, which is validated below and used at graph build time (via # cls._map_inputs_) to determine what arguments to pass to map. map . __signature__ = Signature ( # type: ignore[attr-defined] [ Parameter ( name = name , annotation = StoragePartitions , kind = Parameter . KEYWORD_ONLY ) for name , artifact in cls . _input_artifact_classes_ . items () if name in cls . _build_inputs_ ], return_annotation = PartitionDependencies , ) cls . map = cast ( MapSig , staticmethod ( map )) if not isinstance ( getattr_static ( cls , \"map\" ), ( classmethod , staticmethod )): raise ValueError ( \"must be a @classmethod or @staticmethod\" ) map_sig = signature ( cls . map ) map_inputs = MapInputs ( cls . _validate_parameters ( map_sig , validator = cls . _validate_map_param )) return map_sig , map_inputs # TODO: Verify map output hint matches TBD spec @classmethod def _validate_no_unused_fields ( cls ) -> None : if unused_fields := set ( cls . _input_artifact_classes_ ) - ( set ( cls . _build_sig_ . parameters ) | set ( cls . _map_sig_ . parameters ) ): raise ValueError ( f \"the following fields aren't used in `.build` or `.map`: { unused_fields } \" ) @validator ( \"*\" ) @classmethod def _validate_instance_artifact_args ( cls , value : Artifact , field : ModelField ) -> Artifact : if ( view := cls . _build_inputs_ . get ( field . name )) is not None : view . check_artifact_compatibility ( value ) return value # NOTE: pydantic defines .__iter__ to return `self.__dict__.items()` to support `dict(model)`, # but we want to override to support easy expansion/assignment to a Graph without `.out()` (eg: # `g.artifacts.a, g.artifacts.b = MyProducer(...)`). def __iter__ ( self ) -> Iterator [ Artifact ]: # type: ignore[override] ret = self . out () if not isinstance ( ret , tuple ): ret = ( ret ,) return iter ( ret ) def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str , StoragePartitions ] ) -> Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_inputs_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected { expected_names } , got { input_names } \" ) # We only care if the *code* or *input partition contents* changed, not if the input file # paths changed (but have the same content as a prior run). return Fingerprint . from_string ( self . _class_key_ ) . combine ( self . version . fingerprint , * ( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), ) def compute_dependencies ( self , input_partitions : InputPartitions ) -> tuple [ PartitionDependencies , InputFingerprints ]: # TODO: Validate the partition_dependencies against the Producer's partitioning scheme and # such (basically, check user error). eg: if output is not partitioned, we expect only 1 # entry in partition_dependencies (NotPartitioned). partition_dependencies = self . map ( ** { name : partitions for name , partitions in input_partitions . items () if name in self . _map_inputs_ } ) partition_input_fingerprints = InputFingerprints ( { composite_key : self . compute_input_fingerprint ( dependency_partitions ) for composite_key , dependency_partitions in partition_dependencies . items () } ) return partition_dependencies , partition_input_fingerprints @property def inputs ( self ) -> dict [ str , Artifact ]: return { k : getattr ( self , k ) for k in self . _input_artifact_classes_ } def out ( self , * outputs : Artifact ) -> Union [ Artifact , tuple [ Artifact , ... ]]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : outputs = tuple ( view . artifact_class ( type = view . type ) for view in self . _outputs_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = \", \" . join ([ str ( v ) for v in self . _build_sig_ . return_annotation ]) raise ValueError ( f \" { self . _class_key_ } .out() - expected { expected_n } arguments of ( { ret_str } ), but got: { outputs } \" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : view = self . _outputs_ [ ord ] with wrap_exc ( ValueError , prefix = f \" { self . _class_key_ } .out() { ordinal ( ord + 1 ) } argument\" ): view . check_artifact_compatibility ( artifact ) if artifact . producer_output is not None : raise ValueError ( f \" { artifact } is produced by { artifact . producer_output . producer } !\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord )} ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs def producer ( * , annotations : Optional [ tuple [ Annotation , ... ]] = None , map : Optional [ MapSig ] = None , name : Optional [ str ] = None , validate_outputs : Optional [ ValidateSig ] = None , version : Optional [ Version ] = None , ) -> Callable [[ BuildSig ], type [ Producer ]]: def decorate ( build : BuildSig ) -> type [ Producer ]: nonlocal name name = build . __name__ if name is None else name __annotations__ : dict [ str , Any ] = {} for param in signature ( build ) . parameters . values (): with wrap_exc ( ValueError , prefix = f \" { name } { param . name } param\" ): view = View . from_annotation ( param . annotation , mode = \"READ\" ) __annotations__ [ param . name ] = view . artifact_class # If overriding, set an explicit \"annotations\" hint until [1] is released. # # 1: https://github.com/samuelcolvin/pydantic/pull/3018 if annotations : __annotations__ [ \"annotations\" ] = tuple [ Annotation , ... ] if version : __annotations__ [ \"version\" ] = Version return type ( name , ( Producer ,), { k : v for k , v in { \"__annotations__\" : __annotations__ , \"__module__\" : get_module_name ( depth = 2 ), # Not our module, but our caller's. \"annotations\" : annotations , \"build\" : staticmethod ( build ), \"map\" : None if map is None else staticmethod ( map ), \"validate_outputs\" : ( None if validate_outputs is None else staticmethod ( validate_outputs ) ), \"version\" : version , } . items () if v is not None }, ) return decorate class ProducerOutput ( Model ): producer : Producer position : int # TODO: Support named output (defaulting to artifact classname?) Artifact . update_forward_refs ( ProducerOutput = ProducerOutput )","title":"Module arti.producers"},{"location":"reference/arti/producers/#variables","text":"BuildInputs BuildSig InputPartitions MapInputs MapSig Outputs PartitionDependencies TYPE_CHECKING ValidateSig","title":"Variables"},{"location":"reference/arti/producers/#functions","text":"","title":"Functions"},{"location":"reference/arti/producers/#producer","text":"def producer ( * , annotations : 'Optional[tuple[Annotation, ...]]' = None , map : 'Optional[MapSig]' = None , name : 'Optional[str]' = None , validate_outputs : 'Optional[ValidateSig]' = None , version : 'Optional[Version]' = None ) -> 'Callable[[BuildSig], type[Producer]]' View Source def producer ( * , annotations : Optional [ tuple[Annotation, ... ] ] = None , map : Optional [ MapSig ] = None , name : Optional [ str ] = None , validate_outputs : Optional [ ValidateSig ] = None , version : Optional [ Version ] = None , ) -> Callable [ [BuildSig ] , type [ Producer ] ]: def decorate ( build : BuildSig ) -> type [ Producer ] : nonlocal name name = build . __name__ if name is None else name __annotations__ : dict [ str, Any ] = {} for param in signature ( build ). parameters . values () : with wrap_exc ( ValueError , prefix = f \"{name} {param.name} param\" ) : view = View . from_annotation ( param . annotation , mode = \"READ\" ) __annotations__ [ param.name ] = view . artifact_class # If overriding , set an explicit \"annotations\" hint until [ 1 ] is released . # # 1 : https : // github . com / samuelcolvin / pydantic / pull / 3018 if annotations : __annotations__ [ \"annotations\" ] = tuple [ Annotation, ... ] if version : __annotations__ [ \"version\" ] = Version return type ( name , ( Producer ,), { k : v for k , v in { \"__annotations__\" : __annotations__ , \"__module__\" : get_module_name ( depth = 2 ), # Not our module , but our caller ' s . \"annotations\" : annotations , \"build\" : staticmethod ( build ), \"map\" : None if map is None else staticmethod ( map ), \"validate_outputs\" : ( None if validate_outputs is None else staticmethod ( validate_outputs ) ), \"version\" : version , } . items () if v is not None } , ) return decorate","title":"producer"},{"location":"reference/arti/producers/#classes","text":"","title":"Classes"},{"location":"reference/arti/producers/#producer_1","text":"class Producer ( __pydantic_self__ , ** data : Any ) View Source class Producer ( Model ) : \"\"\"A Producer is a task that builds one or more Artifacts.\"\"\" # User fields / methods annotations : tuple [ Annotation, ... ] = () version : Version = SemVer ( major = 0 , minor = 0 , patch = 1 ) # The map / build / validate_outputs parameters are intended to be dynamic and set by subclasses , # however mypy doesn 't like the \"incompatible\" signature on subclasses if actually defined here # (nor support ParamSpec yet). `map` is generated during subclassing if not set, `build` is # required, and `validate_outputs` defaults to no-op checks (hence is the only one with a # provided method). # # These must be @classmethods or @staticmethods. map: ClassVar[MapSig] build: ClassVar[BuildSig] if TYPE_CHECKING: validate_outputs: ClassVar[ValidateSig] else: @staticmethod def validate_outputs(*outputs: Any) -> Union[bool, tuple[bool, str]]: \"\"\"Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\"\" return True, \"No validation performed.\" # Internal fields/methods _abstract_: ClassVar[bool] = True _fingerprint_excludes_ = frozenset([\"annotations\"]) # NOTE: The following are set in __init_subclass__ _input_artifact_classes_: ClassVar[frozendict[str, type[Artifact]]] _build_inputs_: ClassVar[BuildInputs] _build_sig_: ClassVar[Signature] _map_inputs_: ClassVar[MapInputs] _map_sig_: ClassVar[Signature] _outputs_: ClassVar[Outputs] @classmethod def __init_subclass__(cls, **kwargs: Any) -> None: super().__init_subclass__(**kwargs) if not cls._abstract_: with wrap_exc(ValueError, prefix=cls.__name__): cls._input_artifact_classes_ = cls._validate_fields() with wrap_exc(ValueError, prefix=\".build\"): ( cls._build_sig_, cls._build_inputs_, cls._outputs_, ) = cls._validate_build_sig() with wrap_exc(ValueError, prefix=\".validate_output\"): cls._validate_validate_output_sig() with wrap_exc(ValueError, prefix=\".map\"): cls._map_sig_, cls._map_inputs_ = cls._validate_map_sig() cls._validate_no_unused_fields() @classmethod def _validate_fields(cls) -> frozendict[str, type[Artifact]]: # NOTE: Aside from the base producer fields, all others should (currently) be Artifacts. # # Users can set additional class attributes, but they must be properly hinted as ClassVars. # These won' t interact with the \"framework\" and can 't be parameters to build/map. artifact_fields = {k: v for k, v in cls.__fields__.items() if k not in Producer.__fields__} for name, field in artifact_fields.items(): with wrap_exc(ValueError, prefix=f\".{name}\"): if not (field.default is None and field.default_factory is None and field.required): raise ValueError(\"field must not have a default nor be Optional.\") if not lenient_issubclass(field.outer_type_, Artifact): raise ValueError( f\"type hint must be an Artifact subclass, got: {field.outer_type_}\" ) return frozendict({name: field.outer_type_ for name, field in artifact_fields.items()}) @classmethod def _validate_parameters( cls, sig: Signature, *, validator: Callable[[str, Parameter], _T] ) -> Iterator[_T]: if undefined_params := set(sig.parameters) - set(cls._input_artifact_classes_): raise ValueError( f\"the following parameter(s) must be defined as a field: {undefined_params}\" ) for name, param in sig.parameters.items(): with wrap_exc(ValueError, prefix=f\" {name} param\"): if param.annotation is param.empty: raise ValueError(\"must have a type hint.\") if param.default is not param.empty: raise ValueError(\"must not have a default.\") if param.kind not in (param.POSITIONAL_OR_KEYWORD, param.KEYWORD_ONLY): raise ValueError(\"must be usable as a keyword argument.\") yield validator(name, param) @classmethod def _validate_build_param(cls, name: str, param: Parameter) -> tuple[str, View]: annotation = param.annotation field_artifact_class = cls._input_artifact_classes_[param.name] # If there is no Artifact hint, add in the field value as the default. if get_item_from_annotated(annotation, Artifact, is_subclass=True) is None: annotation = Annotated[annotation, field_artifact_class] view = View.from_annotation(annotation, mode=\"READ\") if view.artifact_class != field_artifact_class: raise ValueError( f\"annotation Artifact class ({view.artifact_class}) does not match that set on the field ({field_artifact_class}).\" ) return name, view @classmethod def _validate_build_sig_return(cls, annotation: Any, *, i: int) -> View: with wrap_exc(ValueError, prefix=f\" {ordinal(i+1)} return\"): return View.from_annotation(annotation, mode=\"WRITE\") @classmethod def _validate_build_sig(cls) -> tuple[Signature, BuildInputs, Outputs]: \"\"\"Validate the .build method\"\"\" if not hasattr(cls, \"build\"): raise ValueError(\"must be implemented\") if not isinstance(getattr_static(cls, \"build\"), (classmethod, staticmethod)): raise ValueError(\"must be a @classmethod or @staticmethod\") build_sig = signature(cls.build, force_tuple_return=True, remove_owner=True) # Validate the parameters build_inputs = BuildInputs( cls._validate_parameters(build_sig, validator=cls._validate_build_param) ) # Validate the return definition return_annotation = build_sig.return_annotation if return_annotation is build_sig.empty: # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError(\"a return value must be set with the output Artifact(s).\") if return_annotation == (NoneType,): raise ValueError(\"missing return signature\") outputs = Outputs( cls._validate_build_sig_return(annotation, i=i) for i, annotation in enumerate(return_annotation) ) # Validate all outputs have equivalent partitioning schemes. # # We currently require the partition key type *and* name to match, but in the future we # might be able to extend the dependency metadata to support heterogeneous names if # necessary. seen_key_types = {PartitionKey.types_from(view.type) for view in outputs} if len(seen_key_types) != 1: raise ValueError(\"all outputs must have the same partitioning scheme\") return build_sig, build_inputs, outputs @classmethod def _validate_validate_output_sig(cls) -> None: build_output_hints = [ get_args(hint)[0] if get_origin(hint) is Annotated else hint for hint in cls._build_sig_.return_annotation ] match_build_str = f\"match the `.build` return (`{build_output_hints}`)\" validate_parameters = signature(cls.validate_outputs).parameters def param_matches(param: Parameter, build_return: type) -> bool: # Skip checking non-hinted parameters to allow lambdas. # # NOTE: Parameter type hints are *contravariant* (you can' t pass a \"Manager\" into a # function expecting an \"Employee\" ), hence the lenient_issubclass has build_return as # the subtype and param . annotation as the supertype . return param . annotation is param . empty or lenient_issubclass ( build_return , param . annotation ) if ( # Allow ` * args : Any ` or ` * args : T ` for ` build (...) -> tuple [ T, ... ] ` len ( validate_parameters ) == 1 and ( param : = tuple ( validate_parameters . values ()) [ 0 ] ). kind == param . VAR_POSITIONAL ) : if not all ( param_matches ( param , output_hint ) for output_hint in build_output_hints ) : with wrap_exc ( ValueError , prefix = f \" {param.name} param\" ) : raise ValueError ( f \"type hint must be `Any` or {match_build_str}\" ) else : # Otherwise , check pairwise if len ( validate_parameters ) != len ( build_output_hints ) : raise ValueError ( f \"must {match_build_str}\" ) for i , ( name , param ) in enumerate ( validate_parameters . items ()) : with wrap_exc ( ValueError , prefix = f \" {name} param\" ) : if param . default is not param . empty : raise ValueError ( \"must not have a default.\" ) if param . kind not in ( param . POSITIONAL_ONLY , param . POSITIONAL_OR_KEYWORD ) : raise ValueError ( \"must be usable as a positional argument.\" ) if not param_matches ( param , ( expected : = build_output_hints [ i ] )) : raise ValueError ( f \"type hint must match the {ordinal(i+1)} `.build` return (`{expected}`)\" ) # TODO : Validate return signature ? @classmethod def _validate_map_param ( cls , name : str , param : Parameter ) -> str : # TODO : Should we add some ArtifactPartition [ MyArtifact ] type ? if param . annotation != StoragePartitions : raise ValueError ( \"type hint must be `StoragePartitions`\" ) return name @classmethod def _validate_map_sig ( cls ) -> tuple [ Signature, MapInputs ] : \"\"\"Validate partitioned Artifacts and the .map method\"\"\" if not hasattr ( cls , \"map\" ) : # TODO : Add runtime checking of ` map ` output ( ie : output aligns w / output # artifacts and such ). if any ( is_partitioned ( view . type ) for view in cls . _outputs_ ) : raise ValueError ( \"must be implemented when the `build` outputs are partitioned\" ) def map ( ** kwargs : StoragePartitions ) -> PartitionDependencies : return PartitionDependencies ( { NotPartitioned : frozendict ( kwargs ) } ) # Narrow the map signature , which is validated below and used at graph build time ( via # cls . _map_inputs_ ) to determine what arguments to pass to map . map . __signature__ = Signature ( # type : ignore [ attr-defined ] [ Parameter(name=name, annotation=StoragePartitions, kind=Parameter.KEYWORD_ONLY) for name, artifact in cls._input_artifact_classes_.items() if name in cls._build_inputs_ ] , return_annotation = PartitionDependencies , ) cls . map = cast ( MapSig , staticmethod ( map )) if not isinstance ( getattr_static ( cls , \"map\" ), ( classmethod , staticmethod )) : raise ValueError ( \"must be a @classmethod or @staticmethod\" ) map_sig = signature ( cls . map ) map_inputs = MapInputs ( cls . _validate_parameters ( map_sig , validator = cls . _validate_map_param )) return map_sig , map_inputs # TODO : Verify map output hint matches TBD spec @classmethod def _validate_no_unused_fields ( cls ) -> None : if unused_fields : = set ( cls . _input_artifact_classes_ ) - ( set ( cls . _build_sig_ . parameters ) | set ( cls . _map_sig_ . parameters ) ) : raise ValueError ( f \"the following fields aren't used in `.build` or `.map`: {unused_fields}\" ) @validator ( \"*\" ) @classmethod def _validate_instance_artifact_args ( cls , value : Artifact , field : ModelField ) -> Artifact : if ( view : = cls . _build_inputs_ . get ( field . name )) is not None : view . check_artifact_compatibility ( value ) return value # NOTE : pydantic defines . __iter__ to return ` self . __dict__ . items () ` to support ` dict ( model ) ` , # but we want to override to support easy expansion / assignment to a Graph without ` . out () ` ( eg : # ` g . artifacts . a , g . artifacts . b = MyProducer (...) ` ). def __iter__ ( self ) -> Iterator [ Artifact ] : # type : ignore [ override ] ret = self . out () if not isinstance ( ret , tuple ) : ret = ( ret ,) return iter ( ret ) def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str, StoragePartitions ] ) -> Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_inputs_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected {expected_names}, got {input_names}\" ) # We only care if the * code * or * input partition contents * changed , not if the input file # paths changed ( but have the same content as a prior run ). return Fingerprint . from_string ( self . _class_key_ ). combine ( self . version . fingerprint , * ( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), ) def compute_dependencies ( self , input_partitions : InputPartitions ) -> tuple [ PartitionDependencies, InputFingerprints ] : # TODO : Validate the partition_dependencies against the Producer ' s partitioning scheme and # such ( basically , check user error ). eg : if output is not partitioned , we expect only 1 # entry in partition_dependencies ( NotPartitioned ). partition_dependencies = self . map ( ** { name : partitions for name , partitions in input_partitions . items () if name in self . _map_inputs_ } ) partition_input_fingerprints = InputFingerprints ( { composite_key : self . compute_input_fingerprint ( dependency_partitions ) for composite_key , dependency_partitions in partition_dependencies . items () } ) return partition_dependencies , partition_input_fingerprints @property def inputs ( self ) -> dict [ str, Artifact ] : return { k : getattr ( self , k ) for k in self . _input_artifact_classes_ } def out ( self , * outputs : Artifact ) -> Union [ Artifact, tuple[Artifact, ... ] ]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : outputs = tuple ( view . artifact_class ( type = view . type ) for view in self . _outputs_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = \", \" . join ( [ str(v) for v in self._build_sig_.return_annotation ] ) raise ValueError ( f \"{self._class_key_}.out() - expected {expected_n} arguments of ({ret_str}), but got: {outputs}\" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : view = self . _outputs_ [ ord ] with wrap_exc ( ValueError , prefix = f \"{self._class_key_}.out() {ordinal(ord+1)} argument\" ) : view . check_artifact_compatibility ( artifact ) if artifact . producer_output is not None : raise ValueError ( f \"{artifact} is produced by {artifact.producer_output.producer}!\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord ) } ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs","title":"Producer"},{"location":"reference/arti/producers/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/producers/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/producers/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/producers/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/producers/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/producers/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/producers/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/producers/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/producers/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/producers/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/producers/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/producers/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/producers/#validate_outputs","text":"def validate_outputs ( * outputs : 'Any' ) -> 'Union[bool, tuple[bool, str]]' Validate the Producer.build outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of build will be passed in as it was returned, for example: def build(...): return 1, 2 will result in validate_outputs(1, 2) . NOTE: validate_outputs is a stopgap until Statistics and Thresholds are fully implemented. View Source @staticmethod def validate_outputs ( * outputs : Any ) -> Union [ bool , tuple [ bool , str ]] : \" \"\" Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\" \" return True , \"No validation performed.\"","title":"validate_outputs"},{"location":"reference/arti/producers/#instance-variables","text":"fingerprint inputs","title":"Instance variables"},{"location":"reference/arti/producers/#methods","text":"","title":"Methods"},{"location":"reference/arti/producers/#compute_dependencies","text":"def compute_dependencies ( self , input_partitions : 'InputPartitions' ) -> 'tuple[PartitionDependencies, InputFingerprints]' View Source def compute_dependencies ( self , input_partitions : InputPartitions ) -> tuple [ PartitionDependencies , InputFingerprints ] : # TODO : Validate the partition_dependencies against the Producer 's partitioning scheme and # such (basically, check user error). eg: if output is not partitioned, we expect only 1 # entry in partition_dependencies (NotPartitioned). partition_dependencies = self.map( **{ name: partitions for name, partitions in input_partitions.items() if name in self._map_inputs_ } ) partition_input_fingerprints = InputFingerprints( { composite_key: self.compute_input_fingerprint(dependency_partitions) for composite_key, dependency_partitions in partition_dependencies.items() } ) return partition_dependencies, partition_input_fingerprints","title":"compute_dependencies"},{"location":"reference/arti/producers/#compute_input_fingerprint","text":"def compute_input_fingerprint ( self , dependency_partitions : 'frozendict[str, StoragePartitions]' ) -> 'Fingerprint' View Source def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str , StoragePartitions ] ) - > Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_inputs_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected {expected_names}, got {input_names}\" ) # We only care if the * code * or * input partition contents * changed , not if the input file # paths changed ( but have the same content as a prior run ). return Fingerprint . from_string ( self . _class_key_ ) . combine ( self . version . fingerprint , *( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), )","title":"compute_input_fingerprint"},{"location":"reference/arti/producers/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/producers/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/producers/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/producers/#out","text":"def out ( self , * outputs : 'Artifact' ) -> 'Union[Artifact, tuple[Artifact, ...]]' Configure the output Artifacts this Producer will build. The arguments are matched to the Producer.build return signature in order. View Source def out ( self , * outputs : Artifact ) -> Union [ Artifact, tuple[Artifact, ... ] ]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : outputs = tuple ( view . artifact_class ( type = view . type ) for view in self . _outputs_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = \", \" . join ( [ str(v) for v in self._build_sig_.return_annotation ] ) raise ValueError ( f \"{self._class_key_}.out() - expected {expected_n} arguments of ({ret_str}), but got: {outputs}\" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : view = self . _outputs_ [ ord ] with wrap_exc ( ValueError , prefix = f \"{self._class_key_}.out() {ordinal(ord+1)} argument\" ) : view . check_artifact_compatibility ( artifact ) if artifact . producer_output is not None : raise ValueError ( f \"{artifact} is produced by {artifact.producer_output.producer}!\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord ) } ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs","title":"out"},{"location":"reference/arti/producers/#produceroutput","text":"class ProducerOutput ( __pydantic_self__ , ** data : Any ) View Source class ProducerOutput ( Model ): producer: Producer position: int # TODO: Support named output (defaulting to artifact classname?)","title":"ProducerOutput"},{"location":"reference/arti/producers/#ancestors-in-mro_1","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/producers/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/arti/producers/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/producers/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/producers/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/producers/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/producers/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/producers/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/producers/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/producers/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/producers/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/producers/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/producers/#instance-variables_1","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/producers/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/producers/#copy_1","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/producers/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/producers/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/statistics/","text":"Module arti.statistics None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from arti.internal.models import Model class Statistic ( Model ): pass # TODO: Determine the interface for Statistics # class FieldStatistic(Statistic): # \"\"\" A FieldStatistic is a Statistic associated with a particular Artifact field. # \"\"\" # # # class Count(FieldStatistic): # type = Int64() # # # class CountDistinct(FieldStatistic): # type = Int64() # # # class MaxInt64(FieldStatistic): # type = Int64() # # # class MinInt64(FieldStatistic): # type = Int64() # # # class SumInt64(FieldStatistic): # type = Int64() Classes Statistic class Statistic ( __pydantic_self__ , ** data : Any ) View Source class Statistic ( Model ): pass # TODO: Determine the interface for Statistics Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Statistics"},{"location":"reference/arti/statistics/#module-artistatistics","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from arti.internal.models import Model class Statistic ( Model ): pass # TODO: Determine the interface for Statistics # class FieldStatistic(Statistic): # \"\"\" A FieldStatistic is a Statistic associated with a particular Artifact field. # \"\"\" # # # class Count(FieldStatistic): # type = Int64() # # # class CountDistinct(FieldStatistic): # type = Int64() # # # class MaxInt64(FieldStatistic): # type = Int64() # # # class MinInt64(FieldStatistic): # type = Int64() # # # class SumInt64(FieldStatistic): # type = Int64()","title":"Module arti.statistics"},{"location":"reference/arti/statistics/#classes","text":"","title":"Classes"},{"location":"reference/arti/statistics/#statistic","text":"class Statistic ( __pydantic_self__ , ** data : Any ) View Source class Statistic ( Model ): pass # TODO: Determine the interface for Statistics","title":"Statistic"},{"location":"reference/arti/statistics/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/statistics/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/statistics/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/statistics/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/statistics/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/statistics/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/statistics/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/statistics/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/statistics/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/statistics/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/statistics/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/statistics/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/statistics/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/statistics/#methods","text":"","title":"Methods"},{"location":"reference/arti/statistics/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/statistics/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/statistics/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/thresholds/","text":"Module arti.thresholds None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from typing import Any , ClassVar from arti.internal.models import Model from arti.types import Type class Threshold ( Model ): type : ClassVar [ type [ Type ]] def check ( self , value : Any ) -> bool : raise NotImplementedError () Classes Threshold class Threshold ( __pydantic_self__ , ** data : Any ) View Source class Threshold ( Model ) : type : ClassVar [ type[Type ] ] def check ( self , value : Any ) -> bool : raise NotImplementedError () Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods check def check ( self , value : 'Any' ) -> 'bool' View Source def check ( self , value : Any ) -> bool : raise NotImplementedError () copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Thresholds"},{"location":"reference/arti/thresholds/#module-artithresholds","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from typing import Any , ClassVar from arti.internal.models import Model from arti.types import Type class Threshold ( Model ): type : ClassVar [ type [ Type ]] def check ( self , value : Any ) -> bool : raise NotImplementedError ()","title":"Module arti.thresholds"},{"location":"reference/arti/thresholds/#classes","text":"","title":"Classes"},{"location":"reference/arti/thresholds/#threshold","text":"class Threshold ( __pydantic_self__ , ** data : Any ) View Source class Threshold ( Model ) : type : ClassVar [ type[Type ] ] def check ( self , value : Any ) -> bool : raise NotImplementedError ()","title":"Threshold"},{"location":"reference/arti/thresholds/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/thresholds/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/thresholds/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/thresholds/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/thresholds/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/thresholds/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/thresholds/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/thresholds/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/thresholds/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/thresholds/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/thresholds/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/thresholds/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/thresholds/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/thresholds/#methods","text":"","title":"Methods"},{"location":"reference/arti/thresholds/#check","text":"def check ( self , value : 'Any' ) -> 'bool' View Source def check ( self , value : Any ) -> bool : raise NotImplementedError ()","title":"check"},{"location":"reference/arti/thresholds/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/thresholds/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/thresholds/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/versions/","text":"Module arti.versions None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import inspect import subprocess from collections.abc import Callable from datetime import datetime , timezone from typing import Any , cast from pydantic import Field , validator from arti.fingerprints import Fingerprint from arti.internal.models import Model class Version ( Model ): _abstract_ = True class GitCommit ( Version ): sha : str = Field ( default_factory = ( lambda : subprocess . check_output ([ \"git\" , \"rev-parse\" , \"HEAD\" ]) . decode () . strip () ) ) class SemVer ( Version ): \"\"\"SemVer fingerprinting only considers the major component, unless it is less than 0. By only considering the major version, we can add incremental bumps to a Producer without triggering historical backfills. The major version MUST be incremented on schema or methodological changes. \"\"\" major : int minor : int patch : int @property def fingerprint ( self ) -> Fingerprint : s = str ( self . major ) if self . major == 0 : s = f \" { self . major } . { self . minor } . { self . patch } \" return Fingerprint . from_string ( s ) class String ( Version ): value : str class _SourceDescriptor : # Experimental :) # Using AST rather than literal source will likely be less \"noisy\": # https://github.com/artigraph/artigraph/pull/36#issuecomment-824131156 def __get__ ( self , obj : Any , type_ : type ) -> String : return String ( value = inspect . getsource ( type_ )) _Source = cast ( Callable [[], String ], _SourceDescriptor ) class Timestamp ( Version ): dt : datetime = Field ( default_factory = lambda : datetime . now ( tz = timezone . utc )) @validator ( \"dt\" , always = True ) @classmethod def _requires_timezone ( cls , dt : datetime ) -> datetime : if dt is not None and dt . tzinfo is None : raise ValueError ( \"Timestamp requires a timezone-aware datetime!\" ) return dt @property def fingerprint ( self ) -> Fingerprint : return Fingerprint . from_int ( round ( self . dt . timestamp ())) # TODO: Consider a Timestamp like version with a \"frequency\" arg (day, hour, etc) that we floor/ceil # to trigger \"scheduled\" invalidation. This doesn't solve imperative work like \"process X every hour # on the hour and play catch up for missed runs\" (rather, we should aim to represent that with # upstream Artifact partitions), but rather solves work like \"ingest all data at most this # frequently\" (eg: full export from an API - no point in a backfill). Classes GitCommit class GitCommit ( __pydantic_self__ , ** data : Any ) View Source class GitCommit ( Version ): sha: str = Field ( default_factory =( lambda: subprocess . check_output ([ \"git\" , \"rev-parse\" , \"HEAD\" ]). decode (). strip () ) ) Ancestors (in MRO) arti.versions.Version arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . SemVer class SemVer ( __pydantic_self__ , ** data : Any ) View Source class SemVer ( Version ) : \"\"\"SemVer fingerprinting only considers the major component, unless it is less than 0. By only considering the major version, we can add incremental bumps to a Producer without triggering historical backfills. The major version MUST be incremented on schema or methodological changes. \"\"\" major : int minor : int patch : int @property def fingerprint ( self ) -> Fingerprint : s = str ( self . major ) if self . major == 0 : s = f \"{self.major}.{self.minor}.{self.patch}\" return Fingerprint . from_string ( s ) Ancestors (in MRO) arti.versions.Version arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . String class String ( __pydantic_self__ , ** data : Any ) View Source class String ( Version ): value: str Ancestors (in MRO) arti.versions.Version arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Timestamp class Timestamp ( __pydantic_self__ , ** data : Any ) View Source class Timestamp ( Version ) : dt : datetime = Field ( default_factory = lambda : datetime . now ( tz = timezone . utc )) @validator ( \"dt\" , always = True ) @classmethod def _requires_timezone ( cls , dt : datetime ) -> datetime : if dt is not None and dt . tzinfo is None : raise ValueError ( \"Timestamp requires a timezone-aware datetime!\" ) return dt @property def fingerprint ( self ) -> Fingerprint : return Fingerprint . from_int ( round ( self . dt . timestamp ())) Ancestors (in MRO) arti.versions.Version arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Version class Version ( __pydantic_self__ , ** data : Any ) View Source class Version ( Model ): _abstract_ = True Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.versions.GitCommit arti.versions.SemVer arti.versions.String arti.versions.Timestamp Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Versions"},{"location":"reference/arti/versions/#module-artiversions","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import inspect import subprocess from collections.abc import Callable from datetime import datetime , timezone from typing import Any , cast from pydantic import Field , validator from arti.fingerprints import Fingerprint from arti.internal.models import Model class Version ( Model ): _abstract_ = True class GitCommit ( Version ): sha : str = Field ( default_factory = ( lambda : subprocess . check_output ([ \"git\" , \"rev-parse\" , \"HEAD\" ]) . decode () . strip () ) ) class SemVer ( Version ): \"\"\"SemVer fingerprinting only considers the major component, unless it is less than 0. By only considering the major version, we can add incremental bumps to a Producer without triggering historical backfills. The major version MUST be incremented on schema or methodological changes. \"\"\" major : int minor : int patch : int @property def fingerprint ( self ) -> Fingerprint : s = str ( self . major ) if self . major == 0 : s = f \" { self . major } . { self . minor } . { self . patch } \" return Fingerprint . from_string ( s ) class String ( Version ): value : str class _SourceDescriptor : # Experimental :) # Using AST rather than literal source will likely be less \"noisy\": # https://github.com/artigraph/artigraph/pull/36#issuecomment-824131156 def __get__ ( self , obj : Any , type_ : type ) -> String : return String ( value = inspect . getsource ( type_ )) _Source = cast ( Callable [[], String ], _SourceDescriptor ) class Timestamp ( Version ): dt : datetime = Field ( default_factory = lambda : datetime . now ( tz = timezone . utc )) @validator ( \"dt\" , always = True ) @classmethod def _requires_timezone ( cls , dt : datetime ) -> datetime : if dt is not None and dt . tzinfo is None : raise ValueError ( \"Timestamp requires a timezone-aware datetime!\" ) return dt @property def fingerprint ( self ) -> Fingerprint : return Fingerprint . from_int ( round ( self . dt . timestamp ())) # TODO: Consider a Timestamp like version with a \"frequency\" arg (day, hour, etc) that we floor/ceil # to trigger \"scheduled\" invalidation. This doesn't solve imperative work like \"process X every hour # on the hour and play catch up for missed runs\" (rather, we should aim to represent that with # upstream Artifact partitions), but rather solves work like \"ingest all data at most this # frequently\" (eg: full export from an API - no point in a backfill).","title":"Module arti.versions"},{"location":"reference/arti/versions/#classes","text":"","title":"Classes"},{"location":"reference/arti/versions/#gitcommit","text":"class GitCommit ( __pydantic_self__ , ** data : Any ) View Source class GitCommit ( Version ): sha: str = Field ( default_factory =( lambda: subprocess . check_output ([ \"git\" , \"rev-parse\" , \"HEAD\" ]). decode (). strip () ) )","title":"GitCommit"},{"location":"reference/arti/versions/#ancestors-in-mro","text":"arti.versions.Version arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/versions/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/versions/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/versions/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/versions/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/versions/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/versions/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/versions/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/versions/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/versions/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/versions/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/versions/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/versions/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/versions/#methods","text":"","title":"Methods"},{"location":"reference/arti/versions/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/versions/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/versions/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/versions/#semver","text":"class SemVer ( __pydantic_self__ , ** data : Any ) View Source class SemVer ( Version ) : \"\"\"SemVer fingerprinting only considers the major component, unless it is less than 0. By only considering the major version, we can add incremental bumps to a Producer without triggering historical backfills. The major version MUST be incremented on schema or methodological changes. \"\"\" major : int minor : int patch : int @property def fingerprint ( self ) -> Fingerprint : s = str ( self . major ) if self . major == 0 : s = f \"{self.major}.{self.minor}.{self.patch}\" return Fingerprint . from_string ( s )","title":"SemVer"},{"location":"reference/arti/versions/#ancestors-in-mro_1","text":"arti.versions.Version arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/versions/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/arti/versions/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/versions/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/versions/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/versions/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/versions/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/versions/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/versions/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/versions/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/versions/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/versions/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/versions/#instance-variables_1","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/versions/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/versions/#copy_1","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/versions/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/versions/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/versions/#string","text":"class String ( __pydantic_self__ , ** data : Any ) View Source class String ( Version ): value: str","title":"String"},{"location":"reference/arti/versions/#ancestors-in-mro_2","text":"arti.versions.Version arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/versions/#class-variables_2","text":"Config","title":"Class variables"},{"location":"reference/arti/versions/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/arti/versions/#construct_2","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/versions/#from_orm_2","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/versions/#parse_file_2","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/versions/#parse_obj_2","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/versions/#parse_raw_2","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/versions/#schema_2","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/versions/#schema_json_2","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/versions/#update_forward_refs_2","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/versions/#validate_2","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/versions/#instance-variables_2","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/versions/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/versions/#copy_2","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/versions/#dict_2","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/versions/#json_2","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/versions/#timestamp","text":"class Timestamp ( __pydantic_self__ , ** data : Any ) View Source class Timestamp ( Version ) : dt : datetime = Field ( default_factory = lambda : datetime . now ( tz = timezone . utc )) @validator ( \"dt\" , always = True ) @classmethod def _requires_timezone ( cls , dt : datetime ) -> datetime : if dt is not None and dt . tzinfo is None : raise ValueError ( \"Timestamp requires a timezone-aware datetime!\" ) return dt @property def fingerprint ( self ) -> Fingerprint : return Fingerprint . from_int ( round ( self . dt . timestamp ()))","title":"Timestamp"},{"location":"reference/arti/versions/#ancestors-in-mro_3","text":"arti.versions.Version arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/versions/#class-variables_3","text":"Config","title":"Class variables"},{"location":"reference/arti/versions/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/arti/versions/#construct_3","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/versions/#from_orm_3","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/versions/#parse_file_3","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/versions/#parse_obj_3","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/versions/#parse_raw_3","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/versions/#schema_3","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/versions/#schema_json_3","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/versions/#update_forward_refs_3","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/versions/#validate_3","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/versions/#instance-variables_3","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/versions/#methods_3","text":"","title":"Methods"},{"location":"reference/arti/versions/#copy_3","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/versions/#dict_3","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/versions/#json_3","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/versions/#version","text":"class Version ( __pydantic_self__ , ** data : Any ) View Source class Version ( Model ): _abstract_ = True","title":"Version"},{"location":"reference/arti/versions/#ancestors-in-mro_4","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/versions/#descendants","text":"arti.versions.GitCommit arti.versions.SemVer arti.versions.String arti.versions.Timestamp","title":"Descendants"},{"location":"reference/arti/versions/#class-variables_4","text":"Config","title":"Class variables"},{"location":"reference/arti/versions/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/arti/versions/#construct_4","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/versions/#from_orm_4","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/versions/#parse_file_4","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/versions/#parse_obj_4","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/versions/#parse_raw_4","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/versions/#schema_4","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/versions/#schema_json_4","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/versions/#update_forward_refs_4","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/versions/#validate_4","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/versions/#instance-variables_4","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/versions/#methods_4","text":"","title":"Methods"},{"location":"reference/arti/versions/#copy_4","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/versions/#dict_4","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/versions/#json_4","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/backends/","text":"Module arti.backends None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from abc import abstractmethod from collections.abc import Iterator from contextlib import contextmanager from typing import TYPE_CHECKING , Any , Callable , Generic , TypeVar from pydantic.fields import ModelField from arti.artifacts import Artifact from arti.fingerprints import Fingerprint from arti.internal.models import Model from arti.internal.type_hints import Self from arti.partitions import InputFingerprints from arti.storage import StoragePartitions if TYPE_CHECKING : from arti.graphs import Graph , GraphSnapshot # TODO: Consider adding CRUD methods for \"everything\"? # # Likely worth making a distinction between lower level (\"CRUD\") methods vs higher level (\"RPC\" or # \"composing\") methods. Executors should operate on the high level methods, but those may have # defaults simply calling the lower level methods. If high level methods can be optimized (eg: not a # bunch of low level calls, each own network call), Backend can override. class Connection : \"\"\"Connection is a wrapper around an active connection to a Backend resource. For example, a Backend connecting to a database might wrap up a SQLAlchemy connection in a Connection subclass implementing the required methods. \"\"\" # Artifact partitions - independent of a specific GraphSnapshot @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \"\"\"Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a GraphSnapshot. \"\"\" raise NotImplementedError () @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \"\"\"Add more partitions for a Storage spec.\"\"\" raise NotImplementedError () # Graph @abstractmethod def read_graph ( self , name : str , fingerprint : Fingerprint ) -> Graph : \"\"\"Fetch an instance of the named Graph.\"\"\" raise NotImplementedError () @abstractmethod def write_graph ( self , graph : Graph ) -> None : \"\"\"Write the Graph and all linked Artifacts and Producers to the database.\"\"\" raise NotImplementedError () # GraphSnapshot @abstractmethod def read_snapshot ( self , name : str , fingerprint : Fingerprint ) -> GraphSnapshot : \"\"\"Fetch an instance of the named GraphSnapshot.\"\"\" raise NotImplementedError () @abstractmethod def write_snapshot ( self , snapshot : GraphSnapshot ) -> None : \"\"\"Write the GraphSnapshot to the database.\"\"\" raise NotImplementedError () @abstractmethod def read_snapshot_tag ( self , name : str , tag : str ) -> GraphSnapshot : \"\"\"Fetch the GraphSnapshot for the named tag.\"\"\" raise NotImplementedError () @abstractmethod def write_snapshot_tag ( self , snapshot : GraphSnapshot , tag : str , overwrite : bool = False ) -> None : \"\"\"Stamp a GraphSnapshot with an arbitrary tag.\"\"\" raise NotImplementedError () @abstractmethod def read_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \"\"\"Read the known Partitions for the named Artifact in a specific GraphSnapshot.\"\"\" raise NotImplementedError () @abstractmethod def write_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \"\"\"Link the Partitions to the named Artifact in a specific GraphSnapshot.\"\"\" raise NotImplementedError () # Helpers def write_artifact_and_graph_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_snapshot_partitions ( snapshot , artifact_key , artifact , partitions ) @contextmanager def connect ( self ) -> Iterator [ Self ]: \"\"\"Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... \"\"\" yield self @classmethod def __get_validators__ ( cls ) -> list [ Callable [[ Any , ModelField ], Any ]]: \"\"\"Return an empty list of \"validators\". Allows using a Connection (which is not a model) as a field in other models without setting `arbitrary_types_allowed` (which applies broadly). [1]. 1: https://docs.pydantic.dev/usage/types/#generic-classes-as-types \"\"\" return [] ConnectionVar = TypeVar ( \"ConnectionVar\" , bound = Connection , covariant = True ) class Backend ( Model , Generic [ ConnectionVar ]): \"\"\"Backend represents a storage for internal Artigraph metadata. Backend storage is an addressable location (local path, database connection, etc) that tracks metadata for a collection of Graphs over time, including: - the Artifact(s)->Producer->Artifact(s) dependency graph - Artifact Annotations, Statistics, Partitions, and other metadata - Artifact and Producer Fingerprints - etc \"\"\" @contextmanager @abstractmethod def connect ( self ) -> Iterator [ ConnectionVar ]: raise NotImplementedError () Sub-modules arti.backends.memory Variables ConnectionVar TYPE_CHECKING Classes Backend class Backend ( __pydantic_self__ , ** data : Any ) View Source class Backend ( Model , Generic [ ConnectionVar ] ) : \"\"\"Backend represents a storage for internal Artigraph metadata. Backend storage is an addressable location (local path, database connection, etc) that tracks metadata for a collection of Graphs over time, including: - the Artifact(s)->Producer->Artifact(s) dependency graph - Artifact Annotations, Statistics, Partitions, and other metadata - Artifact and Producer Fingerprints - etc \"\"\" @contextmanager @abstractmethod def connect ( self ) -> Iterator [ ConnectionVar ] : raise NotImplementedError () Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic Descendants arti.backends.memory.MemoryBackend Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods connect def connect ( self ) -> 'Iterator[ConnectionVar]' View Source @contextmanager @abstractmethod def connect ( self ) -> Iterator [ ConnectionVar ] : raise NotImplementedError () copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Connection class Connection ( / , * args , ** kwargs ) View Source class Connection : \" \"\" Connection is a wrapper around an active connection to a Backend resource. For example, a Backend connecting to a database might wrap up a SQLAlchemy connection in a Connection subclass implementing the required methods. \"\" \" # Artifact partitions - independent of a specific GraphSnapshot @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \" \"\" Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a GraphSnapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \" \"\" Add more partitions for a Storage spec. \"\" \" raise NotImplementedError () # Graph @abstractmethod def read_graph ( self , name : str , fingerprint : Fingerprint ) -> Graph : \" \"\" Fetch an instance of the named Graph. \"\" \" raise NotImplementedError () @abstractmethod def write_graph ( self , graph : Graph ) -> None : \" \"\" Write the Graph and all linked Artifacts and Producers to the database. \"\" \" raise NotImplementedError () # GraphSnapshot @abstractmethod def read_snapshot ( self , name : str , fingerprint : Fingerprint ) -> GraphSnapshot : \" \"\" Fetch an instance of the named GraphSnapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_snapshot ( self , snapshot : GraphSnapshot ) -> None : \" \"\" Write the GraphSnapshot to the database. \"\" \" raise NotImplementedError () @abstractmethod def read_snapshot_tag ( self , name : str , tag : str ) -> GraphSnapshot : \" \"\" Fetch the GraphSnapshot for the named tag. \"\" \" raise NotImplementedError () @abstractmethod def write_snapshot_tag ( self , snapshot : GraphSnapshot , tag : str , overwrite : bool = False ) -> None : \" \"\" Stamp a GraphSnapshot with an arbitrary tag. \"\" \" raise NotImplementedError () @abstractmethod def read_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \" \"\" Read the known Partitions for the named Artifact in a specific GraphSnapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \" \"\" Link the Partitions to the named Artifact in a specific GraphSnapshot. \"\" \" raise NotImplementedError () # Helpers def write_artifact_and_graph_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_snapshot_partitions ( snapshot , artifact_key , artifact , partitions ) @contextmanager def connect ( self ) -> Iterator [ Self ] : \" \"\" Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... \"\" \" yield self @classmethod def __get_validators__ ( cls ) -> list [ Callable [[ Any , ModelField ] , Any ]] : \" \"\" Return an empty list of \" validators \". Allows using a Connection (which is not a model) as a field in other models without setting `arbitrary_types_allowed` (which applies broadly). [1]. 1: https://docs.pydantic.dev/usage/types/#generic-classes-as-types \"\" \" return [] Descendants arti.backends.memory.MemoryConnection Methods connect def connect ( self ) -> 'Iterator[Self]' Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... View Source @contextmanager def connect ( self ) -> Iterator [ Self ] : \"\"\"Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... \"\"\" yield self read_artifact_partitions def read_artifact_partitions ( self , artifact : 'Artifact' , input_fingerprints : 'InputFingerprints' = {} ) -> 'StoragePartitions' Read all known Partitions for this Storage spec. If input_fingerprints is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless input_fingerprints is provided matching those for a GraphSnapshot. View Source @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \" \"\" Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a GraphSnapshot. \"\" \" raise NotImplementedError () read_graph def read_graph ( self , name : 'str' , fingerprint : 'Fingerprint' ) -> 'Graph' Fetch an instance of the named Graph. View Source @abstractmethod def read_graph ( self , name : str , fingerprint : Fingerprint ) -> Graph : \"\"\"Fetch an instance of the named Graph.\"\"\" raise NotImplementedError () read_snapshot def read_snapshot ( self , name : 'str' , fingerprint : 'Fingerprint' ) -> 'GraphSnapshot' Fetch an instance of the named GraphSnapshot. View Source @abstractmethod def read_snapshot ( self , name : str , fingerprint : Fingerprint ) -> GraphSnapshot : \"\"\"Fetch an instance of the named GraphSnapshot.\"\"\" raise NotImplementedError () read_snapshot_partitions def read_snapshot_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' ) -> 'StoragePartitions' Read the known Partitions for the named Artifact in a specific GraphSnapshot. View Source @abstractmethod def read_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \"\"\"Read the known Partitions for the named Artifact in a specific GraphSnapshot.\"\"\" raise NotImplementedError () read_snapshot_tag def read_snapshot_tag ( self , name : 'str' , tag : 'str' ) -> 'GraphSnapshot' Fetch the GraphSnapshot for the named tag. View Source @abstractmethod def read_snapshot_tag ( self , name : str , tag : str ) -> GraphSnapshot : \"\"\"Fetch the GraphSnapshot for the named tag.\"\"\" raise NotImplementedError () write_artifact_and_graph_partitions def write_artifact_and_graph_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' View Source def write_artifact_and_graph_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_snapshot_partitions ( snapshot , artifact_key , artifact , partitions ) write_artifact_partitions def write_artifact_partitions ( self , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' Add more partitions for a Storage spec. View Source @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \"\"\"Add more partitions for a Storage spec.\"\"\" raise NotImplementedError () write_graph def write_graph ( self , graph : 'Graph' ) -> 'None' Write the Graph and all linked Artifacts and Producers to the database. View Source @abstractmethod def write_graph ( self , graph : Graph ) -> None : \"\"\"Write the Graph and all linked Artifacts and Producers to the database.\"\"\" raise NotImplementedError () write_snapshot def write_snapshot ( self , snapshot : 'GraphSnapshot' ) -> 'None' Write the GraphSnapshot to the database. View Source @abstractmethod def write_snapshot ( self , snapshot : GraphSnapshot ) -> None : \"\"\"Write the GraphSnapshot to the database.\"\"\" raise NotImplementedError () write_snapshot_partitions def write_snapshot_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' Link the Partitions to the named Artifact in a specific GraphSnapshot. View Source @abstractmethod def write_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \"\"\"Link the Partitions to the named Artifact in a specific GraphSnapshot.\"\"\" raise NotImplementedError () write_snapshot_tag def write_snapshot_tag ( self , snapshot : 'GraphSnapshot' , tag : 'str' , overwrite : 'bool' = False ) -> 'None' Stamp a GraphSnapshot with an arbitrary tag. View Source @abstractmethod def write_snapshot_tag ( self , snapshot : GraphSnapshot , tag : str , overwrite : bool = False ) -> None : \"\"\"Stamp a GraphSnapshot with an arbitrary tag.\"\"\" raise NotImplementedError ()","title":"Index"},{"location":"reference/arti/backends/#module-artibackends","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from abc import abstractmethod from collections.abc import Iterator from contextlib import contextmanager from typing import TYPE_CHECKING , Any , Callable , Generic , TypeVar from pydantic.fields import ModelField from arti.artifacts import Artifact from arti.fingerprints import Fingerprint from arti.internal.models import Model from arti.internal.type_hints import Self from arti.partitions import InputFingerprints from arti.storage import StoragePartitions if TYPE_CHECKING : from arti.graphs import Graph , GraphSnapshot # TODO: Consider adding CRUD methods for \"everything\"? # # Likely worth making a distinction between lower level (\"CRUD\") methods vs higher level (\"RPC\" or # \"composing\") methods. Executors should operate on the high level methods, but those may have # defaults simply calling the lower level methods. If high level methods can be optimized (eg: not a # bunch of low level calls, each own network call), Backend can override. class Connection : \"\"\"Connection is a wrapper around an active connection to a Backend resource. For example, a Backend connecting to a database might wrap up a SQLAlchemy connection in a Connection subclass implementing the required methods. \"\"\" # Artifact partitions - independent of a specific GraphSnapshot @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \"\"\"Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a GraphSnapshot. \"\"\" raise NotImplementedError () @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \"\"\"Add more partitions for a Storage spec.\"\"\" raise NotImplementedError () # Graph @abstractmethod def read_graph ( self , name : str , fingerprint : Fingerprint ) -> Graph : \"\"\"Fetch an instance of the named Graph.\"\"\" raise NotImplementedError () @abstractmethod def write_graph ( self , graph : Graph ) -> None : \"\"\"Write the Graph and all linked Artifacts and Producers to the database.\"\"\" raise NotImplementedError () # GraphSnapshot @abstractmethod def read_snapshot ( self , name : str , fingerprint : Fingerprint ) -> GraphSnapshot : \"\"\"Fetch an instance of the named GraphSnapshot.\"\"\" raise NotImplementedError () @abstractmethod def write_snapshot ( self , snapshot : GraphSnapshot ) -> None : \"\"\"Write the GraphSnapshot to the database.\"\"\" raise NotImplementedError () @abstractmethod def read_snapshot_tag ( self , name : str , tag : str ) -> GraphSnapshot : \"\"\"Fetch the GraphSnapshot for the named tag.\"\"\" raise NotImplementedError () @abstractmethod def write_snapshot_tag ( self , snapshot : GraphSnapshot , tag : str , overwrite : bool = False ) -> None : \"\"\"Stamp a GraphSnapshot with an arbitrary tag.\"\"\" raise NotImplementedError () @abstractmethod def read_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \"\"\"Read the known Partitions for the named Artifact in a specific GraphSnapshot.\"\"\" raise NotImplementedError () @abstractmethod def write_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \"\"\"Link the Partitions to the named Artifact in a specific GraphSnapshot.\"\"\" raise NotImplementedError () # Helpers def write_artifact_and_graph_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_snapshot_partitions ( snapshot , artifact_key , artifact , partitions ) @contextmanager def connect ( self ) -> Iterator [ Self ]: \"\"\"Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... \"\"\" yield self @classmethod def __get_validators__ ( cls ) -> list [ Callable [[ Any , ModelField ], Any ]]: \"\"\"Return an empty list of \"validators\". Allows using a Connection (which is not a model) as a field in other models without setting `arbitrary_types_allowed` (which applies broadly). [1]. 1: https://docs.pydantic.dev/usage/types/#generic-classes-as-types \"\"\" return [] ConnectionVar = TypeVar ( \"ConnectionVar\" , bound = Connection , covariant = True ) class Backend ( Model , Generic [ ConnectionVar ]): \"\"\"Backend represents a storage for internal Artigraph metadata. Backend storage is an addressable location (local path, database connection, etc) that tracks metadata for a collection of Graphs over time, including: - the Artifact(s)->Producer->Artifact(s) dependency graph - Artifact Annotations, Statistics, Partitions, and other metadata - Artifact and Producer Fingerprints - etc \"\"\" @contextmanager @abstractmethod def connect ( self ) -> Iterator [ ConnectionVar ]: raise NotImplementedError ()","title":"Module arti.backends"},{"location":"reference/arti/backends/#sub-modules","text":"arti.backends.memory","title":"Sub-modules"},{"location":"reference/arti/backends/#variables","text":"ConnectionVar TYPE_CHECKING","title":"Variables"},{"location":"reference/arti/backends/#classes","text":"","title":"Classes"},{"location":"reference/arti/backends/#backend","text":"class Backend ( __pydantic_self__ , ** data : Any ) View Source class Backend ( Model , Generic [ ConnectionVar ] ) : \"\"\"Backend represents a storage for internal Artigraph metadata. Backend storage is an addressable location (local path, database connection, etc) that tracks metadata for a collection of Graphs over time, including: - the Artifact(s)->Producer->Artifact(s) dependency graph - Artifact Annotations, Statistics, Partitions, and other metadata - Artifact and Producer Fingerprints - etc \"\"\" @contextmanager @abstractmethod def connect ( self ) -> Iterator [ ConnectionVar ] : raise NotImplementedError ()","title":"Backend"},{"location":"reference/arti/backends/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/arti/backends/#descendants","text":"arti.backends.memory.MemoryBackend","title":"Descendants"},{"location":"reference/arti/backends/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/backends/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/backends/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/backends/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/backends/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/backends/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/backends/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/backends/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/backends/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/backends/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/backends/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/backends/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/backends/#methods","text":"","title":"Methods"},{"location":"reference/arti/backends/#connect","text":"def connect ( self ) -> 'Iterator[ConnectionVar]' View Source @contextmanager @abstractmethod def connect ( self ) -> Iterator [ ConnectionVar ] : raise NotImplementedError ()","title":"connect"},{"location":"reference/arti/backends/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/backends/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/backends/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/backends/#connection","text":"class Connection ( / , * args , ** kwargs ) View Source class Connection : \" \"\" Connection is a wrapper around an active connection to a Backend resource. For example, a Backend connecting to a database might wrap up a SQLAlchemy connection in a Connection subclass implementing the required methods. \"\" \" # Artifact partitions - independent of a specific GraphSnapshot @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \" \"\" Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a GraphSnapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \" \"\" Add more partitions for a Storage spec. \"\" \" raise NotImplementedError () # Graph @abstractmethod def read_graph ( self , name : str , fingerprint : Fingerprint ) -> Graph : \" \"\" Fetch an instance of the named Graph. \"\" \" raise NotImplementedError () @abstractmethod def write_graph ( self , graph : Graph ) -> None : \" \"\" Write the Graph and all linked Artifacts and Producers to the database. \"\" \" raise NotImplementedError () # GraphSnapshot @abstractmethod def read_snapshot ( self , name : str , fingerprint : Fingerprint ) -> GraphSnapshot : \" \"\" Fetch an instance of the named GraphSnapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_snapshot ( self , snapshot : GraphSnapshot ) -> None : \" \"\" Write the GraphSnapshot to the database. \"\" \" raise NotImplementedError () @abstractmethod def read_snapshot_tag ( self , name : str , tag : str ) -> GraphSnapshot : \" \"\" Fetch the GraphSnapshot for the named tag. \"\" \" raise NotImplementedError () @abstractmethod def write_snapshot_tag ( self , snapshot : GraphSnapshot , tag : str , overwrite : bool = False ) -> None : \" \"\" Stamp a GraphSnapshot with an arbitrary tag. \"\" \" raise NotImplementedError () @abstractmethod def read_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \" \"\" Read the known Partitions for the named Artifact in a specific GraphSnapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \" \"\" Link the Partitions to the named Artifact in a specific GraphSnapshot. \"\" \" raise NotImplementedError () # Helpers def write_artifact_and_graph_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_snapshot_partitions ( snapshot , artifact_key , artifact , partitions ) @contextmanager def connect ( self ) -> Iterator [ Self ] : \" \"\" Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... \"\" \" yield self @classmethod def __get_validators__ ( cls ) -> list [ Callable [[ Any , ModelField ] , Any ]] : \" \"\" Return an empty list of \" validators \". Allows using a Connection (which is not a model) as a field in other models without setting `arbitrary_types_allowed` (which applies broadly). [1]. 1: https://docs.pydantic.dev/usage/types/#generic-classes-as-types \"\" \" return []","title":"Connection"},{"location":"reference/arti/backends/#descendants_1","text":"arti.backends.memory.MemoryConnection","title":"Descendants"},{"location":"reference/arti/backends/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/backends/#connect_1","text":"def connect ( self ) -> 'Iterator[Self]' Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... View Source @contextmanager def connect ( self ) -> Iterator [ Self ] : \"\"\"Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... \"\"\" yield self","title":"connect"},{"location":"reference/arti/backends/#read_artifact_partitions","text":"def read_artifact_partitions ( self , artifact : 'Artifact' , input_fingerprints : 'InputFingerprints' = {} ) -> 'StoragePartitions' Read all known Partitions for this Storage spec. If input_fingerprints is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless input_fingerprints is provided matching those for a GraphSnapshot. View Source @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \" \"\" Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a GraphSnapshot. \"\" \" raise NotImplementedError ()","title":"read_artifact_partitions"},{"location":"reference/arti/backends/#read_graph","text":"def read_graph ( self , name : 'str' , fingerprint : 'Fingerprint' ) -> 'Graph' Fetch an instance of the named Graph. View Source @abstractmethod def read_graph ( self , name : str , fingerprint : Fingerprint ) -> Graph : \"\"\"Fetch an instance of the named Graph.\"\"\" raise NotImplementedError ()","title":"read_graph"},{"location":"reference/arti/backends/#read_snapshot","text":"def read_snapshot ( self , name : 'str' , fingerprint : 'Fingerprint' ) -> 'GraphSnapshot' Fetch an instance of the named GraphSnapshot. View Source @abstractmethod def read_snapshot ( self , name : str , fingerprint : Fingerprint ) -> GraphSnapshot : \"\"\"Fetch an instance of the named GraphSnapshot.\"\"\" raise NotImplementedError ()","title":"read_snapshot"},{"location":"reference/arti/backends/#read_snapshot_partitions","text":"def read_snapshot_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' ) -> 'StoragePartitions' Read the known Partitions for the named Artifact in a specific GraphSnapshot. View Source @abstractmethod def read_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \"\"\"Read the known Partitions for the named Artifact in a specific GraphSnapshot.\"\"\" raise NotImplementedError ()","title":"read_snapshot_partitions"},{"location":"reference/arti/backends/#read_snapshot_tag","text":"def read_snapshot_tag ( self , name : 'str' , tag : 'str' ) -> 'GraphSnapshot' Fetch the GraphSnapshot for the named tag. View Source @abstractmethod def read_snapshot_tag ( self , name : str , tag : str ) -> GraphSnapshot : \"\"\"Fetch the GraphSnapshot for the named tag.\"\"\" raise NotImplementedError ()","title":"read_snapshot_tag"},{"location":"reference/arti/backends/#write_artifact_and_graph_partitions","text":"def write_artifact_and_graph_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' View Source def write_artifact_and_graph_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_snapshot_partitions ( snapshot , artifact_key , artifact , partitions )","title":"write_artifact_and_graph_partitions"},{"location":"reference/arti/backends/#write_artifact_partitions","text":"def write_artifact_partitions ( self , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' Add more partitions for a Storage spec. View Source @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \"\"\"Add more partitions for a Storage spec.\"\"\" raise NotImplementedError ()","title":"write_artifact_partitions"},{"location":"reference/arti/backends/#write_graph","text":"def write_graph ( self , graph : 'Graph' ) -> 'None' Write the Graph and all linked Artifacts and Producers to the database. View Source @abstractmethod def write_graph ( self , graph : Graph ) -> None : \"\"\"Write the Graph and all linked Artifacts and Producers to the database.\"\"\" raise NotImplementedError ()","title":"write_graph"},{"location":"reference/arti/backends/#write_snapshot","text":"def write_snapshot ( self , snapshot : 'GraphSnapshot' ) -> 'None' Write the GraphSnapshot to the database. View Source @abstractmethod def write_snapshot ( self , snapshot : GraphSnapshot ) -> None : \"\"\"Write the GraphSnapshot to the database.\"\"\" raise NotImplementedError ()","title":"write_snapshot"},{"location":"reference/arti/backends/#write_snapshot_partitions","text":"def write_snapshot_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' Link the Partitions to the named Artifact in a specific GraphSnapshot. View Source @abstractmethod def write_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \"\"\"Link the Partitions to the named Artifact in a specific GraphSnapshot.\"\"\" raise NotImplementedError ()","title":"write_snapshot_partitions"},{"location":"reference/arti/backends/#write_snapshot_tag","text":"def write_snapshot_tag ( self , snapshot : 'GraphSnapshot' , tag : 'str' , overwrite : 'bool' = False ) -> 'None' Stamp a GraphSnapshot with an arbitrary tag. View Source @abstractmethod def write_snapshot_tag ( self , snapshot : GraphSnapshot , tag : str , overwrite : bool = False ) -> None : \"\"\"Stamp a GraphSnapshot with an arbitrary tag.\"\"\" raise NotImplementedError ()","title":"write_snapshot_tag"},{"location":"reference/arti/backends/memory/","text":"Module arti.backends.memory None None View Source from __future__ import annotations from collections import defaultdict from collections.abc import Iterator from contextlib import contextmanager from functools import partial from pydantic import PrivateAttr from arti import ( Artifact , Backend , Connection , Fingerprint , Graph , GraphSnapshot , InputFingerprints , Storage , StoragePartition , StoragePartitions , ) from arti.internal.utils import NoCopyMixin def _ensure_fingerprinted ( partitions : StoragePartitions ) -> Iterator [ StoragePartition ]: for partition in partitions : yield partition . with_content_fingerprint ( keep_existing = True ) _Graphs = dict [ str , dict [ Fingerprint , Graph ]] _GraphSnapshots = dict [ str , dict [ Fingerprint , GraphSnapshot ]] _GraphSnapshotPartitions = dict [ GraphSnapshot , dict [ str , set [ StoragePartition ]] ] # ...[snapshot][artifact_key] _SnapshotTags = dict [ str , dict [ str , GraphSnapshot ]] # ...[name][tag] _StoragePartitions = dict [ Storage [ StoragePartition ], set [ StoragePartition ]] class _NoCopyContainer ( NoCopyMixin ): \"\"\"Container for MemoryBackend data that bypasses (deep)copying. The MemoryBackend is *intended* to be stateful, like a connection to an external database in other backends. However, we usually prefer immutable data structures and Pydantic models, which (deep)copy often. If we were to (deep)copy these data structures, then we wouldn't be able to track metadata between steps. Instead, this container holds the state and skips (deep)copying. We may also add threading locks around access (with some slight usage changes). \"\"\" def __init__ ( self ) -> None : # NOTE: lambdas are not pickleable, so use partial for any nested defaultdicts. self . graphs : _Graphs = defaultdict ( dict ) self . snapshots : _GraphSnapshots = defaultdict ( dict ) # `container.snapshot_partitions` tracks all the partitions for a *specific* GraphSnapshot. # `container.storage_partitions` tracks all partitions, across all snapshots. This # separation is important to allow for Literals to be used even after a snapshot change. self . snapshot_partitions : _GraphSnapshotPartitions = defaultdict ( partial ( defaultdict , set [ StoragePartition ]) # type: ignore[arg-type] ) self . snapshot_tags : _SnapshotTags = defaultdict ( dict ) self . storage_partitions : _StoragePartitions = defaultdict ( set [ StoragePartition ]) class MemoryConnection ( Connection ): def __init__ ( self , container : _NoCopyContainer ) -> None : self . container = container def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : # The MemoryBackend is (obviously) not persistent, so there may be external data we don't # know about. If we haven't seen this storage before, we'll attempt to \"warm\" the cache. if artifact . storage not in self . container . storage_partitions : self . write_artifact_partitions ( artifact , artifact . storage . discover_partitions ( input_fingerprints ) ) partitions = self . container . storage_partitions [ artifact . storage ] if input_fingerprints : partitions = { partition for partition in partitions if input_fingerprints . get ( partition . keys ) == partition . input_fingerprint } return tuple ( partitions ) def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : self . container . storage_partitions [ artifact . storage ] . update ( _ensure_fingerprinted ( partitions ) ) def read_graph ( self , name : str , fingerprint : Fingerprint ) -> Graph : return self . container . graphs [ name ][ fingerprint ] def write_graph ( self , graph : Graph ) -> None : self . container . graphs [ graph . name ][ graph . fingerprint ] = graph def read_snapshot ( self , name : str , fingerprint : Fingerprint ) -> GraphSnapshot : return self . container . snapshots [ name ][ fingerprint ] def write_snapshot ( self , snapshot : GraphSnapshot ) -> None : self . container . snapshots [ snapshot . name ][ snapshot . fingerprint ] = snapshot def read_snapshot_tag ( self , name : str , tag : str ) -> GraphSnapshot : if tag not in self . container . snapshot_tags [ name ]: raise ValueError ( f \"No known ` { tag } ` tag for GraphSnapshot ` { name } `\" ) return self . container . snapshot_tags [ name ][ tag ] def write_snapshot_tag ( self , snapshot : GraphSnapshot , tag : str , overwrite : bool = False ) -> None : \"\"\"Read the known Partitions for the named Artifact in a specific GraphSnapshot.\"\"\" if ( existing := self . container . snapshot_tags [ snapshot . name ] . get ( tag ) ) is not None and not overwrite : raise ValueError ( f \"Existing ` { tag } ` tag for Graph ` { snapshot . name } ` points to { existing } \" ) self . container . snapshot_tags [ snapshot . name ][ tag ] = snapshot def read_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact ) -> StoragePartitions : return tuple ( self . container . snapshot_partitions [ snapshot ][ artifact_key ]) def write_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . container . snapshot_partitions [ snapshot ][ artifact_key ] . update ( _ensure_fingerprinted ( partitions ) ) class MemoryBackend ( Backend [ MemoryConnection ]): _container : _NoCopyContainer = PrivateAttr ( default_factory = _NoCopyContainer ) @contextmanager def connect ( self ) -> Iterator [ MemoryConnection ]: yield MemoryConnection ( self . _container ) Classes MemoryBackend class MemoryBackend ( __pydantic_self__ , ** data : Any ) View Source class MemoryBackend ( Backend [ MemoryConnection ] ) : _container : _NoCopyContainer = PrivateAttr ( default_factory = _NoCopyContainer ) @contextmanager def connect ( self ) -> Iterator [ MemoryConnection ] : yield MemoryConnection ( self . _container ) Ancestors (in MRO) arti.backends.Backend arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods connect def connect ( self ) -> 'Iterator[MemoryConnection]' View Source @contextmanager def connect ( self ) -> Iterator [ MemoryConnection ] : yield MemoryConnection ( self . _container ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . MemoryConnection class MemoryConnection ( container : '_NoCopyContainer' ) View Source class MemoryConnection ( Connection ) : def __init__ ( self , container : _NoCopyContainer ) -> None : self . container = container def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : # The MemoryBackend is ( obviously ) not persistent , so there may be external data we don 't # know about. If we haven' t seen this storage before , we ' ll attempt to \"warm\" the cache . if artifact . storage not in self . container . storage_partitions : self . write_artifact_partitions ( artifact , artifact . storage . discover_partitions ( input_fingerprints ) ) partitions = self . container . storage_partitions [ artifact.storage ] if input_fingerprints : partitions = { partition for partition in partitions if input_fingerprints . get ( partition . keys ) == partition . input_fingerprint } return tuple ( partitions ) def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : self . container . storage_partitions [ artifact.storage ] . update ( _ensure_fingerprinted ( partitions ) ) def read_graph ( self , name : str , fingerprint : Fingerprint ) -> Graph : return self . container . graphs [ name ][ fingerprint ] def write_graph ( self , graph : Graph ) -> None : self . container . graphs [ graph.name ][ graph.fingerprint ] = graph def read_snapshot ( self , name : str , fingerprint : Fingerprint ) -> GraphSnapshot : return self . container . snapshots [ name ][ fingerprint ] def write_snapshot ( self , snapshot : GraphSnapshot ) -> None : self . container . snapshots [ snapshot.name ][ snapshot.fingerprint ] = snapshot def read_snapshot_tag ( self , name : str , tag : str ) -> GraphSnapshot : if tag not in self . container . snapshot_tags [ name ] : raise ValueError ( f \"No known `{tag}` tag for GraphSnapshot `{name}`\" ) return self . container . snapshot_tags [ name ][ tag ] def write_snapshot_tag ( self , snapshot : GraphSnapshot , tag : str , overwrite : bool = False ) -> None : \"\"\"Read the known Partitions for the named Artifact in a specific GraphSnapshot.\"\"\" if ( existing : = self . container . snapshot_tags [ snapshot.name ] . get ( tag ) ) is not None and not overwrite : raise ValueError ( f \"Existing `{tag}` tag for Graph `{snapshot.name}` points to {existing}\" ) self . container . snapshot_tags [ snapshot.name ][ tag ] = snapshot def read_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact ) -> StoragePartitions : return tuple ( self . container . snapshot_partitions [ snapshot ][ artifact_key ] ) def write_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . container . snapshot_partitions [ snapshot ][ artifact_key ] . update ( _ensure_fingerprinted ( partitions ) ) Ancestors (in MRO) arti.backends.Connection Methods connect def connect ( self ) -> 'Iterator[Self]' Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... View Source @contextmanager def connect ( self ) -> Iterator [ Self ] : \"\"\"Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... \"\"\" yield self read_artifact_partitions def read_artifact_partitions ( self , artifact : 'Artifact' , input_fingerprints : 'InputFingerprints' = {} ) -> 'StoragePartitions' Read all known Partitions for this Storage spec. If input_fingerprints is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless input_fingerprints is provided matching those for a GraphSnapshot. View Source def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : # The MemoryBackend is ( obviously ) not persistent , so there may be external data we don 't # know about. If we haven' t seen this storage before , we 'll attempt to \" warm \" the cache. if artifact.storage not in self.container.storage_partitions: self.write_artifact_partitions( artifact, artifact.storage.discover_partitions(input_fingerprints) ) partitions = self.container.storage_partitions[artifact.storage] if input_fingerprints: partitions = { partition for partition in partitions if input_fingerprints.get(partition.keys) == partition.input_fingerprint } return tuple(partitions) read_graph def read_graph ( self , name : 'str' , fingerprint : 'Fingerprint' ) -> 'Graph' Fetch an instance of the named Graph. View Source def read_graph ( self , name : str , fingerprint : Fingerprint ) -> Graph : return self . container . graphs [ name ][ fingerprint ] read_snapshot def read_snapshot ( self , name : 'str' , fingerprint : 'Fingerprint' ) -> 'GraphSnapshot' Fetch an instance of the named GraphSnapshot. View Source def read_snapshot ( self , name : str , fingerprint : Fingerprint ) -> GraphSnapshot : return self . container . snapshots [ name ][ fingerprint ] read_snapshot_partitions def read_snapshot_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' ) -> 'StoragePartitions' Read the known Partitions for the named Artifact in a specific GraphSnapshot. View Source def read_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact ) -> StoragePartitions : return tuple ( self . container . snapshot_partitions [ snapshot ][ artifact_key ] ) read_snapshot_tag def read_snapshot_tag ( self , name : 'str' , tag : 'str' ) -> 'GraphSnapshot' Fetch the GraphSnapshot for the named tag. View Source def read_snapshot_tag ( self , name : str , tag : str ) -> GraphSnapshot : if tag not in self . container . snapshot_tags [ name ] : raise ValueError ( f \"No known `{tag}` tag for GraphSnapshot `{name}`\" ) return self . container . snapshot_tags [ name ][ tag ] write_artifact_and_graph_partitions def write_artifact_and_graph_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' View Source def write_artifact_and_graph_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_snapshot_partitions ( snapshot , artifact_key , artifact , partitions ) write_artifact_partitions def write_artifact_partitions ( self , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' Add more partitions for a Storage spec. View Source def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : self . container . storage_partitions [ artifact . storage ]. update ( _ensure_fingerprinted ( partitions ) ) write_graph def write_graph ( self , graph : 'Graph' ) -> 'None' Write the Graph and all linked Artifacts and Producers to the database. View Source def write_graph ( self , graph : Graph ) -> None : self . container . graphs [ graph . name ][ graph . fingerprint ] = graph write_snapshot def write_snapshot ( self , snapshot : 'GraphSnapshot' ) -> 'None' Write the GraphSnapshot to the database. View Source def write_snapshot ( self , snapshot : GraphSnapshot ) -> None : self . container . snapshots [ snapshot . name ][ snapshot . fingerprint ] = snapshot write_snapshot_partitions def write_snapshot_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' Link the Partitions to the named Artifact in a specific GraphSnapshot. View Source def write_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . container . snapshot_partitions [ snapshot ][ artifact_key ] . update ( _ensure_fingerprinted ( partitions ) ) write_snapshot_tag def write_snapshot_tag ( self , snapshot : 'GraphSnapshot' , tag : 'str' , overwrite : 'bool' = False ) -> 'None' Read the known Partitions for the named Artifact in a specific GraphSnapshot. View Source def write_snapshot_tag ( self , snapshot : GraphSnapshot , tag : str , overwrite : bool = False ) -> None : \"\"\"Read the known Partitions for the named Artifact in a specific GraphSnapshot.\"\"\" if ( existing : = self . container . snapshot_tags [ snapshot.name ] . get ( tag ) ) is not None and not overwrite : raise ValueError ( f \"Existing `{tag}` tag for Graph `{snapshot.name}` points to {existing}\" ) self . container . snapshot_tags [ snapshot.name ][ tag ] = snapshot","title":"Memory"},{"location":"reference/arti/backends/memory/#module-artibackendsmemory","text":"None None View Source from __future__ import annotations from collections import defaultdict from collections.abc import Iterator from contextlib import contextmanager from functools import partial from pydantic import PrivateAttr from arti import ( Artifact , Backend , Connection , Fingerprint , Graph , GraphSnapshot , InputFingerprints , Storage , StoragePartition , StoragePartitions , ) from arti.internal.utils import NoCopyMixin def _ensure_fingerprinted ( partitions : StoragePartitions ) -> Iterator [ StoragePartition ]: for partition in partitions : yield partition . with_content_fingerprint ( keep_existing = True ) _Graphs = dict [ str , dict [ Fingerprint , Graph ]] _GraphSnapshots = dict [ str , dict [ Fingerprint , GraphSnapshot ]] _GraphSnapshotPartitions = dict [ GraphSnapshot , dict [ str , set [ StoragePartition ]] ] # ...[snapshot][artifact_key] _SnapshotTags = dict [ str , dict [ str , GraphSnapshot ]] # ...[name][tag] _StoragePartitions = dict [ Storage [ StoragePartition ], set [ StoragePartition ]] class _NoCopyContainer ( NoCopyMixin ): \"\"\"Container for MemoryBackend data that bypasses (deep)copying. The MemoryBackend is *intended* to be stateful, like a connection to an external database in other backends. However, we usually prefer immutable data structures and Pydantic models, which (deep)copy often. If we were to (deep)copy these data structures, then we wouldn't be able to track metadata between steps. Instead, this container holds the state and skips (deep)copying. We may also add threading locks around access (with some slight usage changes). \"\"\" def __init__ ( self ) -> None : # NOTE: lambdas are not pickleable, so use partial for any nested defaultdicts. self . graphs : _Graphs = defaultdict ( dict ) self . snapshots : _GraphSnapshots = defaultdict ( dict ) # `container.snapshot_partitions` tracks all the partitions for a *specific* GraphSnapshot. # `container.storage_partitions` tracks all partitions, across all snapshots. This # separation is important to allow for Literals to be used even after a snapshot change. self . snapshot_partitions : _GraphSnapshotPartitions = defaultdict ( partial ( defaultdict , set [ StoragePartition ]) # type: ignore[arg-type] ) self . snapshot_tags : _SnapshotTags = defaultdict ( dict ) self . storage_partitions : _StoragePartitions = defaultdict ( set [ StoragePartition ]) class MemoryConnection ( Connection ): def __init__ ( self , container : _NoCopyContainer ) -> None : self . container = container def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : # The MemoryBackend is (obviously) not persistent, so there may be external data we don't # know about. If we haven't seen this storage before, we'll attempt to \"warm\" the cache. if artifact . storage not in self . container . storage_partitions : self . write_artifact_partitions ( artifact , artifact . storage . discover_partitions ( input_fingerprints ) ) partitions = self . container . storage_partitions [ artifact . storage ] if input_fingerprints : partitions = { partition for partition in partitions if input_fingerprints . get ( partition . keys ) == partition . input_fingerprint } return tuple ( partitions ) def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : self . container . storage_partitions [ artifact . storage ] . update ( _ensure_fingerprinted ( partitions ) ) def read_graph ( self , name : str , fingerprint : Fingerprint ) -> Graph : return self . container . graphs [ name ][ fingerprint ] def write_graph ( self , graph : Graph ) -> None : self . container . graphs [ graph . name ][ graph . fingerprint ] = graph def read_snapshot ( self , name : str , fingerprint : Fingerprint ) -> GraphSnapshot : return self . container . snapshots [ name ][ fingerprint ] def write_snapshot ( self , snapshot : GraphSnapshot ) -> None : self . container . snapshots [ snapshot . name ][ snapshot . fingerprint ] = snapshot def read_snapshot_tag ( self , name : str , tag : str ) -> GraphSnapshot : if tag not in self . container . snapshot_tags [ name ]: raise ValueError ( f \"No known ` { tag } ` tag for GraphSnapshot ` { name } `\" ) return self . container . snapshot_tags [ name ][ tag ] def write_snapshot_tag ( self , snapshot : GraphSnapshot , tag : str , overwrite : bool = False ) -> None : \"\"\"Read the known Partitions for the named Artifact in a specific GraphSnapshot.\"\"\" if ( existing := self . container . snapshot_tags [ snapshot . name ] . get ( tag ) ) is not None and not overwrite : raise ValueError ( f \"Existing ` { tag } ` tag for Graph ` { snapshot . name } ` points to { existing } \" ) self . container . snapshot_tags [ snapshot . name ][ tag ] = snapshot def read_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact ) -> StoragePartitions : return tuple ( self . container . snapshot_partitions [ snapshot ][ artifact_key ]) def write_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . container . snapshot_partitions [ snapshot ][ artifact_key ] . update ( _ensure_fingerprinted ( partitions ) ) class MemoryBackend ( Backend [ MemoryConnection ]): _container : _NoCopyContainer = PrivateAttr ( default_factory = _NoCopyContainer ) @contextmanager def connect ( self ) -> Iterator [ MemoryConnection ]: yield MemoryConnection ( self . _container )","title":"Module arti.backends.memory"},{"location":"reference/arti/backends/memory/#classes","text":"","title":"Classes"},{"location":"reference/arti/backends/memory/#memorybackend","text":"class MemoryBackend ( __pydantic_self__ , ** data : Any ) View Source class MemoryBackend ( Backend [ MemoryConnection ] ) : _container : _NoCopyContainer = PrivateAttr ( default_factory = _NoCopyContainer ) @contextmanager def connect ( self ) -> Iterator [ MemoryConnection ] : yield MemoryConnection ( self . _container )","title":"MemoryBackend"},{"location":"reference/arti/backends/memory/#ancestors-in-mro","text":"arti.backends.Backend arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/arti/backends/memory/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/backends/memory/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/backends/memory/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/backends/memory/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/backends/memory/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/backends/memory/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/backends/memory/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/backends/memory/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/backends/memory/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/backends/memory/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/backends/memory/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/backends/memory/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/backends/memory/#methods","text":"","title":"Methods"},{"location":"reference/arti/backends/memory/#connect","text":"def connect ( self ) -> 'Iterator[MemoryConnection]' View Source @contextmanager def connect ( self ) -> Iterator [ MemoryConnection ] : yield MemoryConnection ( self . _container )","title":"connect"},{"location":"reference/arti/backends/memory/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/backends/memory/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/backends/memory/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/backends/memory/#memoryconnection","text":"class MemoryConnection ( container : '_NoCopyContainer' ) View Source class MemoryConnection ( Connection ) : def __init__ ( self , container : _NoCopyContainer ) -> None : self . container = container def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : # The MemoryBackend is ( obviously ) not persistent , so there may be external data we don 't # know about. If we haven' t seen this storage before , we ' ll attempt to \"warm\" the cache . if artifact . storage not in self . container . storage_partitions : self . write_artifact_partitions ( artifact , artifact . storage . discover_partitions ( input_fingerprints ) ) partitions = self . container . storage_partitions [ artifact.storage ] if input_fingerprints : partitions = { partition for partition in partitions if input_fingerprints . get ( partition . keys ) == partition . input_fingerprint } return tuple ( partitions ) def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : self . container . storage_partitions [ artifact.storage ] . update ( _ensure_fingerprinted ( partitions ) ) def read_graph ( self , name : str , fingerprint : Fingerprint ) -> Graph : return self . container . graphs [ name ][ fingerprint ] def write_graph ( self , graph : Graph ) -> None : self . container . graphs [ graph.name ][ graph.fingerprint ] = graph def read_snapshot ( self , name : str , fingerprint : Fingerprint ) -> GraphSnapshot : return self . container . snapshots [ name ][ fingerprint ] def write_snapshot ( self , snapshot : GraphSnapshot ) -> None : self . container . snapshots [ snapshot.name ][ snapshot.fingerprint ] = snapshot def read_snapshot_tag ( self , name : str , tag : str ) -> GraphSnapshot : if tag not in self . container . snapshot_tags [ name ] : raise ValueError ( f \"No known `{tag}` tag for GraphSnapshot `{name}`\" ) return self . container . snapshot_tags [ name ][ tag ] def write_snapshot_tag ( self , snapshot : GraphSnapshot , tag : str , overwrite : bool = False ) -> None : \"\"\"Read the known Partitions for the named Artifact in a specific GraphSnapshot.\"\"\" if ( existing : = self . container . snapshot_tags [ snapshot.name ] . get ( tag ) ) is not None and not overwrite : raise ValueError ( f \"Existing `{tag}` tag for Graph `{snapshot.name}` points to {existing}\" ) self . container . snapshot_tags [ snapshot.name ][ tag ] = snapshot def read_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact ) -> StoragePartitions : return tuple ( self . container . snapshot_partitions [ snapshot ][ artifact_key ] ) def write_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . container . snapshot_partitions [ snapshot ][ artifact_key ] . update ( _ensure_fingerprinted ( partitions ) )","title":"MemoryConnection"},{"location":"reference/arti/backends/memory/#ancestors-in-mro_1","text":"arti.backends.Connection","title":"Ancestors (in MRO)"},{"location":"reference/arti/backends/memory/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/backends/memory/#connect_1","text":"def connect ( self ) -> 'Iterator[Self]' Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... View Source @contextmanager def connect ( self ) -> Iterator [ Self ] : \"\"\"Return self This makes it easier to work with an Optional connection, eg: with (connection or backend).connect() as conn: ... \"\"\" yield self","title":"connect"},{"location":"reference/arti/backends/memory/#read_artifact_partitions","text":"def read_artifact_partitions ( self , artifact : 'Artifact' , input_fingerprints : 'InputFingerprints' = {} ) -> 'StoragePartitions' Read all known Partitions for this Storage spec. If input_fingerprints is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless input_fingerprints is provided matching those for a GraphSnapshot. View Source def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : # The MemoryBackend is ( obviously ) not persistent , so there may be external data we don 't # know about. If we haven' t seen this storage before , we 'll attempt to \" warm \" the cache. if artifact.storage not in self.container.storage_partitions: self.write_artifact_partitions( artifact, artifact.storage.discover_partitions(input_fingerprints) ) partitions = self.container.storage_partitions[artifact.storage] if input_fingerprints: partitions = { partition for partition in partitions if input_fingerprints.get(partition.keys) == partition.input_fingerprint } return tuple(partitions)","title":"read_artifact_partitions"},{"location":"reference/arti/backends/memory/#read_graph","text":"def read_graph ( self , name : 'str' , fingerprint : 'Fingerprint' ) -> 'Graph' Fetch an instance of the named Graph. View Source def read_graph ( self , name : str , fingerprint : Fingerprint ) -> Graph : return self . container . graphs [ name ][ fingerprint ]","title":"read_graph"},{"location":"reference/arti/backends/memory/#read_snapshot","text":"def read_snapshot ( self , name : 'str' , fingerprint : 'Fingerprint' ) -> 'GraphSnapshot' Fetch an instance of the named GraphSnapshot. View Source def read_snapshot ( self , name : str , fingerprint : Fingerprint ) -> GraphSnapshot : return self . container . snapshots [ name ][ fingerprint ]","title":"read_snapshot"},{"location":"reference/arti/backends/memory/#read_snapshot_partitions","text":"def read_snapshot_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' ) -> 'StoragePartitions' Read the known Partitions for the named Artifact in a specific GraphSnapshot. View Source def read_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact ) -> StoragePartitions : return tuple ( self . container . snapshot_partitions [ snapshot ][ artifact_key ] )","title":"read_snapshot_partitions"},{"location":"reference/arti/backends/memory/#read_snapshot_tag","text":"def read_snapshot_tag ( self , name : 'str' , tag : 'str' ) -> 'GraphSnapshot' Fetch the GraphSnapshot for the named tag. View Source def read_snapshot_tag ( self , name : str , tag : str ) -> GraphSnapshot : if tag not in self . container . snapshot_tags [ name ] : raise ValueError ( f \"No known `{tag}` tag for GraphSnapshot `{name}`\" ) return self . container . snapshot_tags [ name ][ tag ]","title":"read_snapshot_tag"},{"location":"reference/arti/backends/memory/#write_artifact_and_graph_partitions","text":"def write_artifact_and_graph_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' View Source def write_artifact_and_graph_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_snapshot_partitions ( snapshot , artifact_key , artifact , partitions )","title":"write_artifact_and_graph_partitions"},{"location":"reference/arti/backends/memory/#write_artifact_partitions","text":"def write_artifact_partitions ( self , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' Add more partitions for a Storage spec. View Source def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : self . container . storage_partitions [ artifact . storage ]. update ( _ensure_fingerprinted ( partitions ) )","title":"write_artifact_partitions"},{"location":"reference/arti/backends/memory/#write_graph","text":"def write_graph ( self , graph : 'Graph' ) -> 'None' Write the Graph and all linked Artifacts and Producers to the database. View Source def write_graph ( self , graph : Graph ) -> None : self . container . graphs [ graph . name ][ graph . fingerprint ] = graph","title":"write_graph"},{"location":"reference/arti/backends/memory/#write_snapshot","text":"def write_snapshot ( self , snapshot : 'GraphSnapshot' ) -> 'None' Write the GraphSnapshot to the database. View Source def write_snapshot ( self , snapshot : GraphSnapshot ) -> None : self . container . snapshots [ snapshot . name ][ snapshot . fingerprint ] = snapshot","title":"write_snapshot"},{"location":"reference/arti/backends/memory/#write_snapshot_partitions","text":"def write_snapshot_partitions ( self , snapshot : 'GraphSnapshot' , artifact_key : 'str' , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' Link the Partitions to the named Artifact in a specific GraphSnapshot. View Source def write_snapshot_partitions ( self , snapshot : GraphSnapshot , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . container . snapshot_partitions [ snapshot ][ artifact_key ] . update ( _ensure_fingerprinted ( partitions ) )","title":"write_snapshot_partitions"},{"location":"reference/arti/backends/memory/#write_snapshot_tag","text":"def write_snapshot_tag ( self , snapshot : 'GraphSnapshot' , tag : 'str' , overwrite : 'bool' = False ) -> 'None' Read the known Partitions for the named Artifact in a specific GraphSnapshot. View Source def write_snapshot_tag ( self , snapshot : GraphSnapshot , tag : str , overwrite : bool = False ) -> None : \"\"\"Read the known Partitions for the named Artifact in a specific GraphSnapshot.\"\"\" if ( existing : = self . container . snapshot_tags [ snapshot.name ] . get ( tag ) ) is not None and not overwrite : raise ValueError ( f \"Existing `{tag}` tag for Graph `{snapshot.name}` points to {existing}\" ) self . container . snapshot_tags [ snapshot.name ][ tag ] = snapshot","title":"write_snapshot_tag"},{"location":"reference/arti/executors/","text":"Module arti.executors None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import abc import logging from itertools import chain from arti.backends import Connection from arti.fingerprints import Fingerprint from arti.graphs import GraphSnapshot from arti.internal.models import Model from arti.internal.utils import frozendict from arti.partitions import CompositeKey , InputFingerprints from arti.producers import InputPartitions , Producer from arti.storage import StoragePartitions class Executor ( Model ): @abc . abstractmethod def build ( self , snapshot : GraphSnapshot ) -> None : raise NotImplementedError () def get_producer_inputs ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer ) -> InputPartitions : return InputPartitions ( { name : connection . read_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ], artifact ) for name , artifact in producer . inputs . items () } ) def discover_producer_partitions ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , partition_input_fingerprints : InputFingerprints , ) -> set [ CompositeKey ]: # NOTE: The output partitions may be built, but not yet associated with this GraphSnapshot # (eg: raw input data changed, but no changes trickled into this specific Producer). Hence # we'll fetch all StoragePartitions for each Storage, filtered to the PKs and # input_fingerprints we've computed *are* for this snapshot - and then link them to the # snapshot. existing_output_partitions = { output : connection . read_artifact_partitions ( output , partition_input_fingerprints ) for output in snapshot . graph . producer_outputs [ producer ] } for artifact , partitions in existing_output_partitions . items (): connection . write_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ], artifact , partitions ) # TODO: Guarantee all outputs have the same set of identified partitions. Currently, this # pretends a partition is built for all outputs if _any_ are built for that partition. return { partition . keys for partition in chain . from_iterable ( existing_output_partitions . values ()) } def build_producer_partition ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , existing_partition_keys : set [ CompositeKey ], input_fingerprint : Fingerprint , partition_dependencies : frozendict [ str , StoragePartitions ], partition_key : CompositeKey , ) -> None : # TODO: Should this \"skip if exists\" live here or higher up? if partition_key in existing_partition_keys : pk_str = f \" for: { dict ( partition_key ) } \" if partition_key else \".\" logging . info ( f \"Skipping existing { type ( producer ) . __name__ } output { pk_str } \" ) return logging . info ( f \"Building { producer } output for { partition_key } ...\" ) # TODO: Catch DispatchError and give a nicer error... maybe add this to our # @dispatch wrapper (eg: msg arg, or even fn that returns the message to # raise). arguments = { name : snapshot . read ( artifact = producer . inputs [ name ], storage_partitions = partition_dependencies [ name ], view = view , ) for name , view in producer . _build_inputs_ . items () } outputs = producer . build ( ** arguments ) if len ( producer . _outputs_ ) == 1 : outputs = ( outputs ,) validation_passed , validation_message = producer . validate_outputs ( * outputs ) if not validation_passed : raise ValueError ( validation_message ) for i , output in enumerate ( outputs ): snapshot . write ( output , artifact = snapshot . graph . producer_outputs [ producer ][ i ], input_fingerprint = input_fingerprint , keys = partition_key , view = producer . _outputs_ [ i ], ) Sub-modules arti.executors.local Classes Executor class Executor ( __pydantic_self__ , ** data : Any ) View Source class Executor ( Model ) : @abc . abstractmethod def build ( self , snapshot : GraphSnapshot ) -> None : raise NotImplementedError () def get_producer_inputs ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer ) -> InputPartitions : return InputPartitions ( { name : connection . read_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact ) for name , artifact in producer . inputs . items () } ) def discover_producer_partitions ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , partition_input_fingerprints : InputFingerprints , ) -> set [ CompositeKey ] : # NOTE : The output partitions may be built , but not yet associated with this GraphSnapshot # ( eg : raw input data changed , but no changes trickled into this specific Producer ). Hence # we 'll fetch all StoragePartitions for each Storage, filtered to the PKs and # input_fingerprints we' ve computed * are * for this snapshot - and then link them to the # snapshot . existing_output_partitions = { output : connection . read_artifact_partitions ( output , partition_input_fingerprints ) for output in snapshot . graph . producer_outputs [ producer ] } for artifact , partitions in existing_output_partitions . items () : connection . write_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact , partitions ) # TODO : Guarantee all outputs have the same set of identified partitions . Currently , this # pretends a partition is built for all outputs if _any_ are built for that partition . return { partition . keys for partition in chain . from_iterable ( existing_output_partitions . values ()) } def build_producer_partition ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , existing_partition_keys : set [ CompositeKey ] , input_fingerprint : Fingerprint , partition_dependencies : frozendict [ str, StoragePartitions ] , partition_key : CompositeKey , ) -> None : # TODO : Should this \"skip if exists\" live here or higher up ? if partition_key in existing_partition_keys : pk_str = f \" for: {dict(partition_key)}\" if partition_key else \".\" logging . info ( f \"Skipping existing {type(producer).__name__} output{pk_str}\" ) return logging . info ( f \"Building {producer} output for {partition_key}...\" ) # TODO : Catch DispatchError and give a nicer error ... maybe add this to our # @dispatch wrapper ( eg : msg arg , or even fn that returns the message to # raise ). arguments = { name : snapshot . read ( artifact = producer . inputs [ name ] , storage_partitions = partition_dependencies [ name ] , view = view , ) for name , view in producer . _build_inputs_ . items () } outputs = producer . build ( ** arguments ) if len ( producer . _outputs_ ) == 1 : outputs = ( outputs ,) validation_passed , validation_message = producer . validate_outputs ( * outputs ) if not validation_passed : raise ValueError ( validation_message ) for i , output in enumerate ( outputs ) : snapshot . write ( output , artifact = snapshot . graph . producer_outputs [ producer ][ i ] , input_fingerprint = input_fingerprint , keys = partition_key , view = producer . _outputs_ [ i ] , ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.executors.local.LocalExecutor Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods build def build ( self , snapshot : 'GraphSnapshot' ) -> 'None' View Source @abc . abstractmethod def build ( self , snapshot : GraphSnapshot ) -> None : raise NotImplementedError () build_producer_partition def build_producer_partition ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' , * , existing_partition_keys : 'set[CompositeKey]' , input_fingerprint : 'Fingerprint' , partition_dependencies : 'frozendict[str, StoragePartitions]' , partition_key : 'CompositeKey' ) -> 'None' View Source def build_producer_partition ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , existing_partition_keys : set [ CompositeKey ] , input_fingerprint : Fingerprint , partition_dependencies : frozendict [ str, StoragePartitions ] , partition_key : CompositeKey , ) -> None : # TODO : Should this \"skip if exists\" live here or higher up ? if partition_key in existing_partition_keys : pk_str = f \" for: {dict(partition_key)}\" if partition_key else \".\" logging . info ( f \"Skipping existing {type(producer).__name__} output{pk_str}\" ) return logging . info ( f \"Building {producer} output for {partition_key}...\" ) # TODO : Catch DispatchError and give a nicer error ... maybe add this to our # @dispatch wrapper ( eg : msg arg , or even fn that returns the message to # raise ). arguments = { name : snapshot . read ( artifact = producer . inputs [ name ] , storage_partitions = partition_dependencies [ name ] , view = view , ) for name , view in producer . _build_inputs_ . items () } outputs = producer . build ( ** arguments ) if len ( producer . _outputs_ ) == 1 : outputs = ( outputs ,) validation_passed , validation_message = producer . validate_outputs ( * outputs ) if not validation_passed : raise ValueError ( validation_message ) for i , output in enumerate ( outputs ) : snapshot . write ( output , artifact = snapshot . graph . producer_outputs [ producer ][ i ] , input_fingerprint = input_fingerprint , keys = partition_key , view = producer . _outputs_ [ i ] , ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. discover_producer_partitions def discover_producer_partitions ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' , * , partition_input_fingerprints : 'InputFingerprints' ) -> 'set[CompositeKey]' View Source def discover_producer_partitions ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , partition_input_fingerprints : InputFingerprints , ) -> set [ CompositeKey ] : # NOTE : The output partitions may be built , but not yet associated with this GraphSnapshot # ( eg : raw input data changed , but no changes trickled into this specific Producer ). Hence # we 'll fetch all StoragePartitions for each Storage, filtered to the PKs and # input_fingerprints we' ve computed * are * for this snapshot - and then link them to the # snapshot . existing_output_partitions = { output : connection . read_artifact_partitions ( output , partition_input_fingerprints ) for output in snapshot . graph . producer_outputs [ producer ] } for artifact , partitions in existing_output_partitions . items () : connection . write_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact , partitions ) # TODO : Guarantee all outputs have the same set of identified partitions . Currently , this # pretends a partition is built for all outputs if _any_ are built for that partition . return { partition . keys for partition in chain . from_iterable ( existing_output_partitions . values ()) } get_producer_inputs def get_producer_inputs ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' ) -> 'InputPartitions' View Source def get_producer_inputs ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer ) -> InputPartitions : return InputPartitions ( { name : connection . read_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact ) for name , artifact in producer . inputs . items () } ) json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Index"},{"location":"reference/arti/executors/#module-artiexecutors","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import abc import logging from itertools import chain from arti.backends import Connection from arti.fingerprints import Fingerprint from arti.graphs import GraphSnapshot from arti.internal.models import Model from arti.internal.utils import frozendict from arti.partitions import CompositeKey , InputFingerprints from arti.producers import InputPartitions , Producer from arti.storage import StoragePartitions class Executor ( Model ): @abc . abstractmethod def build ( self , snapshot : GraphSnapshot ) -> None : raise NotImplementedError () def get_producer_inputs ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer ) -> InputPartitions : return InputPartitions ( { name : connection . read_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ], artifact ) for name , artifact in producer . inputs . items () } ) def discover_producer_partitions ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , partition_input_fingerprints : InputFingerprints , ) -> set [ CompositeKey ]: # NOTE: The output partitions may be built, but not yet associated with this GraphSnapshot # (eg: raw input data changed, but no changes trickled into this specific Producer). Hence # we'll fetch all StoragePartitions for each Storage, filtered to the PKs and # input_fingerprints we've computed *are* for this snapshot - and then link them to the # snapshot. existing_output_partitions = { output : connection . read_artifact_partitions ( output , partition_input_fingerprints ) for output in snapshot . graph . producer_outputs [ producer ] } for artifact , partitions in existing_output_partitions . items (): connection . write_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ], artifact , partitions ) # TODO: Guarantee all outputs have the same set of identified partitions. Currently, this # pretends a partition is built for all outputs if _any_ are built for that partition. return { partition . keys for partition in chain . from_iterable ( existing_output_partitions . values ()) } def build_producer_partition ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , existing_partition_keys : set [ CompositeKey ], input_fingerprint : Fingerprint , partition_dependencies : frozendict [ str , StoragePartitions ], partition_key : CompositeKey , ) -> None : # TODO: Should this \"skip if exists\" live here or higher up? if partition_key in existing_partition_keys : pk_str = f \" for: { dict ( partition_key ) } \" if partition_key else \".\" logging . info ( f \"Skipping existing { type ( producer ) . __name__ } output { pk_str } \" ) return logging . info ( f \"Building { producer } output for { partition_key } ...\" ) # TODO: Catch DispatchError and give a nicer error... maybe add this to our # @dispatch wrapper (eg: msg arg, or even fn that returns the message to # raise). arguments = { name : snapshot . read ( artifact = producer . inputs [ name ], storage_partitions = partition_dependencies [ name ], view = view , ) for name , view in producer . _build_inputs_ . items () } outputs = producer . build ( ** arguments ) if len ( producer . _outputs_ ) == 1 : outputs = ( outputs ,) validation_passed , validation_message = producer . validate_outputs ( * outputs ) if not validation_passed : raise ValueError ( validation_message ) for i , output in enumerate ( outputs ): snapshot . write ( output , artifact = snapshot . graph . producer_outputs [ producer ][ i ], input_fingerprint = input_fingerprint , keys = partition_key , view = producer . _outputs_ [ i ], )","title":"Module arti.executors"},{"location":"reference/arti/executors/#sub-modules","text":"arti.executors.local","title":"Sub-modules"},{"location":"reference/arti/executors/#classes","text":"","title":"Classes"},{"location":"reference/arti/executors/#executor","text":"class Executor ( __pydantic_self__ , ** data : Any ) View Source class Executor ( Model ) : @abc . abstractmethod def build ( self , snapshot : GraphSnapshot ) -> None : raise NotImplementedError () def get_producer_inputs ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer ) -> InputPartitions : return InputPartitions ( { name : connection . read_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact ) for name , artifact in producer . inputs . items () } ) def discover_producer_partitions ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , partition_input_fingerprints : InputFingerprints , ) -> set [ CompositeKey ] : # NOTE : The output partitions may be built , but not yet associated with this GraphSnapshot # ( eg : raw input data changed , but no changes trickled into this specific Producer ). Hence # we 'll fetch all StoragePartitions for each Storage, filtered to the PKs and # input_fingerprints we' ve computed * are * for this snapshot - and then link them to the # snapshot . existing_output_partitions = { output : connection . read_artifact_partitions ( output , partition_input_fingerprints ) for output in snapshot . graph . producer_outputs [ producer ] } for artifact , partitions in existing_output_partitions . items () : connection . write_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact , partitions ) # TODO : Guarantee all outputs have the same set of identified partitions . Currently , this # pretends a partition is built for all outputs if _any_ are built for that partition . return { partition . keys for partition in chain . from_iterable ( existing_output_partitions . values ()) } def build_producer_partition ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , existing_partition_keys : set [ CompositeKey ] , input_fingerprint : Fingerprint , partition_dependencies : frozendict [ str, StoragePartitions ] , partition_key : CompositeKey , ) -> None : # TODO : Should this \"skip if exists\" live here or higher up ? if partition_key in existing_partition_keys : pk_str = f \" for: {dict(partition_key)}\" if partition_key else \".\" logging . info ( f \"Skipping existing {type(producer).__name__} output{pk_str}\" ) return logging . info ( f \"Building {producer} output for {partition_key}...\" ) # TODO : Catch DispatchError and give a nicer error ... maybe add this to our # @dispatch wrapper ( eg : msg arg , or even fn that returns the message to # raise ). arguments = { name : snapshot . read ( artifact = producer . inputs [ name ] , storage_partitions = partition_dependencies [ name ] , view = view , ) for name , view in producer . _build_inputs_ . items () } outputs = producer . build ( ** arguments ) if len ( producer . _outputs_ ) == 1 : outputs = ( outputs ,) validation_passed , validation_message = producer . validate_outputs ( * outputs ) if not validation_passed : raise ValueError ( validation_message ) for i , output in enumerate ( outputs ) : snapshot . write ( output , artifact = snapshot . graph . producer_outputs [ producer ][ i ] , input_fingerprint = input_fingerprint , keys = partition_key , view = producer . _outputs_ [ i ] , )","title":"Executor"},{"location":"reference/arti/executors/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/executors/#descendants","text":"arti.executors.local.LocalExecutor","title":"Descendants"},{"location":"reference/arti/executors/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/executors/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/executors/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/executors/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/executors/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/executors/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/executors/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/executors/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/executors/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/executors/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/executors/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/executors/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/executors/#methods","text":"","title":"Methods"},{"location":"reference/arti/executors/#build","text":"def build ( self , snapshot : 'GraphSnapshot' ) -> 'None' View Source @abc . abstractmethod def build ( self , snapshot : GraphSnapshot ) -> None : raise NotImplementedError ()","title":"build"},{"location":"reference/arti/executors/#build_producer_partition","text":"def build_producer_partition ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' , * , existing_partition_keys : 'set[CompositeKey]' , input_fingerprint : 'Fingerprint' , partition_dependencies : 'frozendict[str, StoragePartitions]' , partition_key : 'CompositeKey' ) -> 'None' View Source def build_producer_partition ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , existing_partition_keys : set [ CompositeKey ] , input_fingerprint : Fingerprint , partition_dependencies : frozendict [ str, StoragePartitions ] , partition_key : CompositeKey , ) -> None : # TODO : Should this \"skip if exists\" live here or higher up ? if partition_key in existing_partition_keys : pk_str = f \" for: {dict(partition_key)}\" if partition_key else \".\" logging . info ( f \"Skipping existing {type(producer).__name__} output{pk_str}\" ) return logging . info ( f \"Building {producer} output for {partition_key}...\" ) # TODO : Catch DispatchError and give a nicer error ... maybe add this to our # @dispatch wrapper ( eg : msg arg , or even fn that returns the message to # raise ). arguments = { name : snapshot . read ( artifact = producer . inputs [ name ] , storage_partitions = partition_dependencies [ name ] , view = view , ) for name , view in producer . _build_inputs_ . items () } outputs = producer . build ( ** arguments ) if len ( producer . _outputs_ ) == 1 : outputs = ( outputs ,) validation_passed , validation_message = producer . validate_outputs ( * outputs ) if not validation_passed : raise ValueError ( validation_message ) for i , output in enumerate ( outputs ) : snapshot . write ( output , artifact = snapshot . graph . producer_outputs [ producer ][ i ] , input_fingerprint = input_fingerprint , keys = partition_key , view = producer . _outputs_ [ i ] , )","title":"build_producer_partition"},{"location":"reference/arti/executors/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/executors/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/executors/#discover_producer_partitions","text":"def discover_producer_partitions ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' , * , partition_input_fingerprints : 'InputFingerprints' ) -> 'set[CompositeKey]' View Source def discover_producer_partitions ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , partition_input_fingerprints : InputFingerprints , ) -> set [ CompositeKey ] : # NOTE : The output partitions may be built , but not yet associated with this GraphSnapshot # ( eg : raw input data changed , but no changes trickled into this specific Producer ). Hence # we 'll fetch all StoragePartitions for each Storage, filtered to the PKs and # input_fingerprints we' ve computed * are * for this snapshot - and then link them to the # snapshot . existing_output_partitions = { output : connection . read_artifact_partitions ( output , partition_input_fingerprints ) for output in snapshot . graph . producer_outputs [ producer ] } for artifact , partitions in existing_output_partitions . items () : connection . write_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact , partitions ) # TODO : Guarantee all outputs have the same set of identified partitions . Currently , this # pretends a partition is built for all outputs if _any_ are built for that partition . return { partition . keys for partition in chain . from_iterable ( existing_output_partitions . values ()) }","title":"discover_producer_partitions"},{"location":"reference/arti/executors/#get_producer_inputs","text":"def get_producer_inputs ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' ) -> 'InputPartitions' View Source def get_producer_inputs ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer ) -> InputPartitions : return InputPartitions ( { name : connection . read_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact ) for name , artifact in producer . inputs . items () } )","title":"get_producer_inputs"},{"location":"reference/arti/executors/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/executors/local/","text":"Module arti.executors.local None None View Source from __future__ import annotations import logging from graphlib import TopologicalSorter from arti.artifacts import Artifact from arti.executors import Executor from arti.graphs import GraphSnapshot from arti.producers import Producer class LocalExecutor ( Executor ): # TODO: Should we separate .map and .build steps so we can: # - add \"sync\" / \"dry run\" sort of things # - parallelize build # # We may still want to repeat the .map phase in the future, if we wanted to support some sort of # iterated or cyclic Producers (eg: first pass output feeds into second run - in that case, # `.map` should describe how to \"converge\" by returning the same outputs as a prior call). def build ( self , snapshot : GraphSnapshot ) -> None : # NOTE: Raw Artifacts will already be discovered and linked in the backend to this snapshot. with snapshot . backend . connect () as backend : for node in TopologicalSorter ( snapshot . graph . dependencies ) . static_order (): if isinstance ( node , Artifact ): # TODO: Compute Statistics (if not already computed for the partition) and check # Thresholds (every time, as they may be changed, dynamic, or overridden). pass elif isinstance ( node , Producer ): logging . info ( f \"Building { node } ...\" ) input_partitions = self . get_producer_inputs ( snapshot , backend , node ) ( partition_dependencies , partition_input_fingerprints , ) = node . compute_dependencies ( input_partitions ) existing_keys = self . discover_producer_partitions ( snapshot , backend , node , partition_input_fingerprints = partition_input_fingerprints , ) for partition_key , dependencies in partition_dependencies . items (): self . build_producer_partition ( snapshot , backend , node , existing_partition_keys = existing_keys , input_fingerprint = partition_input_fingerprints [ partition_key ], partition_dependencies = dependencies , partition_key = partition_key , ) else : raise NotImplementedError () logging . info ( \"Build finished.\" ) Classes LocalExecutor class LocalExecutor ( __pydantic_self__ , ** data : Any ) View Source class LocalExecutor ( Executor ) : # TODO : Should we separate . map and . build steps so we can : # - add \"sync\" / \"dry run\" sort of things # - parallelize build # # We may still want to repeat the . map phase in the future , if we wanted to support some sort of # iterated or cyclic Producers ( eg : first pass output feeds into second run - in that case , # ` . map ` should describe how to \"converge\" by returning the same outputs as a prior call ). def build ( self , snapshot : GraphSnapshot ) -> None : # NOTE : Raw Artifacts will already be discovered and linked in the backend to this snapshot . with snapshot . backend . connect () as backend : for node in TopologicalSorter ( snapshot . graph . dependencies ). static_order () : if isinstance ( node , Artifact ) : # TODO : Compute Statistics ( if not already computed for the partition ) and check # Thresholds ( every time , as they may be changed , dynamic , or overridden ). pass elif isinstance ( node , Producer ) : logging . info ( f \"Building {node}...\" ) input_partitions = self . get_producer_inputs ( snapshot , backend , node ) ( partition_dependencies , partition_input_fingerprints , ) = node . compute_dependencies ( input_partitions ) existing_keys = self . discover_producer_partitions ( snapshot , backend , node , partition_input_fingerprints = partition_input_fingerprints , ) for partition_key , dependencies in partition_dependencies . items () : self . build_producer_partition ( snapshot , backend , node , existing_partition_keys = existing_keys , input_fingerprint = partition_input_fingerprints [ partition_key ] , partition_dependencies = dependencies , partition_key = partition_key , ) else : raise NotImplementedError () logging . info ( \"Build finished.\" ) Ancestors (in MRO) arti.executors.Executor arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods build def build ( self , snapshot : 'GraphSnapshot' ) -> 'None' View Source def build ( self , snapshot : GraphSnapshot ) -> None : # NOTE : Raw Artifacts will already be discovered and linked in the backend to this snapshot . with snapshot . backend . connect () as backend : for node in TopologicalSorter ( snapshot . graph . dependencies ). static_order () : if isinstance ( node , Artifact ) : # TODO : Compute Statistics ( if not already computed for the partition ) and check # Thresholds ( every time , as they may be changed , dynamic , or overridden ). pass elif isinstance ( node , Producer ) : logging . info ( f \"Building {node}...\" ) input_partitions = self . get_producer_inputs ( snapshot , backend , node ) ( partition_dependencies , partition_input_fingerprints , ) = node . compute_dependencies ( input_partitions ) existing_keys = self . discover_producer_partitions ( snapshot , backend , node , partition_input_fingerprints = partition_input_fingerprints , ) for partition_key , dependencies in partition_dependencies . items () : self . build_producer_partition ( snapshot , backend , node , existing_partition_keys = existing_keys , input_fingerprint = partition_input_fingerprints [ partition_key ] , partition_dependencies = dependencies , partition_key = partition_key , ) else : raise NotImplementedError () logging . info ( \"Build finished.\" ) build_producer_partition def build_producer_partition ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' , * , existing_partition_keys : 'set[CompositeKey]' , input_fingerprint : 'Fingerprint' , partition_dependencies : 'frozendict[str, StoragePartitions]' , partition_key : 'CompositeKey' ) -> 'None' View Source def build_producer_partition ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , existing_partition_keys : set [ CompositeKey ] , input_fingerprint : Fingerprint , partition_dependencies : frozendict [ str, StoragePartitions ] , partition_key : CompositeKey , ) -> None : # TODO : Should this \"skip if exists\" live here or higher up ? if partition_key in existing_partition_keys : pk_str = f \" for: {dict(partition_key)}\" if partition_key else \".\" logging . info ( f \"Skipping existing {type(producer).__name__} output{pk_str}\" ) return logging . info ( f \"Building {producer} output for {partition_key}...\" ) # TODO : Catch DispatchError and give a nicer error ... maybe add this to our # @dispatch wrapper ( eg : msg arg , or even fn that returns the message to # raise ). arguments = { name : snapshot . read ( artifact = producer . inputs [ name ] , storage_partitions = partition_dependencies [ name ] , view = view , ) for name , view in producer . _build_inputs_ . items () } outputs = producer . build ( ** arguments ) if len ( producer . _outputs_ ) == 1 : outputs = ( outputs ,) validation_passed , validation_message = producer . validate_outputs ( * outputs ) if not validation_passed : raise ValueError ( validation_message ) for i , output in enumerate ( outputs ) : snapshot . write ( output , artifact = snapshot . graph . producer_outputs [ producer ][ i ] , input_fingerprint = input_fingerprint , keys = partition_key , view = producer . _outputs_ [ i ] , ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. discover_producer_partitions def discover_producer_partitions ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' , * , partition_input_fingerprints : 'InputFingerprints' ) -> 'set[CompositeKey]' View Source def discover_producer_partitions ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , partition_input_fingerprints : InputFingerprints , ) -> set [ CompositeKey ] : # NOTE : The output partitions may be built , but not yet associated with this GraphSnapshot # ( eg : raw input data changed , but no changes trickled into this specific Producer ). Hence # we 'll fetch all StoragePartitions for each Storage, filtered to the PKs and # input_fingerprints we' ve computed * are * for this snapshot - and then link them to the # snapshot . existing_output_partitions = { output : connection . read_artifact_partitions ( output , partition_input_fingerprints ) for output in snapshot . graph . producer_outputs [ producer ] } for artifact , partitions in existing_output_partitions . items () : connection . write_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact , partitions ) # TODO : Guarantee all outputs have the same set of identified partitions . Currently , this # pretends a partition is built for all outputs if _any_ are built for that partition . return { partition . keys for partition in chain . from_iterable ( existing_output_partitions . values ()) } get_producer_inputs def get_producer_inputs ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' ) -> 'InputPartitions' View Source def get_producer_inputs ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer ) -> InputPartitions : return InputPartitions ( { name : connection . read_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact ) for name , artifact in producer . inputs . items () } ) json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Local"},{"location":"reference/arti/executors/local/#module-artiexecutorslocal","text":"None None View Source from __future__ import annotations import logging from graphlib import TopologicalSorter from arti.artifacts import Artifact from arti.executors import Executor from arti.graphs import GraphSnapshot from arti.producers import Producer class LocalExecutor ( Executor ): # TODO: Should we separate .map and .build steps so we can: # - add \"sync\" / \"dry run\" sort of things # - parallelize build # # We may still want to repeat the .map phase in the future, if we wanted to support some sort of # iterated or cyclic Producers (eg: first pass output feeds into second run - in that case, # `.map` should describe how to \"converge\" by returning the same outputs as a prior call). def build ( self , snapshot : GraphSnapshot ) -> None : # NOTE: Raw Artifacts will already be discovered and linked in the backend to this snapshot. with snapshot . backend . connect () as backend : for node in TopologicalSorter ( snapshot . graph . dependencies ) . static_order (): if isinstance ( node , Artifact ): # TODO: Compute Statistics (if not already computed for the partition) and check # Thresholds (every time, as they may be changed, dynamic, or overridden). pass elif isinstance ( node , Producer ): logging . info ( f \"Building { node } ...\" ) input_partitions = self . get_producer_inputs ( snapshot , backend , node ) ( partition_dependencies , partition_input_fingerprints , ) = node . compute_dependencies ( input_partitions ) existing_keys = self . discover_producer_partitions ( snapshot , backend , node , partition_input_fingerprints = partition_input_fingerprints , ) for partition_key , dependencies in partition_dependencies . items (): self . build_producer_partition ( snapshot , backend , node , existing_partition_keys = existing_keys , input_fingerprint = partition_input_fingerprints [ partition_key ], partition_dependencies = dependencies , partition_key = partition_key , ) else : raise NotImplementedError () logging . info ( \"Build finished.\" )","title":"Module arti.executors.local"},{"location":"reference/arti/executors/local/#classes","text":"","title":"Classes"},{"location":"reference/arti/executors/local/#localexecutor","text":"class LocalExecutor ( __pydantic_self__ , ** data : Any ) View Source class LocalExecutor ( Executor ) : # TODO : Should we separate . map and . build steps so we can : # - add \"sync\" / \"dry run\" sort of things # - parallelize build # # We may still want to repeat the . map phase in the future , if we wanted to support some sort of # iterated or cyclic Producers ( eg : first pass output feeds into second run - in that case , # ` . map ` should describe how to \"converge\" by returning the same outputs as a prior call ). def build ( self , snapshot : GraphSnapshot ) -> None : # NOTE : Raw Artifacts will already be discovered and linked in the backend to this snapshot . with snapshot . backend . connect () as backend : for node in TopologicalSorter ( snapshot . graph . dependencies ). static_order () : if isinstance ( node , Artifact ) : # TODO : Compute Statistics ( if not already computed for the partition ) and check # Thresholds ( every time , as they may be changed , dynamic , or overridden ). pass elif isinstance ( node , Producer ) : logging . info ( f \"Building {node}...\" ) input_partitions = self . get_producer_inputs ( snapshot , backend , node ) ( partition_dependencies , partition_input_fingerprints , ) = node . compute_dependencies ( input_partitions ) existing_keys = self . discover_producer_partitions ( snapshot , backend , node , partition_input_fingerprints = partition_input_fingerprints , ) for partition_key , dependencies in partition_dependencies . items () : self . build_producer_partition ( snapshot , backend , node , existing_partition_keys = existing_keys , input_fingerprint = partition_input_fingerprints [ partition_key ] , partition_dependencies = dependencies , partition_key = partition_key , ) else : raise NotImplementedError () logging . info ( \"Build finished.\" )","title":"LocalExecutor"},{"location":"reference/arti/executors/local/#ancestors-in-mro","text":"arti.executors.Executor arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/executors/local/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/executors/local/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/executors/local/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/executors/local/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/executors/local/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/executors/local/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/executors/local/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/executors/local/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/executors/local/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/executors/local/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/executors/local/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/executors/local/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/executors/local/#methods","text":"","title":"Methods"},{"location":"reference/arti/executors/local/#build","text":"def build ( self , snapshot : 'GraphSnapshot' ) -> 'None' View Source def build ( self , snapshot : GraphSnapshot ) -> None : # NOTE : Raw Artifacts will already be discovered and linked in the backend to this snapshot . with snapshot . backend . connect () as backend : for node in TopologicalSorter ( snapshot . graph . dependencies ). static_order () : if isinstance ( node , Artifact ) : # TODO : Compute Statistics ( if not already computed for the partition ) and check # Thresholds ( every time , as they may be changed , dynamic , or overridden ). pass elif isinstance ( node , Producer ) : logging . info ( f \"Building {node}...\" ) input_partitions = self . get_producer_inputs ( snapshot , backend , node ) ( partition_dependencies , partition_input_fingerprints , ) = node . compute_dependencies ( input_partitions ) existing_keys = self . discover_producer_partitions ( snapshot , backend , node , partition_input_fingerprints = partition_input_fingerprints , ) for partition_key , dependencies in partition_dependencies . items () : self . build_producer_partition ( snapshot , backend , node , existing_partition_keys = existing_keys , input_fingerprint = partition_input_fingerprints [ partition_key ] , partition_dependencies = dependencies , partition_key = partition_key , ) else : raise NotImplementedError () logging . info ( \"Build finished.\" )","title":"build"},{"location":"reference/arti/executors/local/#build_producer_partition","text":"def build_producer_partition ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' , * , existing_partition_keys : 'set[CompositeKey]' , input_fingerprint : 'Fingerprint' , partition_dependencies : 'frozendict[str, StoragePartitions]' , partition_key : 'CompositeKey' ) -> 'None' View Source def build_producer_partition ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , existing_partition_keys : set [ CompositeKey ] , input_fingerprint : Fingerprint , partition_dependencies : frozendict [ str, StoragePartitions ] , partition_key : CompositeKey , ) -> None : # TODO : Should this \"skip if exists\" live here or higher up ? if partition_key in existing_partition_keys : pk_str = f \" for: {dict(partition_key)}\" if partition_key else \".\" logging . info ( f \"Skipping existing {type(producer).__name__} output{pk_str}\" ) return logging . info ( f \"Building {producer} output for {partition_key}...\" ) # TODO : Catch DispatchError and give a nicer error ... maybe add this to our # @dispatch wrapper ( eg : msg arg , or even fn that returns the message to # raise ). arguments = { name : snapshot . read ( artifact = producer . inputs [ name ] , storage_partitions = partition_dependencies [ name ] , view = view , ) for name , view in producer . _build_inputs_ . items () } outputs = producer . build ( ** arguments ) if len ( producer . _outputs_ ) == 1 : outputs = ( outputs ,) validation_passed , validation_message = producer . validate_outputs ( * outputs ) if not validation_passed : raise ValueError ( validation_message ) for i , output in enumerate ( outputs ) : snapshot . write ( output , artifact = snapshot . graph . producer_outputs [ producer ][ i ] , input_fingerprint = input_fingerprint , keys = partition_key , view = producer . _outputs_ [ i ] , )","title":"build_producer_partition"},{"location":"reference/arti/executors/local/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/executors/local/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/executors/local/#discover_producer_partitions","text":"def discover_producer_partitions ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' , * , partition_input_fingerprints : 'InputFingerprints' ) -> 'set[CompositeKey]' View Source def discover_producer_partitions ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer , * , partition_input_fingerprints : InputFingerprints , ) -> set [ CompositeKey ] : # NOTE : The output partitions may be built , but not yet associated with this GraphSnapshot # ( eg : raw input data changed , but no changes trickled into this specific Producer ). Hence # we 'll fetch all StoragePartitions for each Storage, filtered to the PKs and # input_fingerprints we' ve computed * are * for this snapshot - and then link them to the # snapshot . existing_output_partitions = { output : connection . read_artifact_partitions ( output , partition_input_fingerprints ) for output in snapshot . graph . producer_outputs [ producer ] } for artifact , partitions in existing_output_partitions . items () : connection . write_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact , partitions ) # TODO : Guarantee all outputs have the same set of identified partitions . Currently , this # pretends a partition is built for all outputs if _any_ are built for that partition . return { partition . keys for partition in chain . from_iterable ( existing_output_partitions . values ()) }","title":"discover_producer_partitions"},{"location":"reference/arti/executors/local/#get_producer_inputs","text":"def get_producer_inputs ( self , snapshot : 'GraphSnapshot' , connection : 'Connection' , producer : 'Producer' ) -> 'InputPartitions' View Source def get_producer_inputs ( self , snapshot : GraphSnapshot , connection : Connection , producer : Producer ) -> InputPartitions : return InputPartitions ( { name : connection . read_snapshot_partitions ( snapshot , snapshot . graph . artifact_to_key [ artifact ] , artifact ) for name , artifact in producer . inputs . items () } )","title":"get_producer_inputs"},{"location":"reference/arti/executors/local/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/formats/","text":"Module arti.formats None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from typing import ClassVar from arti.internal.models import Model from arti.internal.type_hints import Self from arti.types import Type , TypeSystem class Format ( Model ): \"\"\"Format represents file formats such as CSV, Parquet, native (eg: databases), etc. Formats are associated with a type system that provides a bridge between the internal Artigraph types and any external type information. \"\"\" _abstract_ = True type_system : ClassVar [ TypeSystem ] extension : str = \"\" def _visit_type ( self , type_ : Type ) -> Self : # Ensure our type system can handle the provided type. self . type_system . to_system ( type_ , hints = {}) return self @classmethod def get_default ( cls ) -> Format : from arti.formats.json import JSON return JSON () # TODO: Support some sort of configurable defaults. Sub-modules arti.formats.json arti.formats.pickle Classes Format class Format ( __pydantic_self__ , ** data : Any ) View Source class Format ( Model ): \"\"\"Format represents file formats such as CSV, Parquet, native (eg: databases), etc. Formats are associated with a type system that provides a bridge between the internal Artigraph types and any external type information. \"\"\" _abstract_ = True type_system : ClassVar [ TypeSystem ] extension : str = \"\" def _visit_type ( self , type_ : Type ) -> Self : # Ensure our type system can handle the provided type. self . type_system . to_system ( type_ , hints = {}) return self @classmethod def get_default ( cls ) -> Format : from arti.formats.json import JSON return JSON () # TODO: Support some sort of configurable defaults. Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.formats.json.JSON arti.formats.pickle.Pickle Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_default def get_default ( ) -> 'Format' View Source @classmethod def get_default ( cls ) -> Format : from arti.formats.json import JSON return JSON () # TODO: Support some sort of configurable defaults. parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Index"},{"location":"reference/arti/formats/#module-artiformats","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from typing import ClassVar from arti.internal.models import Model from arti.internal.type_hints import Self from arti.types import Type , TypeSystem class Format ( Model ): \"\"\"Format represents file formats such as CSV, Parquet, native (eg: databases), etc. Formats are associated with a type system that provides a bridge between the internal Artigraph types and any external type information. \"\"\" _abstract_ = True type_system : ClassVar [ TypeSystem ] extension : str = \"\" def _visit_type ( self , type_ : Type ) -> Self : # Ensure our type system can handle the provided type. self . type_system . to_system ( type_ , hints = {}) return self @classmethod def get_default ( cls ) -> Format : from arti.formats.json import JSON return JSON () # TODO: Support some sort of configurable defaults.","title":"Module arti.formats"},{"location":"reference/arti/formats/#sub-modules","text":"arti.formats.json arti.formats.pickle","title":"Sub-modules"},{"location":"reference/arti/formats/#classes","text":"","title":"Classes"},{"location":"reference/arti/formats/#format","text":"class Format ( __pydantic_self__ , ** data : Any ) View Source class Format ( Model ): \"\"\"Format represents file formats such as CSV, Parquet, native (eg: databases), etc. Formats are associated with a type system that provides a bridge between the internal Artigraph types and any external type information. \"\"\" _abstract_ = True type_system : ClassVar [ TypeSystem ] extension : str = \"\" def _visit_type ( self , type_ : Type ) -> Self : # Ensure our type system can handle the provided type. self . type_system . to_system ( type_ , hints = {}) return self @classmethod def get_default ( cls ) -> Format : from arti.formats.json import JSON return JSON () # TODO: Support some sort of configurable defaults.","title":"Format"},{"location":"reference/arti/formats/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/formats/#descendants","text":"arti.formats.json.JSON arti.formats.pickle.Pickle","title":"Descendants"},{"location":"reference/arti/formats/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/formats/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/formats/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/formats/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/formats/#get_default","text":"def get_default ( ) -> 'Format' View Source @classmethod def get_default ( cls ) -> Format : from arti.formats.json import JSON return JSON () # TODO: Support some sort of configurable defaults.","title":"get_default"},{"location":"reference/arti/formats/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/formats/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/formats/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/formats/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/formats/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/formats/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/formats/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/formats/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/formats/#methods","text":"","title":"Methods"},{"location":"reference/arti/formats/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/formats/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/formats/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/formats/json/","text":"Module arti.formats.json None None View Source from __future__ import annotations from arti.formats import Format from arti.types.python import python_type_system class JSON ( Format ): extension = \".json\" # Perhaps we narrow down a json_type_system with the subset of supported types + a way to hook # into json.JSON{De,En}coders. type_system = python_type_system Classes JSON class JSON ( __pydantic_self__ , ** data : Any ) View Source class JSON ( Format ): extension = \".json\" # Perhaps we narrow down a json_type_system with the subset of supported types + a way to hook # into json.JSON{De,En}coders. type_system = python_type_system Ancestors (in MRO) arti.formats.Format arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config extension type_system Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_default def get_default ( ) -> 'Format' View Source @classmethod def get_default ( cls ) -> Format : from arti.formats.json import JSON return JSON () # TODO: Support some sort of configurable defaults. parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Json"},{"location":"reference/arti/formats/json/#module-artiformatsjson","text":"None None View Source from __future__ import annotations from arti.formats import Format from arti.types.python import python_type_system class JSON ( Format ): extension = \".json\" # Perhaps we narrow down a json_type_system with the subset of supported types + a way to hook # into json.JSON{De,En}coders. type_system = python_type_system","title":"Module arti.formats.json"},{"location":"reference/arti/formats/json/#classes","text":"","title":"Classes"},{"location":"reference/arti/formats/json/#json","text":"class JSON ( __pydantic_self__ , ** data : Any ) View Source class JSON ( Format ): extension = \".json\" # Perhaps we narrow down a json_type_system with the subset of supported types + a way to hook # into json.JSON{De,En}coders. type_system = python_type_system","title":"JSON"},{"location":"reference/arti/formats/json/#ancestors-in-mro","text":"arti.formats.Format arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/formats/json/#class-variables","text":"Config extension type_system","title":"Class variables"},{"location":"reference/arti/formats/json/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/formats/json/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/formats/json/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/formats/json/#get_default","text":"def get_default ( ) -> 'Format' View Source @classmethod def get_default ( cls ) -> Format : from arti.formats.json import JSON return JSON () # TODO: Support some sort of configurable defaults.","title":"get_default"},{"location":"reference/arti/formats/json/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/formats/json/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/formats/json/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/formats/json/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/formats/json/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/formats/json/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/formats/json/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/formats/json/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/formats/json/#methods","text":"","title":"Methods"},{"location":"reference/arti/formats/json/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/formats/json/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/formats/json/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/formats/pickle/","text":"Module arti.formats.pickle None None View Source from __future__ import annotations from arti.formats import Format from arti.types.python import python_type_system class Pickle ( Format ): extension = \".pickle\" type_system = python_type_system Classes Pickle class Pickle ( __pydantic_self__ , ** data : Any ) View Source class Pickle ( Format ): extension = \".pickle\" type_system = python_type_system Ancestors (in MRO) arti.formats.Format arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config extension type_system Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_default def get_default ( ) -> 'Format' View Source @classmethod def get_default ( cls ) -> Format : from arti.formats.json import JSON return JSON () # TODO: Support some sort of configurable defaults. parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Pickle"},{"location":"reference/arti/formats/pickle/#module-artiformatspickle","text":"None None View Source from __future__ import annotations from arti.formats import Format from arti.types.python import python_type_system class Pickle ( Format ): extension = \".pickle\" type_system = python_type_system","title":"Module arti.formats.pickle"},{"location":"reference/arti/formats/pickle/#classes","text":"","title":"Classes"},{"location":"reference/arti/formats/pickle/#pickle","text":"class Pickle ( __pydantic_self__ , ** data : Any ) View Source class Pickle ( Format ): extension = \".pickle\" type_system = python_type_system","title":"Pickle"},{"location":"reference/arti/formats/pickle/#ancestors-in-mro","text":"arti.formats.Format arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/formats/pickle/#class-variables","text":"Config extension type_system","title":"Class variables"},{"location":"reference/arti/formats/pickle/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/formats/pickle/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/formats/pickle/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/formats/pickle/#get_default","text":"def get_default ( ) -> 'Format' View Source @classmethod def get_default ( cls ) -> Format : from arti.formats.json import JSON return JSON () # TODO: Support some sort of configurable defaults.","title":"get_default"},{"location":"reference/arti/formats/pickle/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/formats/pickle/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/formats/pickle/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/formats/pickle/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/formats/pickle/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/formats/pickle/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/formats/pickle/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/formats/pickle/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/formats/pickle/#methods","text":"","title":"Methods"},{"location":"reference/arti/formats/pickle/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/formats/pickle/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/formats/pickle/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/internal/","text":"Module arti.internal None None View Source from __future__ import annotations from collections.abc import Iterator from contextlib import contextmanager @contextmanager def wrap_exc ( error_type : type [ Exception ], * , prefix : str ) -> Iterator [ None ]: \"\"\"Wrap exceptions of `error_type` and add a message prefix. `error_type` must be initializable with a single string message argument. NOTE: When used inside a generator, any exceptions raised by the *caller of the generator* will **not** be wrapped. \"\"\" try : yield except error_type as e : msg = str ( e ) if getattr ( e , \"wrapped\" , False ) and e . __cause__ is not None : src = e . __cause__ # Shorten exception chains to the root and last wrapped only else : msg = f \" - { msg } \" src = e error = error_type ( f \" { prefix }{ msg } \" ) error . wrapped = True # type: ignore[attr-defined] raise error from src Sub-modules arti.internal.dispatch arti.internal.models arti.internal.patches arti.internal.type_hints arti.internal.utils Functions wrap_exc def wrap_exc ( error_type : 'type[Exception]' , * , prefix : 'str' ) -> 'Iterator[None]' Wrap exceptions of error_type and add a message prefix. error_type must be initializable with a single string message argument. NOTE: When used inside a generator, any exceptions raised by the caller of the generator will not be wrapped. View Source @contextmanager def wrap_exc ( error_type : type [ Exception ] , * , prefix : str ) -> Iterator [ None ] : \"\"\"Wrap exceptions of `error_type` and add a message prefix. `error_type` must be initializable with a single string message argument. NOTE: When used inside a generator, any exceptions raised by the *caller of the generator* will **not** be wrapped. \"\"\" try : yield except error_type as e : msg = str ( e ) if getattr ( e , \"wrapped\" , False ) and e . __cause__ is not None : src = e . __cause__ # Shorten exception chains to the root and last wrapped only else : msg = f \" - {msg}\" src = e error = error_type ( f \"{prefix}{msg}\" ) error . wrapped = True # type : ignore [ attr-defined ] raise error from src","title":"Index"},{"location":"reference/arti/internal/#module-artiinternal","text":"None None View Source from __future__ import annotations from collections.abc import Iterator from contextlib import contextmanager @contextmanager def wrap_exc ( error_type : type [ Exception ], * , prefix : str ) -> Iterator [ None ]: \"\"\"Wrap exceptions of `error_type` and add a message prefix. `error_type` must be initializable with a single string message argument. NOTE: When used inside a generator, any exceptions raised by the *caller of the generator* will **not** be wrapped. \"\"\" try : yield except error_type as e : msg = str ( e ) if getattr ( e , \"wrapped\" , False ) and e . __cause__ is not None : src = e . __cause__ # Shorten exception chains to the root and last wrapped only else : msg = f \" - { msg } \" src = e error = error_type ( f \" { prefix }{ msg } \" ) error . wrapped = True # type: ignore[attr-defined] raise error from src","title":"Module arti.internal"},{"location":"reference/arti/internal/#sub-modules","text":"arti.internal.dispatch arti.internal.models arti.internal.patches arti.internal.type_hints arti.internal.utils","title":"Sub-modules"},{"location":"reference/arti/internal/#functions","text":"","title":"Functions"},{"location":"reference/arti/internal/#wrap_exc","text":"def wrap_exc ( error_type : 'type[Exception]' , * , prefix : 'str' ) -> 'Iterator[None]' Wrap exceptions of error_type and add a message prefix. error_type must be initializable with a single string message argument. NOTE: When used inside a generator, any exceptions raised by the caller of the generator will not be wrapped. View Source @contextmanager def wrap_exc ( error_type : type [ Exception ] , * , prefix : str ) -> Iterator [ None ] : \"\"\"Wrap exceptions of `error_type` and add a message prefix. `error_type` must be initializable with a single string message argument. NOTE: When used inside a generator, any exceptions raised by the *caller of the generator* will **not** be wrapped. \"\"\" try : yield except error_type as e : msg = str ( e ) if getattr ( e , \"wrapped\" , False ) and e . __cause__ is not None : src = e . __cause__ # Shorten exception chains to the root and last wrapped only else : msg = f \" - {msg}\" src = e error = error_type ( f \"{prefix}{msg}\" ) error . wrapped = True # type : ignore [ attr-defined ] raise error from src","title":"wrap_exc"},{"location":"reference/arti/internal/dispatch/","text":"Module arti.internal.dispatch None None View Source from __future__ import annotations import inspect from collections.abc import Callable from typing import Any , Optional , TypeVar , cast , overload import multimethod as _multimethod # Minimize name confusion from arti.internal.type_hints import lenient_issubclass , tidy_signature RETURN = TypeVar ( \"RETURN\" ) REGISTERED = TypeVar ( \"REGISTERED\" , bound = Callable [ ... , Any ]) # This may be less useful once mypy supports ParamSpecs - after that, we might be able to define # multidispatch with a ParamSpec and have mypy check the handlers' arguments are covariant. class _multipledispatch ( _multimethod . multidispatch [ RETURN ]): \"\"\"Multiple dispatch for a set of functions based on parameter type. Usage is similar to `@functools.singledispatch`. The original definition defines the \"spec\" that subsequent handlers must follow, namely the name and (base)class of parameters. \"\"\" # NOTE: We can't add extra (kw)args without also overriding __new__. However, `__new__` is # called for each *registered* func in the multimethod internals (a bit confusing). Instead, we # can just set attrs in a helper func below. def __init__ ( self , func : Callable [ ... , RETURN ]) -> None : super () . __init__ ( func ) self . canonical_name : Optional [ str ] = None self . discovery_func : Optional [ Callable [[], None ]] = None assert self . signature is not None self . clean_signature = tidy_signature ( func , self . signature ) def __missing__ ( self , types : tuple [ Any , ... ]) -> Callable [ ... , RETURN ]: if self . discovery_func is not None : self . discovery_func () return super () . __missing__ ( types ) def lookup ( self , * args : Optional [ type [ Any ]]) -> Callable [ ... , Any ]: # multimethod wraps Generics (eg: `list[int]`) with an internal helper. We must do the same # before looking up. Non-Generics pass through as is. args = tuple ( _multimethod . subtype ( arg ) for arg in args ) # type: ignore[no-untyped-call] # NOTE: multimethod doesn't override __contains__ (likely so __missing__ will still run), so # \"args in self\" will be False when using subclasses of any arg. missing_error = ValueError ( f \"No ` { self . canonical_name } ` implementation found for: { args } \" ) try : handler = cast ( Callable [ ... , Any ], self [ args ]) # multimethod raises a TypeError instead of KeyError, as __call__. except TypeError as e : # pragma: no cover raise missing_error from e # Filter out the base \"NotImplementedError\" handler. if getattr ( handler , \"_abstract_\" , False ): raise missing_error return handler @overload def register ( self , __func : REGISTERED ) -> REGISTERED : ... @overload def register ( self , * args : type ) -> Callable [[ REGISTERED ], REGISTERED ]: ... def register ( self , * args : Any ) -> Callable [[ REGISTERED ], REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ], \"__annotations__\" ): func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ): raise TypeError ( f \"Expected ` { func . __name__ } ` to have { sorted ( set ( spec . parameters )) } parameters, got { sorted ( set ( sig . parameters )) } \" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ], spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the ` { func . __name__ } . { name } ` parameter to be { spec_param . kind } , got { sig_param . kind } \" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ): raise TypeError ( f \"Expected the ` { func . __name__ } . { name } ` parameter to be a subclass of { spec_param . annotation } , got { sig_param . annotation } \" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ): raise TypeError ( f \"Expected the ` { func . __name__ } ` return to match { spec . return_annotation } , got { sig . return_annotation } \" ) return cast ( Callable [ ... , Any ], super () . register ( * args )) def multipledispatch ( canonical_name : str , * , discovery_func : Optional [ Callable [[], None ]] = None ) -> Callable [[ Callable [ ... , RETURN ]], _multipledispatch [ RETURN ]]: def wrap ( func : Callable [ ... , RETURN ]) -> _multipledispatch [ RETURN ]: # The base handler is expected to `raise NotImplementedError` func . _abstract_ = True # type: ignore[attr-defined] dispatch = _multipledispatch ( func ) dispatch . canonical_name = canonical_name dispatch . discovery_func = discovery_func return dispatch return wrap Variables REGISTERED RETURN Functions multipledispatch def multipledispatch ( canonical_name : 'str' , * , discovery_func : 'Optional[Callable[[], None]]' = None ) -> 'Callable[[Callable[..., RETURN]], _multipledispatch[RETURN]]' View Source def multipledispatch ( canonical_name : str , * , discovery_func : Optional [ Callable[[ ] , None ]] = None ) -> Callable [ [Callable[..., RETURN ] ] , _multipledispatch [ RETURN ] ]: def wrap ( func : Callable [ ..., RETURN ] ) -> _multipledispatch [ RETURN ] : # The base handler is expected to ` raise NotImplementedError ` func . _abstract_ = True # type : ignore [ attr-defined ] dispatch = _multipledispatch ( func ) dispatch . canonical_name = canonical_name dispatch . discovery_func = discovery_func return dispatch return wrap","title":"Dispatch"},{"location":"reference/arti/internal/dispatch/#module-artiinternaldispatch","text":"None None View Source from __future__ import annotations import inspect from collections.abc import Callable from typing import Any , Optional , TypeVar , cast , overload import multimethod as _multimethod # Minimize name confusion from arti.internal.type_hints import lenient_issubclass , tidy_signature RETURN = TypeVar ( \"RETURN\" ) REGISTERED = TypeVar ( \"REGISTERED\" , bound = Callable [ ... , Any ]) # This may be less useful once mypy supports ParamSpecs - after that, we might be able to define # multidispatch with a ParamSpec and have mypy check the handlers' arguments are covariant. class _multipledispatch ( _multimethod . multidispatch [ RETURN ]): \"\"\"Multiple dispatch for a set of functions based on parameter type. Usage is similar to `@functools.singledispatch`. The original definition defines the \"spec\" that subsequent handlers must follow, namely the name and (base)class of parameters. \"\"\" # NOTE: We can't add extra (kw)args without also overriding __new__. However, `__new__` is # called for each *registered* func in the multimethod internals (a bit confusing). Instead, we # can just set attrs in a helper func below. def __init__ ( self , func : Callable [ ... , RETURN ]) -> None : super () . __init__ ( func ) self . canonical_name : Optional [ str ] = None self . discovery_func : Optional [ Callable [[], None ]] = None assert self . signature is not None self . clean_signature = tidy_signature ( func , self . signature ) def __missing__ ( self , types : tuple [ Any , ... ]) -> Callable [ ... , RETURN ]: if self . discovery_func is not None : self . discovery_func () return super () . __missing__ ( types ) def lookup ( self , * args : Optional [ type [ Any ]]) -> Callable [ ... , Any ]: # multimethod wraps Generics (eg: `list[int]`) with an internal helper. We must do the same # before looking up. Non-Generics pass through as is. args = tuple ( _multimethod . subtype ( arg ) for arg in args ) # type: ignore[no-untyped-call] # NOTE: multimethod doesn't override __contains__ (likely so __missing__ will still run), so # \"args in self\" will be False when using subclasses of any arg. missing_error = ValueError ( f \"No ` { self . canonical_name } ` implementation found for: { args } \" ) try : handler = cast ( Callable [ ... , Any ], self [ args ]) # multimethod raises a TypeError instead of KeyError, as __call__. except TypeError as e : # pragma: no cover raise missing_error from e # Filter out the base \"NotImplementedError\" handler. if getattr ( handler , \"_abstract_\" , False ): raise missing_error return handler @overload def register ( self , __func : REGISTERED ) -> REGISTERED : ... @overload def register ( self , * args : type ) -> Callable [[ REGISTERED ], REGISTERED ]: ... def register ( self , * args : Any ) -> Callable [[ REGISTERED ], REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ], \"__annotations__\" ): func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ): raise TypeError ( f \"Expected ` { func . __name__ } ` to have { sorted ( set ( spec . parameters )) } parameters, got { sorted ( set ( sig . parameters )) } \" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ], spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the ` { func . __name__ } . { name } ` parameter to be { spec_param . kind } , got { sig_param . kind } \" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ): raise TypeError ( f \"Expected the ` { func . __name__ } . { name } ` parameter to be a subclass of { spec_param . annotation } , got { sig_param . annotation } \" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ): raise TypeError ( f \"Expected the ` { func . __name__ } ` return to match { spec . return_annotation } , got { sig . return_annotation } \" ) return cast ( Callable [ ... , Any ], super () . register ( * args )) def multipledispatch ( canonical_name : str , * , discovery_func : Optional [ Callable [[], None ]] = None ) -> Callable [[ Callable [ ... , RETURN ]], _multipledispatch [ RETURN ]]: def wrap ( func : Callable [ ... , RETURN ]) -> _multipledispatch [ RETURN ]: # The base handler is expected to `raise NotImplementedError` func . _abstract_ = True # type: ignore[attr-defined] dispatch = _multipledispatch ( func ) dispatch . canonical_name = canonical_name dispatch . discovery_func = discovery_func return dispatch return wrap","title":"Module arti.internal.dispatch"},{"location":"reference/arti/internal/dispatch/#variables","text":"REGISTERED RETURN","title":"Variables"},{"location":"reference/arti/internal/dispatch/#functions","text":"","title":"Functions"},{"location":"reference/arti/internal/dispatch/#multipledispatch","text":"def multipledispatch ( canonical_name : 'str' , * , discovery_func : 'Optional[Callable[[], None]]' = None ) -> 'Callable[[Callable[..., RETURN]], _multipledispatch[RETURN]]' View Source def multipledispatch ( canonical_name : str , * , discovery_func : Optional [ Callable[[ ] , None ]] = None ) -> Callable [ [Callable[..., RETURN ] ] , _multipledispatch [ RETURN ] ]: def wrap ( func : Callable [ ..., RETURN ] ) -> _multipledispatch [ RETURN ] : # The base handler is expected to ` raise NotImplementedError ` func . _abstract_ = True # type : ignore [ attr-defined ] dispatch = _multipledispatch ( func ) dispatch . canonical_name = canonical_name dispatch . discovery_func = discovery_func return dispatch return wrap","title":"multipledispatch"},{"location":"reference/arti/internal/models/","text":"Module arti.internal.models None None View Source from __future__ import annotations from collections.abc import Generator , Mapping , Sequence from copy import deepcopy from functools import cached_property , partial from typing import ( TYPE_CHECKING , Annotated , Any , ClassVar , Literal , Optional , TypeVar , Union , get_args , get_origin , ) from box import Box from pydantic import BaseModel , Extra , root_validator , validator from pydantic.fields import ModelField , Undefined from pydantic.json import pydantic_encoder as pydantic_json_encoder from arti.internal.type_hints import Self , is_union , lenient_issubclass from arti.internal.utils import class_name , frozendict if TYPE_CHECKING : from pydantic.typing import AbstractSetIntStr , MappingIntStrAny from arti.fingerprints import Fingerprint from arti.types import Type def _check_types ( value : Any , type_ : type ) -> Any : mismatch_error = ValueError ( f \"expected an instance of { type_ } , got: { value } \" ) if type_ is Any : return value origin = get_origin ( type_ ) if origin is not None : args = get_args ( type_ ) if origin is Annotated : return _check_types ( value , args [ 0 ]) if origin is Literal : return _check_types ( value , type ( args [ 0 ])) # NOTE: Optional[t] -> Union[t, NoneType] if is_union ( origin ): for subtype in args : try : return _check_types ( value , subtype ) except ValueError : pass raise mismatch_error if issubclass ( origin , ( dict , Mapping )): value = _check_types ( value , origin ) for k , v in value . items (): _check_types ( k , args [ 0 ]) _check_types ( v , args [ 1 ]) return value # Variadic tuples will be handled below if issubclass ( origin , tuple ) and ... not in args : value = _check_types ( value , origin ) if len ( value ) != len ( args ): raise mismatch_error for i , subtype in enumerate ( args ): _check_types ( value [ i ], subtype ) return value for t in ( tuple , list , set , frozenset , Sequence ): if issubclass ( origin , t ): value = _check_types ( value , origin ) for subvalue in value : _check_types ( subvalue , args [ 0 ]) return value if issubclass ( origin , type ): if not lenient_issubclass ( value , args [ 0 ]): raise ValueError ( f \"expected a subclass of { args [ 0 ] } , got: { value } \" ) return value if set ( args ) == { Any }: return _check_types ( value , origin ) # pragma: no cover if isinstance ( value , origin ): # Other Generic args can't really be checked generally return value raise ValueError ( f \"expected a instance of { origin } , got: { value } \" ) # pragma: no cover if isinstance ( type_ , TypeVar ): return value # TODO: Check __bound__, __covariant__, __contravariant__ # Models are immutable, so we convert all Mappings to frozendicts. if isinstance ( value , Mapping ) and not isinstance ( value , frozendict ): value = frozendict ( value ) if lenient_issubclass ( type_ , Mapping ): type_ = frozendict if not lenient_issubclass ( type ( value ), type_ ): raise mismatch_error return value _Model = TypeVar ( \"_Model\" , bound = \"Model\" ) class Model ( BaseModel ): # A model can be marked _abstract_ to prevent direct instantiation, such as when it is intended # as a base class for other models with arbitrary data. As the subclasses of an _abstract_ model # have unknown fields (varying per subclass), we don't have targets to mark abstract with # abc.ABC nor typing.Protocol. See [1] for more context. # # 1: https://github.com/artigraph/artigraph/pull/60#discussion_r669089086 _abstract_ : ClassVar [ bool ] = True _class_key_ : ClassVar [ str ] = class_name () _fingerprint_excludes_ : ClassVar [ Optional [ frozenset [ str ]]] = None _fingerprint_includes_ : ClassVar [ Optional [ frozenset [ str ]]] = None @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) # Default _abstract_ to False if not set explicitly on the class. __dict__ is read-only. cls . _abstract_ = cls . __dict__ . get ( \"_abstract_\" , False ) field_names = set ( cls . __fields__ ) if cls . _fingerprint_excludes_ and ( unknown_excludes := cls . _fingerprint_excludes_ - field_names ): raise ValueError ( f \"Unknown `_fingerprint_excludes_` field(s): { unknown_excludes } \" ) if cls . _fingerprint_includes_ and ( unknown_includes := cls . _fingerprint_includes_ - field_names ): raise ValueError ( f \"Unknown `_fingerprint_includes_` field(s): { unknown_includes } \" ) @root_validator ( pre = True ) @classmethod def _block_abstract_instance ( cls , values : dict [ str , Any ]) -> dict [ str , Any ]: if cls . _abstract_ : raise ValueError ( f \" { cls } cannot be instantiated directly!\" ) return values @validator ( \"*\" , pre = True ) @classmethod def _strict_types ( cls , value : Any , field : ModelField ) -> Any : \"\"\"Check that the value is a stricter instance of the declared type annotation. Pydantic will attempt to *parse* values (eg: \"5\" -> 5), but we'd prefer stricter values for clarity and to avoid silent precision loss (eg: 5.3 -> 5). \"\"\" # `field.type_` points to the *inner* type (eg: `int`->`int`; `tuple[int, ...]` -> `int`) # while `field.outer_type_` will (mostly) include the full spec and match the `value` we # received. The caveat is the `field.outer_type_` will never be wrapped in `Optional` # (though nested fields like `tuple[tuple[Optional[int]]]` would). Hence, we pull the # `field.outer_type_`, but add back the `Optional` wrapping if necessary. type_ = field . outer_type_ if field . allow_none : type_ = Optional [ type_ ] return _check_types ( value , type_ ) # By default, pydantic just compares models by their dict representation, causing models of # different types but same fields (eg: Int8 and Int16) to be equivalent. This can be removed if # [1] is merged+released. # # 1: https://github.com/samuelcolvin/pydantic/pull/3066 def __eq__ ( self , other : Any ) -> bool : return self . __class__ == other . __class__ and tuple ( self . _iter ()) == tuple ( other . _iter ()) def __hash__ ( self ) -> int : # Override the default __hash__ to match the fingerprint, which notably excludes # `cached_property`s. # # If `cached_property`s are included, the hash will be different before and after caching, # which wrecks havoc if a model is a key in a dict (`key in mydict` will be `False`...). # # This is only safe as the models are (mostly) frozen. assert ( key := self . fingerprint . key ) is not None return key # Omitting unpassed args in repr by default def __repr_args__ ( self ) -> Sequence [ tuple [ Optional [ str ], Any ]]: return [( k , v ) for k , v in super () . __repr_args__ () if k in self . __fields_set__ ] def __str__ ( self ) -> str : return repr ( self ) class Config : extra = Extra . forbid frozen = True json_encoders = { frozendict : dict } keep_untouched = ( cached_property ,) smart_union = True validate_assignment = True # Unused with frozen, unless that is overridden in subclass. def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super () . copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy @staticmethod def _fingerprint_json_encoder ( obj : Any , encoder : Any = pydantic_json_encoder ) -> Any : from arti.fingerprints import Fingerprint if isinstance ( obj , Fingerprint ): return obj . key if isinstance ( obj , Model ): return obj . fingerprint if lenient_issubclass ( obj , Model ): return obj . _class_key_ # eg: View.artifact_class return encoder ( obj ) @property def fingerprint ( self ) -> Fingerprint : from arti.fingerprints import Fingerprint # `.json` cannot be used, even with a custom encoder, because it calls `.dict`, which # converts the sub-models to dicts. Instead, we want to access `.fingerprint` (in the # decoder). data = dict ( sorted ( # Sort to ensure stability self . _iter ( exclude = self . _fingerprint_excludes_ , include = self . _fingerprint_includes_ , ), key = lambda kv : kv [ 0 ], ) ) json_repr = self . __config__ . json_dumps ( data , default = partial ( self . _fingerprint_json_encoder , encoder = self . __json_encoder__ ), sort_keys = True , ) return Fingerprint . from_string ( f \" { self . _class_key_ } : { json_repr } \" ) @classmethod def _get_value ( cls , v : Any , to_dict : bool , by_alias : bool , include : Optional [ Union [ AbstractSetIntStr , MappingIntStrAny ]], exclude : Optional [ Union [ AbstractSetIntStr , MappingIntStrAny ]], exclude_unset : bool , exclude_defaults : bool , exclude_none : bool , ) -> Any : new = super () . _get_value ( v , to_dict = to_dict , by_alias = by_alias , include = include , exclude = exclude , exclude_unset = exclude_unset , exclude_defaults = exclude_defaults , exclude_none = exclude_none , ) # Copying dict subclasses doesn't preserve the subclass[1]. Further, we have extra Box # configuration (namely frozen_box=True) we need to preserve. # # 1: https://github.com/pydantic/pydantic/issues/5225 if isinstance ( v , Box ): return v . __class__ ( new , ** v . _Box__box_config ()) return new # Filter out non-fields from ._iter (and thus .dict, .json, etc), such as `@cached_property` # after access (which just gets cached in .__dict__). def _iter ( self , * args : Any , ** kwargs : Any ) -> Generator [ tuple [ str , Any ], None , None ]: for key , value in super () . _iter ( * args , ** kwargs ): if key in self . __fields__ : yield key , value @classmethod def _pydantic_type_system_post_field_conversion_hook_ ( cls , type_ : Type , * , name : str , required : bool ) -> Type : return type_ def get_field_default ( model : type [ Model ], field : str ) -> Optional [ Any ]: return model . __fields__ [ field ] . default Variables TYPE_CHECKING Functions get_field_default def get_field_default ( model : 'type[Model]' , field : 'str' ) -> 'Optional[Any]' View Source def get_field_default ( model : type [ Model ] , field : str ) -> Optional [ Any ] : return model . __fields__ [ field ] . default Classes Model class Model ( __pydantic_self__ , ** data : Any ) View Source class Model ( BaseModel ): # A model can be marked _abstract_ to prevent direct instantiation, such as when it is intended # as a base class for other models with arbitrary data. As the subclasses of an _abstract_ model # have unknown fields (varying per subclass), we don't have targets to mark abstract with # abc.ABC nor typing.Protocol. See [1] for more context. # # 1: https://github.com/artigraph/artigraph/pull/60#discussion_r669089086 _abstract_ : ClassVar [ bool ] = True _class_key_ : ClassVar [ str ] = class_name () _fingerprint_excludes_ : ClassVar [ Optional [ frozenset [ str ]]] = None _fingerprint_includes_ : ClassVar [ Optional [ frozenset [ str ]]] = None @ classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) # Default _abstract_ to False if not set explicitly on the class. __dict__ is read-only. cls . _abstract_ = cls . __dict__ . get ( \"_abstract_\" , False ) field_names = set ( cls . __fields__ ) if cls . _fingerprint_excludes_ and ( unknown_excludes : = cls . _fingerprint_excludes_ - field_names ): raise ValueError ( f \"Unknown `_fingerprint_excludes_` field(s): {unknown_excludes}\" ) if cls . _fingerprint_includes_ and ( unknown_includes : = cls . _fingerprint_includes_ - field_names ): raise ValueError ( f \"Unknown `_fingerprint_includes_` field(s): {unknown_includes}\" ) @ root_validator ( pre = True ) @ classmethod def _block_abstract_instance ( cls , values : dict [ str , Any ]) -> dict [ str , Any ]: if cls . _abstract_ : raise ValueError ( f \"{cls} cannot be instantiated directly!\" ) return values @ validator ( \"*\" , pre = True ) @ classmethod def _strict_types ( cls , value : Any , field : ModelField ) -> Any : \"\"\"Check that the value is a stricter instance of the declared type annotation. Pydantic will attempt to *parse* values (eg: \"5\" -> 5), but we'd prefer stricter values for clarity and to avoid silent precision loss (eg: 5.3 -> 5). \"\"\" # `field.type_` points to the *inner* type (eg: `int`->`int`; `tuple[int, ...]` -> `int`) # while `field.outer_type_` will (mostly) include the full spec and match the `value` we # received. The caveat is the `field.outer_type_` will never be wrapped in `Optional` # (though nested fields like `tuple[tuple[Optional[int]]]` would). Hence, we pull the # `field.outer_type_`, but add back the `Optional` wrapping if necessary. type_ = field . outer_type_ if field . allow_none : type_ = Optional [ type_ ] return _check_types ( value , type_ ) # By default, pydantic just compares models by their dict representation, causing models of # different types but same fields (eg: Int8 and Int16) to be equivalent. This can be removed if # [1] is merged+released. # # 1: https://github.com/samuelcolvin/pydantic/pull/3066 def __eq__ ( self , other : Any ) -> bool : return self . __class__ == other . __class__ and tuple ( self . _iter ()) == tuple ( other . _iter ()) def __hash__ ( self ) -> int : # Override the default __hash__ to match the fingerprint, which notably excludes # `cached_property`s. # # If `cached_property`s are included, the hash will be different before and after caching, # which wrecks havoc if a model is a key in a dict (`key in mydict` will be `False`...). # # This is only safe as the models are (mostly) frozen. assert ( key : = self . fingerprint . key ) is not None return key # Omitting unpassed args in repr by default def __repr_args__ ( self ) -> Sequence [ tuple [ Optional [ str ], Any ]]: return [( k , v ) for k , v in super () . __repr_args__ () if k in self . __fields_set__ ] def __str__ ( self ) -> str : return repr ( self ) class Config : extra = Extra . forbid frozen = True json_encoders = { frozendict : dict } keep_untouched = ( cached_property ,) smart_union = True validate_assignment = True # Unused with frozen, unless that is overridden in subclass. def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super () . copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value : = getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy @ staticmethod def _fingerprint_json_encoder ( obj : Any , encoder : Any = pydantic_json_encoder ) -> Any : from arti . fingerprints import Fingerprint if isinstance ( obj , Fingerprint ): return obj . key if isinstance ( obj , Model ): return obj . fingerprint if lenient_issubclass ( obj , Model ): return obj . _class_key_ # eg: View.artifact_class return encoder ( obj ) @ property def fingerprint ( self ) -> Fingerprint : from arti . fingerprints import Fingerprint # `.json` cannot be used, even with a custom encoder, because it calls `.dict`, which # converts the sub-models to dicts. Instead, we want to access `.fingerprint` (in the # decoder). data = dict ( sorted ( # Sort to ensure stability self . _iter ( exclude = self . _fingerprint_excludes_ , include = self . _fingerprint_includes_ , ), key = lambda kv : kv [ 0 ], ) ) json_repr = self . __config__ . json_dumps ( data , default = partial ( self . _fingerprint_json_encoder , encoder = self . __json_encoder__ ), sort_keys = True , ) return Fingerprint . from_string ( f \"{self._class_key_}:{json_repr}\" ) @ classmethod def _get_value ( cls , v : Any , to_dict : bool , by_alias : bool , include : Optional [ Union [ AbstractSetIntStr , MappingIntStrAny ]], exclude : Optional [ Union [ AbstractSetIntStr , MappingIntStrAny ]], exclude_unset : bool , exclude_defaults : bool , exclude_none : bool , ) -> Any : new = super () . _get_value ( v , to_dict = to_dict , by_alias = by_alias , include = include , exclude = exclude , exclude_unset = exclude_unset , exclude_defaults = exclude_defaults , exclude_none = exclude_none , ) # Copying dict subclasses doesn't preserve the subclass[1]. Further, we have extra Box # configuration (namely frozen_box=True) we need to preserve. # # 1: https://github.com/pydantic/pydantic/issues/5225 if isinstance ( v , Box ): return v . __class__ ( new , ** v . _Box__box_config ()) return new # Filter out non-fields from ._iter (and thus .dict, .json, etc), such as `@cached_property` # after access (which just gets cached in .__dict__). def _iter ( self , * args : Any , ** kwargs : Any ) -> Generator [ tuple [ str , Any ], None , None ]: for key , value in super () . _iter ( * args , ** kwargs ): if key in self . __fields__ : yield key , value @ classmethod def _pydantic_type_system_post_field_conversion_hook_ ( cls , type_ : Type , * , name : str , required : bool ) -> Type : return type_ Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.annotations.Annotation arti.types.Type arti.types._ContainerMixin arti.types._NamedMixin arti.types._TimeMixin arti.types.TypeSystem arti.formats.Format arti.statistics.Statistic arti.fingerprints.Fingerprint arti.partitions.PartitionKey arti.storage.StoragePartition arti.storage.Storage arti.artifacts.Artifact arti.backends.Backend arti.views.View arti.versions.Version arti.producers.Producer arti.producers.ProducerOutput arti.graphs.Graph arti.graphs.GraphSnapshot arti.executors.Executor arti.thresholds.Threshold arti.storage.google.cloud.storage._GCSMixin Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Models"},{"location":"reference/arti/internal/models/#module-artiinternalmodels","text":"None None View Source from __future__ import annotations from collections.abc import Generator , Mapping , Sequence from copy import deepcopy from functools import cached_property , partial from typing import ( TYPE_CHECKING , Annotated , Any , ClassVar , Literal , Optional , TypeVar , Union , get_args , get_origin , ) from box import Box from pydantic import BaseModel , Extra , root_validator , validator from pydantic.fields import ModelField , Undefined from pydantic.json import pydantic_encoder as pydantic_json_encoder from arti.internal.type_hints import Self , is_union , lenient_issubclass from arti.internal.utils import class_name , frozendict if TYPE_CHECKING : from pydantic.typing import AbstractSetIntStr , MappingIntStrAny from arti.fingerprints import Fingerprint from arti.types import Type def _check_types ( value : Any , type_ : type ) -> Any : mismatch_error = ValueError ( f \"expected an instance of { type_ } , got: { value } \" ) if type_ is Any : return value origin = get_origin ( type_ ) if origin is not None : args = get_args ( type_ ) if origin is Annotated : return _check_types ( value , args [ 0 ]) if origin is Literal : return _check_types ( value , type ( args [ 0 ])) # NOTE: Optional[t] -> Union[t, NoneType] if is_union ( origin ): for subtype in args : try : return _check_types ( value , subtype ) except ValueError : pass raise mismatch_error if issubclass ( origin , ( dict , Mapping )): value = _check_types ( value , origin ) for k , v in value . items (): _check_types ( k , args [ 0 ]) _check_types ( v , args [ 1 ]) return value # Variadic tuples will be handled below if issubclass ( origin , tuple ) and ... not in args : value = _check_types ( value , origin ) if len ( value ) != len ( args ): raise mismatch_error for i , subtype in enumerate ( args ): _check_types ( value [ i ], subtype ) return value for t in ( tuple , list , set , frozenset , Sequence ): if issubclass ( origin , t ): value = _check_types ( value , origin ) for subvalue in value : _check_types ( subvalue , args [ 0 ]) return value if issubclass ( origin , type ): if not lenient_issubclass ( value , args [ 0 ]): raise ValueError ( f \"expected a subclass of { args [ 0 ] } , got: { value } \" ) return value if set ( args ) == { Any }: return _check_types ( value , origin ) # pragma: no cover if isinstance ( value , origin ): # Other Generic args can't really be checked generally return value raise ValueError ( f \"expected a instance of { origin } , got: { value } \" ) # pragma: no cover if isinstance ( type_ , TypeVar ): return value # TODO: Check __bound__, __covariant__, __contravariant__ # Models are immutable, so we convert all Mappings to frozendicts. if isinstance ( value , Mapping ) and not isinstance ( value , frozendict ): value = frozendict ( value ) if lenient_issubclass ( type_ , Mapping ): type_ = frozendict if not lenient_issubclass ( type ( value ), type_ ): raise mismatch_error return value _Model = TypeVar ( \"_Model\" , bound = \"Model\" ) class Model ( BaseModel ): # A model can be marked _abstract_ to prevent direct instantiation, such as when it is intended # as a base class for other models with arbitrary data. As the subclasses of an _abstract_ model # have unknown fields (varying per subclass), we don't have targets to mark abstract with # abc.ABC nor typing.Protocol. See [1] for more context. # # 1: https://github.com/artigraph/artigraph/pull/60#discussion_r669089086 _abstract_ : ClassVar [ bool ] = True _class_key_ : ClassVar [ str ] = class_name () _fingerprint_excludes_ : ClassVar [ Optional [ frozenset [ str ]]] = None _fingerprint_includes_ : ClassVar [ Optional [ frozenset [ str ]]] = None @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) # Default _abstract_ to False if not set explicitly on the class. __dict__ is read-only. cls . _abstract_ = cls . __dict__ . get ( \"_abstract_\" , False ) field_names = set ( cls . __fields__ ) if cls . _fingerprint_excludes_ and ( unknown_excludes := cls . _fingerprint_excludes_ - field_names ): raise ValueError ( f \"Unknown `_fingerprint_excludes_` field(s): { unknown_excludes } \" ) if cls . _fingerprint_includes_ and ( unknown_includes := cls . _fingerprint_includes_ - field_names ): raise ValueError ( f \"Unknown `_fingerprint_includes_` field(s): { unknown_includes } \" ) @root_validator ( pre = True ) @classmethod def _block_abstract_instance ( cls , values : dict [ str , Any ]) -> dict [ str , Any ]: if cls . _abstract_ : raise ValueError ( f \" { cls } cannot be instantiated directly!\" ) return values @validator ( \"*\" , pre = True ) @classmethod def _strict_types ( cls , value : Any , field : ModelField ) -> Any : \"\"\"Check that the value is a stricter instance of the declared type annotation. Pydantic will attempt to *parse* values (eg: \"5\" -> 5), but we'd prefer stricter values for clarity and to avoid silent precision loss (eg: 5.3 -> 5). \"\"\" # `field.type_` points to the *inner* type (eg: `int`->`int`; `tuple[int, ...]` -> `int`) # while `field.outer_type_` will (mostly) include the full spec and match the `value` we # received. The caveat is the `field.outer_type_` will never be wrapped in `Optional` # (though nested fields like `tuple[tuple[Optional[int]]]` would). Hence, we pull the # `field.outer_type_`, but add back the `Optional` wrapping if necessary. type_ = field . outer_type_ if field . allow_none : type_ = Optional [ type_ ] return _check_types ( value , type_ ) # By default, pydantic just compares models by their dict representation, causing models of # different types but same fields (eg: Int8 and Int16) to be equivalent. This can be removed if # [1] is merged+released. # # 1: https://github.com/samuelcolvin/pydantic/pull/3066 def __eq__ ( self , other : Any ) -> bool : return self . __class__ == other . __class__ and tuple ( self . _iter ()) == tuple ( other . _iter ()) def __hash__ ( self ) -> int : # Override the default __hash__ to match the fingerprint, which notably excludes # `cached_property`s. # # If `cached_property`s are included, the hash will be different before and after caching, # which wrecks havoc if a model is a key in a dict (`key in mydict` will be `False`...). # # This is only safe as the models are (mostly) frozen. assert ( key := self . fingerprint . key ) is not None return key # Omitting unpassed args in repr by default def __repr_args__ ( self ) -> Sequence [ tuple [ Optional [ str ], Any ]]: return [( k , v ) for k , v in super () . __repr_args__ () if k in self . __fields_set__ ] def __str__ ( self ) -> str : return repr ( self ) class Config : extra = Extra . forbid frozen = True json_encoders = { frozendict : dict } keep_untouched = ( cached_property ,) smart_union = True validate_assignment = True # Unused with frozen, unless that is overridden in subclass. def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super () . copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy @staticmethod def _fingerprint_json_encoder ( obj : Any , encoder : Any = pydantic_json_encoder ) -> Any : from arti.fingerprints import Fingerprint if isinstance ( obj , Fingerprint ): return obj . key if isinstance ( obj , Model ): return obj . fingerprint if lenient_issubclass ( obj , Model ): return obj . _class_key_ # eg: View.artifact_class return encoder ( obj ) @property def fingerprint ( self ) -> Fingerprint : from arti.fingerprints import Fingerprint # `.json` cannot be used, even with a custom encoder, because it calls `.dict`, which # converts the sub-models to dicts. Instead, we want to access `.fingerprint` (in the # decoder). data = dict ( sorted ( # Sort to ensure stability self . _iter ( exclude = self . _fingerprint_excludes_ , include = self . _fingerprint_includes_ , ), key = lambda kv : kv [ 0 ], ) ) json_repr = self . __config__ . json_dumps ( data , default = partial ( self . _fingerprint_json_encoder , encoder = self . __json_encoder__ ), sort_keys = True , ) return Fingerprint . from_string ( f \" { self . _class_key_ } : { json_repr } \" ) @classmethod def _get_value ( cls , v : Any , to_dict : bool , by_alias : bool , include : Optional [ Union [ AbstractSetIntStr , MappingIntStrAny ]], exclude : Optional [ Union [ AbstractSetIntStr , MappingIntStrAny ]], exclude_unset : bool , exclude_defaults : bool , exclude_none : bool , ) -> Any : new = super () . _get_value ( v , to_dict = to_dict , by_alias = by_alias , include = include , exclude = exclude , exclude_unset = exclude_unset , exclude_defaults = exclude_defaults , exclude_none = exclude_none , ) # Copying dict subclasses doesn't preserve the subclass[1]. Further, we have extra Box # configuration (namely frozen_box=True) we need to preserve. # # 1: https://github.com/pydantic/pydantic/issues/5225 if isinstance ( v , Box ): return v . __class__ ( new , ** v . _Box__box_config ()) return new # Filter out non-fields from ._iter (and thus .dict, .json, etc), such as `@cached_property` # after access (which just gets cached in .__dict__). def _iter ( self , * args : Any , ** kwargs : Any ) -> Generator [ tuple [ str , Any ], None , None ]: for key , value in super () . _iter ( * args , ** kwargs ): if key in self . __fields__ : yield key , value @classmethod def _pydantic_type_system_post_field_conversion_hook_ ( cls , type_ : Type , * , name : str , required : bool ) -> Type : return type_ def get_field_default ( model : type [ Model ], field : str ) -> Optional [ Any ]: return model . __fields__ [ field ] . default","title":"Module arti.internal.models"},{"location":"reference/arti/internal/models/#variables","text":"TYPE_CHECKING","title":"Variables"},{"location":"reference/arti/internal/models/#functions","text":"","title":"Functions"},{"location":"reference/arti/internal/models/#get_field_default","text":"def get_field_default ( model : 'type[Model]' , field : 'str' ) -> 'Optional[Any]' View Source def get_field_default ( model : type [ Model ] , field : str ) -> Optional [ Any ] : return model . __fields__ [ field ] . default","title":"get_field_default"},{"location":"reference/arti/internal/models/#classes","text":"","title":"Classes"},{"location":"reference/arti/internal/models/#model","text":"class Model ( __pydantic_self__ , ** data : Any ) View Source class Model ( BaseModel ): # A model can be marked _abstract_ to prevent direct instantiation, such as when it is intended # as a base class for other models with arbitrary data. As the subclasses of an _abstract_ model # have unknown fields (varying per subclass), we don't have targets to mark abstract with # abc.ABC nor typing.Protocol. See [1] for more context. # # 1: https://github.com/artigraph/artigraph/pull/60#discussion_r669089086 _abstract_ : ClassVar [ bool ] = True _class_key_ : ClassVar [ str ] = class_name () _fingerprint_excludes_ : ClassVar [ Optional [ frozenset [ str ]]] = None _fingerprint_includes_ : ClassVar [ Optional [ frozenset [ str ]]] = None @ classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) # Default _abstract_ to False if not set explicitly on the class. __dict__ is read-only. cls . _abstract_ = cls . __dict__ . get ( \"_abstract_\" , False ) field_names = set ( cls . __fields__ ) if cls . _fingerprint_excludes_ and ( unknown_excludes : = cls . _fingerprint_excludes_ - field_names ): raise ValueError ( f \"Unknown `_fingerprint_excludes_` field(s): {unknown_excludes}\" ) if cls . _fingerprint_includes_ and ( unknown_includes : = cls . _fingerprint_includes_ - field_names ): raise ValueError ( f \"Unknown `_fingerprint_includes_` field(s): {unknown_includes}\" ) @ root_validator ( pre = True ) @ classmethod def _block_abstract_instance ( cls , values : dict [ str , Any ]) -> dict [ str , Any ]: if cls . _abstract_ : raise ValueError ( f \"{cls} cannot be instantiated directly!\" ) return values @ validator ( \"*\" , pre = True ) @ classmethod def _strict_types ( cls , value : Any , field : ModelField ) -> Any : \"\"\"Check that the value is a stricter instance of the declared type annotation. Pydantic will attempt to *parse* values (eg: \"5\" -> 5), but we'd prefer stricter values for clarity and to avoid silent precision loss (eg: 5.3 -> 5). \"\"\" # `field.type_` points to the *inner* type (eg: `int`->`int`; `tuple[int, ...]` -> `int`) # while `field.outer_type_` will (mostly) include the full spec and match the `value` we # received. The caveat is the `field.outer_type_` will never be wrapped in `Optional` # (though nested fields like `tuple[tuple[Optional[int]]]` would). Hence, we pull the # `field.outer_type_`, but add back the `Optional` wrapping if necessary. type_ = field . outer_type_ if field . allow_none : type_ = Optional [ type_ ] return _check_types ( value , type_ ) # By default, pydantic just compares models by their dict representation, causing models of # different types but same fields (eg: Int8 and Int16) to be equivalent. This can be removed if # [1] is merged+released. # # 1: https://github.com/samuelcolvin/pydantic/pull/3066 def __eq__ ( self , other : Any ) -> bool : return self . __class__ == other . __class__ and tuple ( self . _iter ()) == tuple ( other . _iter ()) def __hash__ ( self ) -> int : # Override the default __hash__ to match the fingerprint, which notably excludes # `cached_property`s. # # If `cached_property`s are included, the hash will be different before and after caching, # which wrecks havoc if a model is a key in a dict (`key in mydict` will be `False`...). # # This is only safe as the models are (mostly) frozen. assert ( key : = self . fingerprint . key ) is not None return key # Omitting unpassed args in repr by default def __repr_args__ ( self ) -> Sequence [ tuple [ Optional [ str ], Any ]]: return [( k , v ) for k , v in super () . __repr_args__ () if k in self . __fields_set__ ] def __str__ ( self ) -> str : return repr ( self ) class Config : extra = Extra . forbid frozen = True json_encoders = { frozendict : dict } keep_untouched = ( cached_property ,) smart_union = True validate_assignment = True # Unused with frozen, unless that is overridden in subclass. def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super () . copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value : = getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy @ staticmethod def _fingerprint_json_encoder ( obj : Any , encoder : Any = pydantic_json_encoder ) -> Any : from arti . fingerprints import Fingerprint if isinstance ( obj , Fingerprint ): return obj . key if isinstance ( obj , Model ): return obj . fingerprint if lenient_issubclass ( obj , Model ): return obj . _class_key_ # eg: View.artifact_class return encoder ( obj ) @ property def fingerprint ( self ) -> Fingerprint : from arti . fingerprints import Fingerprint # `.json` cannot be used, even with a custom encoder, because it calls `.dict`, which # converts the sub-models to dicts. Instead, we want to access `.fingerprint` (in the # decoder). data = dict ( sorted ( # Sort to ensure stability self . _iter ( exclude = self . _fingerprint_excludes_ , include = self . _fingerprint_includes_ , ), key = lambda kv : kv [ 0 ], ) ) json_repr = self . __config__ . json_dumps ( data , default = partial ( self . _fingerprint_json_encoder , encoder = self . __json_encoder__ ), sort_keys = True , ) return Fingerprint . from_string ( f \"{self._class_key_}:{json_repr}\" ) @ classmethod def _get_value ( cls , v : Any , to_dict : bool , by_alias : bool , include : Optional [ Union [ AbstractSetIntStr , MappingIntStrAny ]], exclude : Optional [ Union [ AbstractSetIntStr , MappingIntStrAny ]], exclude_unset : bool , exclude_defaults : bool , exclude_none : bool , ) -> Any : new = super () . _get_value ( v , to_dict = to_dict , by_alias = by_alias , include = include , exclude = exclude , exclude_unset = exclude_unset , exclude_defaults = exclude_defaults , exclude_none = exclude_none , ) # Copying dict subclasses doesn't preserve the subclass[1]. Further, we have extra Box # configuration (namely frozen_box=True) we need to preserve. # # 1: https://github.com/pydantic/pydantic/issues/5225 if isinstance ( v , Box ): return v . __class__ ( new , ** v . _Box__box_config ()) return new # Filter out non-fields from ._iter (and thus .dict, .json, etc), such as `@cached_property` # after access (which just gets cached in .__dict__). def _iter ( self , * args : Any , ** kwargs : Any ) -> Generator [ tuple [ str , Any ], None , None ]: for key , value in super () . _iter ( * args , ** kwargs ): if key in self . __fields__ : yield key , value @ classmethod def _pydantic_type_system_post_field_conversion_hook_ ( cls , type_ : Type , * , name : str , required : bool ) -> Type : return type_","title":"Model"},{"location":"reference/arti/internal/models/#ancestors-in-mro","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/internal/models/#descendants","text":"arti.annotations.Annotation arti.types.Type arti.types._ContainerMixin arti.types._NamedMixin arti.types._TimeMixin arti.types.TypeSystem arti.formats.Format arti.statistics.Statistic arti.fingerprints.Fingerprint arti.partitions.PartitionKey arti.storage.StoragePartition arti.storage.Storage arti.artifacts.Artifact arti.backends.Backend arti.views.View arti.versions.Version arti.producers.Producer arti.producers.ProducerOutput arti.graphs.Graph arti.graphs.GraphSnapshot arti.executors.Executor arti.thresholds.Threshold arti.storage.google.cloud.storage._GCSMixin","title":"Descendants"},{"location":"reference/arti/internal/models/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/internal/models/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/internal/models/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/internal/models/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/internal/models/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/internal/models/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/internal/models/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/internal/models/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/internal/models/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/internal/models/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/internal/models/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/internal/models/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/internal/models/#methods","text":"","title":"Methods"},{"location":"reference/arti/internal/models/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/internal/models/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/internal/models/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/internal/patches/","text":"Module arti.internal.patches None None View Source from __future__ import annotations from typing import no_type_check @no_type_check # The mypy errors vary based on python version (and we error for unused ignores), so just skip completely def patch_TopologicalSorter_class_getitem () -> None : \"\"\"Patch adding TopologicalSorter.__class_getitem__ to support subscription. TopologicalSorter is considered Generic in typeshed (hence mypy expects a type arg), but is not at runtime. This has been fixed for 3.11+ (https://github.com/python/cpython/pull/28714). \"\"\" from graphlib import TopologicalSorter from types import GenericAlias if not hasattr ( TopologicalSorter , \"__class_getitem__\" ): # pragma: no cover TopologicalSorter . __class_getitem__ = classmethod ( GenericAlias ) Functions patch_TopologicalSorter_class_getitem def patch_TopologicalSorter_class_getitem ( ) -> 'None' Patch adding TopologicalSorter. class_getitem to support subscription. TopologicalSorter is considered Generic in typeshed (hence mypy expects a type arg), but is not at runtime. This has been fixed for 3.11+ (https://github.com/python/cpython/pull/28714). View Source @no_type_check # The mypy errors vary based on python version (and we error for unused ignores), so just skip completely def patch_TopologicalSorter_class_getitem () -> None : \"\"\"Patch adding TopologicalSorter.__class_getitem__ to support subscription. TopologicalSorter is considered Generic in typeshed (hence mypy expects a type arg), but is not at runtime. This has been fixed for 3.11+ (https://github.com/python/cpython/pull/28714). \"\"\" from graphlib import TopologicalSorter from types import GenericAlias if not hasattr ( TopologicalSorter , \"__class_getitem__\" ): # pragma: no cover TopologicalSorter . __class_getitem__ = classmethod ( GenericAlias )","title":"Patches"},{"location":"reference/arti/internal/patches/#module-artiinternalpatches","text":"None None View Source from __future__ import annotations from typing import no_type_check @no_type_check # The mypy errors vary based on python version (and we error for unused ignores), so just skip completely def patch_TopologicalSorter_class_getitem () -> None : \"\"\"Patch adding TopologicalSorter.__class_getitem__ to support subscription. TopologicalSorter is considered Generic in typeshed (hence mypy expects a type arg), but is not at runtime. This has been fixed for 3.11+ (https://github.com/python/cpython/pull/28714). \"\"\" from graphlib import TopologicalSorter from types import GenericAlias if not hasattr ( TopologicalSorter , \"__class_getitem__\" ): # pragma: no cover TopologicalSorter . __class_getitem__ = classmethod ( GenericAlias )","title":"Module arti.internal.patches"},{"location":"reference/arti/internal/patches/#functions","text":"","title":"Functions"},{"location":"reference/arti/internal/patches/#patch_topologicalsorter_class_getitem","text":"def patch_TopologicalSorter_class_getitem ( ) -> 'None' Patch adding TopologicalSorter. class_getitem to support subscription. TopologicalSorter is considered Generic in typeshed (hence mypy expects a type arg), but is not at runtime. This has been fixed for 3.11+ (https://github.com/python/cpython/pull/28714). View Source @no_type_check # The mypy errors vary based on python version (and we error for unused ignores), so just skip completely def patch_TopologicalSorter_class_getitem () -> None : \"\"\"Patch adding TopologicalSorter.__class_getitem__ to support subscription. TopologicalSorter is considered Generic in typeshed (hence mypy expects a type arg), but is not at runtime. This has been fixed for 3.11+ (https://github.com/python/cpython/pull/28714). \"\"\" from graphlib import TopologicalSorter from types import GenericAlias if not hasattr ( TopologicalSorter , \"__class_getitem__\" ): # pragma: no cover TopologicalSorter . __class_getitem__ = classmethod ( GenericAlias )","title":"patch_TopologicalSorter_class_getitem"},{"location":"reference/arti/internal/type_hints/","text":"Module arti.internal.type_hints None None View Source from __future__ import annotations import inspect import sys import types from collections.abc import Callable from datetime import date , datetime from typing import ( TYPE_CHECKING , Annotated , Any , Literal , Optional , TypeVar , Union , cast , get_args , get_origin , get_type_hints , overload , ) _T = TypeVar ( \"_T\" ) NoneType = cast ( type , type ( None )) # mypy otherwise treats type(None) as an object def _check_issubclass ( klass : Any , check_type : type ) -> bool : # If a hint is Annotated, we want to unwrap the underlying type and discard the rest of the # metadata. klass = discard_Annotated ( klass ) klass_origin , klass_args = get_origin ( klass ), get_args ( klass ) if isinstance ( klass , TypeVar ): klass = cast ( type , Any ) if klass . __bound__ is None else klass . __bound__ klass_origin , klass_args = get_origin ( klass ), get_args ( klass ) check_type_origin , check_type_args = get_origin ( check_type ), get_args ( check_type ) if check_type_origin is Annotated : check_type = check_type_args [ 0 ] check_type_origin , check_type_args = get_origin ( check_type ), get_args ( check_type ) if isinstance ( check_type , TypeVar ): check_type = cast ( type , Any ) if check_type . __bound__ is None else check_type . __bound__ check_type_origin , check_type_args = get_origin ( check_type ), get_args ( check_type ) if klass is Any : return check_type is Any if check_type is Any : return True if check_type is None : return klass is NoneType # eg: issubclass(tuple, tuple) if klass_origin is None and check_type_origin is None : return issubclass ( klass , check_type ) # eg: issubclass(tuple[int], tuple) if klass_origin is not None and check_type_origin is None : return issubclass ( klass_origin , check_type ) # eg: issubclass(tuple, tuple[int]) if klass_origin is None and check_type_origin is not None : return issubclass ( klass , check_type_origin ) and not check_type_args # eg: issubclass(tuple[int], tuple[int]) if klass_origin is not None and check_type_origin is not None : # NOTE: Considering all container types covariant for simplicity (mypy may be more strict). # # The builtin mutable containers (list, dict, etc) are invariant (klass_args == # check_type_args), but the interfaces (Mapping, Sequence, etc) and immutable containers are # covariant. if check_type_args and not ( len ( klass_args ) == len ( check_type_args ) and all ( # check subclass OR things like \"...\" lenient_issubclass ( klass_arg , check_type_arg ) or klass_arg is check_type_arg for ( klass_arg , check_type_arg ) in zip ( klass_args , check_type_args ) ) ): return False return lenient_issubclass ( klass_origin , check_type_origin ) # Shouldn't happen, but need to explicitly say \"x is not None\" to narrow mypy types. raise NotImplementedError ( \"The origin conditions don't cover all cases!\" ) def discard_Annotated ( type_ : Any ) -> Any : return get_args ( type_ )[ 0 ] if is_Annotated ( type_ ) else type_ def get_class_type_vars ( klass : type ) -> tuple [ type , ... ]: \"\"\"Get the bound type variables from a class NOTE: Only vars from the *first* Generic in the mro *with all variables bound* will be returned. \"\"\" bases = ( klass ,) if is_generic_alias ( klass ) else klass . __orig_bases__ # type: ignore[attr-defined] for base in bases : base_origin = get_origin ( base ) if base_origin is None : continue args = get_args ( base ) if any ( isinstance ( arg , TypeVar ) for arg in args ): continue return args raise TypeError ( f \" { klass . __name__ } must subclass a subscripted Generic\" ) @overload def get_item_from_annotated ( annotation : Any , klass : type [ _T ], * , is_subclass : Literal [ True ] ) -> Optional [ type [ _T ]]: ... @overload def get_item_from_annotated ( annotation : Any , klass : type [ _T ], * , is_subclass : Literal [ False ] ) -> Optional [ _T ]: ... @overload def get_item_from_annotated ( annotation : Any , klass : type [ _T ], * , is_subclass : bool ) -> Optional [ Union [ _T , type [ _T ]]]: ... def get_item_from_annotated ( annotation : Any , klass : type [ _T ], * , is_subclass : bool ) -> Optional [ Union [ _T , type [ _T ]]]: from arti.internal.utils import one_or_none if not is_Annotated ( annotation ): return None _ , * hints = get_args ( annotation ) checker = lenient_issubclass if is_subclass else isinstance return one_or_none ([ hint for hint in hints if checker ( hint , klass )], item_name = klass . __name__ ) def get_annotation_from_value ( value : Any ) -> Any : if value is None : return None if isinstance ( value , ( bool , bytes , date , datetime , float , int , str )): return type ( value ) if isinstance ( value , ( tuple , list , set , frozenset )): first , * tail = tuple ( value ) first_type = type ( first ) if all ( isinstance ( v , first_type ) for v in tail ): if isinstance ( value , tuple ): return tuple [ first_type , ... ] # type: ignore[valid-type] return type ( value )[ first_type ] # type: ignore[index] if isinstance ( value , dict ): items = value . items () first_key_type , first_value_type = ( type ( v ) for v in tuple ( items )[ 0 ]) if all ( isinstance ( k , first_key_type ) and isinstance ( v , first_value_type ) for ( k , v ) in items ): return dict [ first_key_type , first_value_type ] # type: ignore[valid-type] # TODO: Implement with TypedDict to support Struct types...? raise NotImplementedError ( f \"Unable to determine type of { value } \" ) def lenient_issubclass ( klass : Any , class_or_tuple : Union [ type , tuple [ type , ... ]]) -> bool : if not ( isinstance ( klass , ( type , types . GenericAlias , TypeVar )) or is_Annotated ( klass ) or klass is Any ): return False if isinstance ( class_or_tuple , tuple ): return any ( lenient_issubclass ( klass , subtype ) for subtype in class_or_tuple ) check_type = class_or_tuple # NOTE: py 3.10 supports issubclass with Unions (eg: `issubclass(str, str | int)`) if is_union_hint ( check_type ): return any ( lenient_issubclass ( klass , subtype ) for subtype in get_args ( check_type )) return _check_issubclass ( klass , check_type ) def _tidy_return ( return_annotation : Any , * , force_tuple_return : bool ) -> Any : if not force_tuple_return : return return_annotation if lenient_issubclass ( get_origin ( return_annotation ), tuple ): return get_args ( return_annotation ) return ( return_annotation ,) def tidy_signature ( fn : Callable [ ... , Any ], sig : inspect . Signature , * , force_tuple_return : bool = False , remove_owner : bool = False , ) -> inspect . Signature : type_hints = get_type_hints ( fn , include_extras = True ) sig = sig . replace ( return_annotation = type_hints . get ( \"return\" , sig . return_annotation )) return sig . replace ( parameters = [ p . replace ( annotation = type_hints . get ( p . name , p . annotation )) for p in sig . parameters . values () if ( p . name not in ( \"cls\" , \"self\" ) if remove_owner else True ) ], return_annotation = ( sig . empty if sig . return_annotation is sig . empty else _tidy_return ( sig . return_annotation , force_tuple_return = force_tuple_return ) ), ) def signature ( fn : Callable [ ... , Any ], * , follow_wrapped : bool = True , force_tuple_return : bool = False , remove_owner : bool = True , ) -> inspect . Signature : \"\"\"Convenience wrapper around `inspect.signature`. The returned Signature will have `cls`/`self` parameters removed if `remove_owner` is `True` and `tuple[...]` converted to `tuple(...)` in the `return_annotation`. \"\"\" return tidy_signature ( fn = fn , sig = inspect . signature ( fn , follow_wrapped = follow_wrapped ), force_tuple_return = force_tuple_return , remove_owner = remove_owner , ) ############################################# # Helpers for typing across python versions # ############################################# # # Focusing on 3.9+ (for now) if sys . version_info < ( 3 , 11 ): # pragma: no cover from typing_extensions import Self as Self else : # pragma: no cover from typing import Self as Self if sys . version_info < ( 3 , 10 ): # pragma: no cover def is_union ( type_ : Any ) -> bool : return type_ is Union def is_typeddict ( type_ : Any ) -> bool : # mypy doesn't know of typing._TypedDictMeta, but `type: ignore` would be \"unused\" (and error) # on other python versions. if TYPE_CHECKING : from typing import _TypedDict as _TypedDictMeta else : from typing import _TypedDictMeta return isinstance ( type_ , _TypedDictMeta ) else : # pragma: no cover from typing import is_typeddict as is_typeddict # noqa: F401 # mypy doesn't know of types.UnionType yet, but `type: ignore` would be \"unused\" # (and error) on other python versions. def is_union ( type_ : Any ) -> bool : # `Union[int, str]` or `int | str` return type_ is Union or type_ is types . UnionType # noqa: E721 def is_Annotated ( type_ : Any ) -> bool : return get_origin ( type_ ) is Annotated def is_generic_alias ( type_ : Any ) -> bool : from typing import _GenericAlias # type: ignore[attr-defined] return isinstance ( type_ , ( _GenericAlias , types . GenericAlias )) def is_optional_hint ( type_ : Any ) -> bool : # Optional[x] is represented as Union[x, NoneType] return is_union ( get_origin ( type_ )) and NoneType in get_args ( type_ ) def is_union_hint ( type_ : Any ) -> bool : return get_origin ( type_ ) is Union Variables TYPE_CHECKING Functions discard_Annotated def discard_Annotated ( type_ : 'Any' ) -> 'Any' View Source def discard_Annotated ( type_ : Any ) -> Any : return get_args ( type_ )[ 0 ] if is_Annotated ( type_ ) else type_ get_annotation_from_value def get_annotation_from_value ( value : 'Any' ) -> 'Any' View Source def get_annotation_from_value ( value : Any ) -> Any : if value is None : return None if isinstance ( value , ( bool , bytes , date , datetime , float , int , str )) : return type ( value ) if isinstance ( value , ( tuple , list , set , frozenset )) : first , * tail = tuple ( value ) first_type = type ( first ) if all ( isinstance ( v , first_type ) for v in tail ) : if isinstance ( value , tuple ) : return tuple [ first_type, ... ] # type : ignore [ valid-type ] return type ( value ) [ first_type ] # type : ignore [ index ] if isinstance ( value , dict ) : items = value . items () first_key_type , first_value_type = ( type ( v ) for v in tuple ( items ) [ 0 ] ) if all ( isinstance ( k , first_key_type ) and isinstance ( v , first_value_type ) for ( k , v ) in items ) : return dict [ first_key_type, first_value_type ] # type : ignore [ valid-type ] # TODO : Implement with TypedDict to support Struct types ... ? raise NotImplementedError ( f \"Unable to determine type of {value}\" ) get_class_type_vars def get_class_type_vars ( klass : 'type' ) -> 'tuple[type, ...]' Get the bound type variables from a class NOTE: Only vars from the first Generic in the mro with all variables bound will be returned. View Source def get_class_type_vars ( klass : type ) -> tuple [ type , ... ]: \"\"\"Get the bound type variables from a class NOTE: Only vars from the *first* Generic in the mro *with all variables bound* will be returned. \"\"\" bases = ( klass ,) if is_generic_alias ( klass ) else klass . __orig_bases__ # type: ignore[attr-defined] for base in bases : base_origin = get_origin ( base ) if base_origin is None : continue args = get_args ( base ) if any ( isinstance ( arg , TypeVar ) for arg in args ): continue return args raise TypeError ( f \"{klass.__name__} must subclass a subscripted Generic\" ) get_item_from_annotated def get_item_from_annotated ( annotation : 'Any' , klass : 'type[_T]' , * , is_subclass : 'bool' ) -> 'Optional[Union[_T, type[_T]]]' View Source def get_item_from_annotated ( annotation : Any , klass : type [ _T ], * , is_subclass : bool ) -> Optional [ Union [ _T , type [ _T ]]]: from arti.internal.utils import one_or_none if not is_Annotated ( annotation ): return None _ , * hints = get_args ( annotation ) checker = lenient_issubclass if is_subclass else isinstance return one_or_none ([ hint for hint in hints if checker ( hint , klass )], item_name = klass . __name__ ) is_Annotated def is_Annotated ( type_ : 'Any' ) -> 'bool' View Source def is_Annotated ( type_ : Any ) -> bool : return get_origin ( type_ ) is Annotated is_generic_alias def is_generic_alias ( type_ : 'Any' ) -> 'bool' View Source def is_generic_alias ( type_ : Any ) -> bool : from typing import _GenericAlias # type: ignore[attr-defined] return isinstance ( type_ , ( _GenericAlias , types . GenericAlias )) is_optional_hint def is_optional_hint ( type_ : 'Any' ) -> 'bool' View Source def is_optional_hint ( type_ : Any ) -> bool : # Optional [ x ] is represented as Union [ x, NoneType ] return is_union ( get_origin ( type_ )) and NoneType in get_args ( type_ ) is_union def is_union ( type_ : 'Any' ) -> 'bool' View Source def is_union ( type_ : Any ) -> bool : # ` Union [ int , str ] ` or ` int | str ` return type_ is Union or type_ is types . UnionType # noqa : E721 is_union_hint def is_union_hint ( type_ : 'Any' ) -> 'bool' View Source def is_union_hint ( type_ : Any ) -> bool : return get_origin ( type_ ) is Union lenient_issubclass def lenient_issubclass ( klass : 'Any' , class_or_tuple : 'Union[type, tuple[type, ...]]' ) -> 'bool' View Source def lenient_issubclass ( klass : Any , class_or_tuple : Union [ type , tuple [ type , ...]]) -> bool : if not ( isinstance ( klass , ( type , types . GenericAlias , TypeVar )) or is_Annotated ( klass ) or klass is Any ) : return False if isinstance ( class_or_tuple , tuple ) : return any ( lenient_issubclass ( klass , subtype ) for subtype in class_or_tuple ) check_type = class_or_tuple # NOTE : py 3.10 supports issubclass with Unions ( eg : ` issubclass ( str , str | int ) ` ) if is_union_hint ( check_type ) : return any ( lenient_issubclass ( klass , subtype ) for subtype in get_args ( check_type )) return _check_issubclass ( klass , check_type ) signature def signature ( fn : 'Callable[..., Any]' , * , follow_wrapped : 'bool' = True , force_tuple_return : 'bool' = False , remove_owner : 'bool' = True ) -> 'inspect.Signature' Convenience wrapper around inspect.signature . The returned Signature will have cls / self parameters removed if remove_owner is True and tuple[...] converted to tuple(...) in the return_annotation . View Source def signature ( fn : Callable [ ..., Any ] , * , follow_wrapped : bool = True , force_tuple_return : bool = False , remove_owner : bool = True , ) -> inspect . Signature : \" \"\" Convenience wrapper around `inspect.signature`. The returned Signature will have `cls`/`self` parameters removed if `remove_owner` is `True` and `tuple[...]` converted to `tuple(...)` in the `return_annotation`. \"\" \" return tidy_signature ( fn = fn , sig = inspect . signature ( fn , follow_wrapped = follow_wrapped ), force_tuple_return = force_tuple_return , remove_owner = remove_owner , ) tidy_signature def tidy_signature ( fn : 'Callable[..., Any]' , sig : 'inspect.Signature' , * , force_tuple_return : 'bool' = False , remove_owner : 'bool' = False ) -> 'inspect.Signature' View Source def tidy_signature ( fn : Callable [..., Any ], sig : inspect . Signature , * , force_tuple_return : bool = False , remove_owner : bool = False , ) -> inspect . Signature : type_hints = get_type_hints ( fn , include_extras = True ) sig = sig . replace ( return_annotation = type_hints . get ( \"return\" , sig . return_annotation )) return sig . replace ( parameters = [ p . replace ( annotation = type_hints . get ( p . name , p . annotation )) for p in sig . parameters . values () if ( p . name not in ( \"cls\" , \"self\" ) if remove_owner else True ) ], return_annotation = ( sig . empty if sig . return_annotation is sig . empty else _tidy_return ( sig . return_annotation , force_tuple_return = force_tuple_return ) ), ) Classes NoneType class NoneType ( / , * args , ** kwargs )","title":"Type Hints"},{"location":"reference/arti/internal/type_hints/#module-artiinternaltype_hints","text":"None None View Source from __future__ import annotations import inspect import sys import types from collections.abc import Callable from datetime import date , datetime from typing import ( TYPE_CHECKING , Annotated , Any , Literal , Optional , TypeVar , Union , cast , get_args , get_origin , get_type_hints , overload , ) _T = TypeVar ( \"_T\" ) NoneType = cast ( type , type ( None )) # mypy otherwise treats type(None) as an object def _check_issubclass ( klass : Any , check_type : type ) -> bool : # If a hint is Annotated, we want to unwrap the underlying type and discard the rest of the # metadata. klass = discard_Annotated ( klass ) klass_origin , klass_args = get_origin ( klass ), get_args ( klass ) if isinstance ( klass , TypeVar ): klass = cast ( type , Any ) if klass . __bound__ is None else klass . __bound__ klass_origin , klass_args = get_origin ( klass ), get_args ( klass ) check_type_origin , check_type_args = get_origin ( check_type ), get_args ( check_type ) if check_type_origin is Annotated : check_type = check_type_args [ 0 ] check_type_origin , check_type_args = get_origin ( check_type ), get_args ( check_type ) if isinstance ( check_type , TypeVar ): check_type = cast ( type , Any ) if check_type . __bound__ is None else check_type . __bound__ check_type_origin , check_type_args = get_origin ( check_type ), get_args ( check_type ) if klass is Any : return check_type is Any if check_type is Any : return True if check_type is None : return klass is NoneType # eg: issubclass(tuple, tuple) if klass_origin is None and check_type_origin is None : return issubclass ( klass , check_type ) # eg: issubclass(tuple[int], tuple) if klass_origin is not None and check_type_origin is None : return issubclass ( klass_origin , check_type ) # eg: issubclass(tuple, tuple[int]) if klass_origin is None and check_type_origin is not None : return issubclass ( klass , check_type_origin ) and not check_type_args # eg: issubclass(tuple[int], tuple[int]) if klass_origin is not None and check_type_origin is not None : # NOTE: Considering all container types covariant for simplicity (mypy may be more strict). # # The builtin mutable containers (list, dict, etc) are invariant (klass_args == # check_type_args), but the interfaces (Mapping, Sequence, etc) and immutable containers are # covariant. if check_type_args and not ( len ( klass_args ) == len ( check_type_args ) and all ( # check subclass OR things like \"...\" lenient_issubclass ( klass_arg , check_type_arg ) or klass_arg is check_type_arg for ( klass_arg , check_type_arg ) in zip ( klass_args , check_type_args ) ) ): return False return lenient_issubclass ( klass_origin , check_type_origin ) # Shouldn't happen, but need to explicitly say \"x is not None\" to narrow mypy types. raise NotImplementedError ( \"The origin conditions don't cover all cases!\" ) def discard_Annotated ( type_ : Any ) -> Any : return get_args ( type_ )[ 0 ] if is_Annotated ( type_ ) else type_ def get_class_type_vars ( klass : type ) -> tuple [ type , ... ]: \"\"\"Get the bound type variables from a class NOTE: Only vars from the *first* Generic in the mro *with all variables bound* will be returned. \"\"\" bases = ( klass ,) if is_generic_alias ( klass ) else klass . __orig_bases__ # type: ignore[attr-defined] for base in bases : base_origin = get_origin ( base ) if base_origin is None : continue args = get_args ( base ) if any ( isinstance ( arg , TypeVar ) for arg in args ): continue return args raise TypeError ( f \" { klass . __name__ } must subclass a subscripted Generic\" ) @overload def get_item_from_annotated ( annotation : Any , klass : type [ _T ], * , is_subclass : Literal [ True ] ) -> Optional [ type [ _T ]]: ... @overload def get_item_from_annotated ( annotation : Any , klass : type [ _T ], * , is_subclass : Literal [ False ] ) -> Optional [ _T ]: ... @overload def get_item_from_annotated ( annotation : Any , klass : type [ _T ], * , is_subclass : bool ) -> Optional [ Union [ _T , type [ _T ]]]: ... def get_item_from_annotated ( annotation : Any , klass : type [ _T ], * , is_subclass : bool ) -> Optional [ Union [ _T , type [ _T ]]]: from arti.internal.utils import one_or_none if not is_Annotated ( annotation ): return None _ , * hints = get_args ( annotation ) checker = lenient_issubclass if is_subclass else isinstance return one_or_none ([ hint for hint in hints if checker ( hint , klass )], item_name = klass . __name__ ) def get_annotation_from_value ( value : Any ) -> Any : if value is None : return None if isinstance ( value , ( bool , bytes , date , datetime , float , int , str )): return type ( value ) if isinstance ( value , ( tuple , list , set , frozenset )): first , * tail = tuple ( value ) first_type = type ( first ) if all ( isinstance ( v , first_type ) for v in tail ): if isinstance ( value , tuple ): return tuple [ first_type , ... ] # type: ignore[valid-type] return type ( value )[ first_type ] # type: ignore[index] if isinstance ( value , dict ): items = value . items () first_key_type , first_value_type = ( type ( v ) for v in tuple ( items )[ 0 ]) if all ( isinstance ( k , first_key_type ) and isinstance ( v , first_value_type ) for ( k , v ) in items ): return dict [ first_key_type , first_value_type ] # type: ignore[valid-type] # TODO: Implement with TypedDict to support Struct types...? raise NotImplementedError ( f \"Unable to determine type of { value } \" ) def lenient_issubclass ( klass : Any , class_or_tuple : Union [ type , tuple [ type , ... ]]) -> bool : if not ( isinstance ( klass , ( type , types . GenericAlias , TypeVar )) or is_Annotated ( klass ) or klass is Any ): return False if isinstance ( class_or_tuple , tuple ): return any ( lenient_issubclass ( klass , subtype ) for subtype in class_or_tuple ) check_type = class_or_tuple # NOTE: py 3.10 supports issubclass with Unions (eg: `issubclass(str, str | int)`) if is_union_hint ( check_type ): return any ( lenient_issubclass ( klass , subtype ) for subtype in get_args ( check_type )) return _check_issubclass ( klass , check_type ) def _tidy_return ( return_annotation : Any , * , force_tuple_return : bool ) -> Any : if not force_tuple_return : return return_annotation if lenient_issubclass ( get_origin ( return_annotation ), tuple ): return get_args ( return_annotation ) return ( return_annotation ,) def tidy_signature ( fn : Callable [ ... , Any ], sig : inspect . Signature , * , force_tuple_return : bool = False , remove_owner : bool = False , ) -> inspect . Signature : type_hints = get_type_hints ( fn , include_extras = True ) sig = sig . replace ( return_annotation = type_hints . get ( \"return\" , sig . return_annotation )) return sig . replace ( parameters = [ p . replace ( annotation = type_hints . get ( p . name , p . annotation )) for p in sig . parameters . values () if ( p . name not in ( \"cls\" , \"self\" ) if remove_owner else True ) ], return_annotation = ( sig . empty if sig . return_annotation is sig . empty else _tidy_return ( sig . return_annotation , force_tuple_return = force_tuple_return ) ), ) def signature ( fn : Callable [ ... , Any ], * , follow_wrapped : bool = True , force_tuple_return : bool = False , remove_owner : bool = True , ) -> inspect . Signature : \"\"\"Convenience wrapper around `inspect.signature`. The returned Signature will have `cls`/`self` parameters removed if `remove_owner` is `True` and `tuple[...]` converted to `tuple(...)` in the `return_annotation`. \"\"\" return tidy_signature ( fn = fn , sig = inspect . signature ( fn , follow_wrapped = follow_wrapped ), force_tuple_return = force_tuple_return , remove_owner = remove_owner , ) ############################################# # Helpers for typing across python versions # ############################################# # # Focusing on 3.9+ (for now) if sys . version_info < ( 3 , 11 ): # pragma: no cover from typing_extensions import Self as Self else : # pragma: no cover from typing import Self as Self if sys . version_info < ( 3 , 10 ): # pragma: no cover def is_union ( type_ : Any ) -> bool : return type_ is Union def is_typeddict ( type_ : Any ) -> bool : # mypy doesn't know of typing._TypedDictMeta, but `type: ignore` would be \"unused\" (and error) # on other python versions. if TYPE_CHECKING : from typing import _TypedDict as _TypedDictMeta else : from typing import _TypedDictMeta return isinstance ( type_ , _TypedDictMeta ) else : # pragma: no cover from typing import is_typeddict as is_typeddict # noqa: F401 # mypy doesn't know of types.UnionType yet, but `type: ignore` would be \"unused\" # (and error) on other python versions. def is_union ( type_ : Any ) -> bool : # `Union[int, str]` or `int | str` return type_ is Union or type_ is types . UnionType # noqa: E721 def is_Annotated ( type_ : Any ) -> bool : return get_origin ( type_ ) is Annotated def is_generic_alias ( type_ : Any ) -> bool : from typing import _GenericAlias # type: ignore[attr-defined] return isinstance ( type_ , ( _GenericAlias , types . GenericAlias )) def is_optional_hint ( type_ : Any ) -> bool : # Optional[x] is represented as Union[x, NoneType] return is_union ( get_origin ( type_ )) and NoneType in get_args ( type_ ) def is_union_hint ( type_ : Any ) -> bool : return get_origin ( type_ ) is Union","title":"Module arti.internal.type_hints"},{"location":"reference/arti/internal/type_hints/#variables","text":"TYPE_CHECKING","title":"Variables"},{"location":"reference/arti/internal/type_hints/#functions","text":"","title":"Functions"},{"location":"reference/arti/internal/type_hints/#discard_annotated","text":"def discard_Annotated ( type_ : 'Any' ) -> 'Any' View Source def discard_Annotated ( type_ : Any ) -> Any : return get_args ( type_ )[ 0 ] if is_Annotated ( type_ ) else type_","title":"discard_Annotated"},{"location":"reference/arti/internal/type_hints/#get_annotation_from_value","text":"def get_annotation_from_value ( value : 'Any' ) -> 'Any' View Source def get_annotation_from_value ( value : Any ) -> Any : if value is None : return None if isinstance ( value , ( bool , bytes , date , datetime , float , int , str )) : return type ( value ) if isinstance ( value , ( tuple , list , set , frozenset )) : first , * tail = tuple ( value ) first_type = type ( first ) if all ( isinstance ( v , first_type ) for v in tail ) : if isinstance ( value , tuple ) : return tuple [ first_type, ... ] # type : ignore [ valid-type ] return type ( value ) [ first_type ] # type : ignore [ index ] if isinstance ( value , dict ) : items = value . items () first_key_type , first_value_type = ( type ( v ) for v in tuple ( items ) [ 0 ] ) if all ( isinstance ( k , first_key_type ) and isinstance ( v , first_value_type ) for ( k , v ) in items ) : return dict [ first_key_type, first_value_type ] # type : ignore [ valid-type ] # TODO : Implement with TypedDict to support Struct types ... ? raise NotImplementedError ( f \"Unable to determine type of {value}\" )","title":"get_annotation_from_value"},{"location":"reference/arti/internal/type_hints/#get_class_type_vars","text":"def get_class_type_vars ( klass : 'type' ) -> 'tuple[type, ...]' Get the bound type variables from a class NOTE: Only vars from the first Generic in the mro with all variables bound will be returned. View Source def get_class_type_vars ( klass : type ) -> tuple [ type , ... ]: \"\"\"Get the bound type variables from a class NOTE: Only vars from the *first* Generic in the mro *with all variables bound* will be returned. \"\"\" bases = ( klass ,) if is_generic_alias ( klass ) else klass . __orig_bases__ # type: ignore[attr-defined] for base in bases : base_origin = get_origin ( base ) if base_origin is None : continue args = get_args ( base ) if any ( isinstance ( arg , TypeVar ) for arg in args ): continue return args raise TypeError ( f \"{klass.__name__} must subclass a subscripted Generic\" )","title":"get_class_type_vars"},{"location":"reference/arti/internal/type_hints/#get_item_from_annotated","text":"def get_item_from_annotated ( annotation : 'Any' , klass : 'type[_T]' , * , is_subclass : 'bool' ) -> 'Optional[Union[_T, type[_T]]]' View Source def get_item_from_annotated ( annotation : Any , klass : type [ _T ], * , is_subclass : bool ) -> Optional [ Union [ _T , type [ _T ]]]: from arti.internal.utils import one_or_none if not is_Annotated ( annotation ): return None _ , * hints = get_args ( annotation ) checker = lenient_issubclass if is_subclass else isinstance return one_or_none ([ hint for hint in hints if checker ( hint , klass )], item_name = klass . __name__ )","title":"get_item_from_annotated"},{"location":"reference/arti/internal/type_hints/#is_annotated","text":"def is_Annotated ( type_ : 'Any' ) -> 'bool' View Source def is_Annotated ( type_ : Any ) -> bool : return get_origin ( type_ ) is Annotated","title":"is_Annotated"},{"location":"reference/arti/internal/type_hints/#is_generic_alias","text":"def is_generic_alias ( type_ : 'Any' ) -> 'bool' View Source def is_generic_alias ( type_ : Any ) -> bool : from typing import _GenericAlias # type: ignore[attr-defined] return isinstance ( type_ , ( _GenericAlias , types . GenericAlias ))","title":"is_generic_alias"},{"location":"reference/arti/internal/type_hints/#is_optional_hint","text":"def is_optional_hint ( type_ : 'Any' ) -> 'bool' View Source def is_optional_hint ( type_ : Any ) -> bool : # Optional [ x ] is represented as Union [ x, NoneType ] return is_union ( get_origin ( type_ )) and NoneType in get_args ( type_ )","title":"is_optional_hint"},{"location":"reference/arti/internal/type_hints/#is_union","text":"def is_union ( type_ : 'Any' ) -> 'bool' View Source def is_union ( type_ : Any ) -> bool : # ` Union [ int , str ] ` or ` int | str ` return type_ is Union or type_ is types . UnionType # noqa : E721","title":"is_union"},{"location":"reference/arti/internal/type_hints/#is_union_hint","text":"def is_union_hint ( type_ : 'Any' ) -> 'bool' View Source def is_union_hint ( type_ : Any ) -> bool : return get_origin ( type_ ) is Union","title":"is_union_hint"},{"location":"reference/arti/internal/type_hints/#lenient_issubclass","text":"def lenient_issubclass ( klass : 'Any' , class_or_tuple : 'Union[type, tuple[type, ...]]' ) -> 'bool' View Source def lenient_issubclass ( klass : Any , class_or_tuple : Union [ type , tuple [ type , ...]]) -> bool : if not ( isinstance ( klass , ( type , types . GenericAlias , TypeVar )) or is_Annotated ( klass ) or klass is Any ) : return False if isinstance ( class_or_tuple , tuple ) : return any ( lenient_issubclass ( klass , subtype ) for subtype in class_or_tuple ) check_type = class_or_tuple # NOTE : py 3.10 supports issubclass with Unions ( eg : ` issubclass ( str , str | int ) ` ) if is_union_hint ( check_type ) : return any ( lenient_issubclass ( klass , subtype ) for subtype in get_args ( check_type )) return _check_issubclass ( klass , check_type )","title":"lenient_issubclass"},{"location":"reference/arti/internal/type_hints/#signature","text":"def signature ( fn : 'Callable[..., Any]' , * , follow_wrapped : 'bool' = True , force_tuple_return : 'bool' = False , remove_owner : 'bool' = True ) -> 'inspect.Signature' Convenience wrapper around inspect.signature . The returned Signature will have cls / self parameters removed if remove_owner is True and tuple[...] converted to tuple(...) in the return_annotation . View Source def signature ( fn : Callable [ ..., Any ] , * , follow_wrapped : bool = True , force_tuple_return : bool = False , remove_owner : bool = True , ) -> inspect . Signature : \" \"\" Convenience wrapper around `inspect.signature`. The returned Signature will have `cls`/`self` parameters removed if `remove_owner` is `True` and `tuple[...]` converted to `tuple(...)` in the `return_annotation`. \"\" \" return tidy_signature ( fn = fn , sig = inspect . signature ( fn , follow_wrapped = follow_wrapped ), force_tuple_return = force_tuple_return , remove_owner = remove_owner , )","title":"signature"},{"location":"reference/arti/internal/type_hints/#tidy_signature","text":"def tidy_signature ( fn : 'Callable[..., Any]' , sig : 'inspect.Signature' , * , force_tuple_return : 'bool' = False , remove_owner : 'bool' = False ) -> 'inspect.Signature' View Source def tidy_signature ( fn : Callable [..., Any ], sig : inspect . Signature , * , force_tuple_return : bool = False , remove_owner : bool = False , ) -> inspect . Signature : type_hints = get_type_hints ( fn , include_extras = True ) sig = sig . replace ( return_annotation = type_hints . get ( \"return\" , sig . return_annotation )) return sig . replace ( parameters = [ p . replace ( annotation = type_hints . get ( p . name , p . annotation )) for p in sig . parameters . values () if ( p . name not in ( \"cls\" , \"self\" ) if remove_owner else True ) ], return_annotation = ( sig . empty if sig . return_annotation is sig . empty else _tidy_return ( sig . return_annotation , force_tuple_return = force_tuple_return ) ), )","title":"tidy_signature"},{"location":"reference/arti/internal/type_hints/#classes","text":"","title":"Classes"},{"location":"reference/arti/internal/type_hints/#nonetype","text":"class NoneType ( / , * args , ** kwargs )","title":"NoneType"},{"location":"reference/arti/internal/utils/","text":"Module arti.internal.utils None None View Source from __future__ import annotations import importlib import inspect import pkgutil import threading from collections.abc import Callable , Generator , Iterable , Iterator , Mapping , MutableMapping from contextlib import contextmanager from pathlib import Path from tempfile import TemporaryDirectory from types import GenericAlias , ModuleType from typing import IO , Any , ClassVar , Optional , SupportsIndex , TypeVar , Union , cast from box import Box from arti.internal.type_hints import Self from arti.internal.vendored.setuptools import find_namespace_packages _K = TypeVar ( \"_K\" ) _V = TypeVar ( \"_V\" ) class ClassName : def __get__ ( self , obj : Any , type_ : type [ Any ]) -> str : return type_ . __name__ class_name = cast ( Callable [[], str ], ClassName ) PropReturn = TypeVar ( \"PropReturn\" ) def classproperty ( meth : Callable [ ... , PropReturn ]) -> PropReturn : \"\"\"Access a @classmethod like a @property.\"\"\" # mypy doesn't understand class properties yet: https://github.com/python/mypy/issues/2563 return classmethod ( property ( meth )) # type: ignore[arg-type,return-value] class frozendict ( Mapping [ _K , _V ]): def __init__ ( self , arg : Union [ Mapping [ _K , _V ], Iterable [ tuple [ _K , _V ]]] = (), ** kwargs : _V ) -> None : self . _data = dict [ _K , _V ]( arg , ** kwargs ) # Eagerly evaluate the hash to confirm elements are also frozen (via frozenset) at # creation time, not just when hashed. self . _hash = hash ( frozenset ( self . _data . items ())) def __getitem__ ( self , key : _K ) -> _V : return self . _data [ key ] def __hash__ ( self ) -> int : return self . _hash def __iter__ ( self ) -> Iterator [ _K ]: return iter ( self . _data ) def __len__ ( self ) -> int : return len ( self . _data ) def __or__ ( self , other : Mapping [ _K , _V ]) -> frozendict [ _K , _V ]: return type ( self )({ ** self , ** other }) __ror__ = __or__ def __repr__ ( self ) -> str : return repr ( self . _data ) def get_module_name ( depth : int = 1 ) -> Optional [ str ]: \"\"\"Return the module name of a specific level in the stack. Depth describes how many levels to traverse, for example: - depth=0: return get_module_name's module - depth=1 (default): return the caller's module - depth=2: return the caller's calling module - ... \"\"\" frame = inspect . currentframe () if frame is None : # the interpreter doesn't support frame inspection return None # pragma: no cover for _ in range ( depth ): frame = frame . f_back if frame is None : return None return frame . f_globals . get ( \"__name__\" , \"__main__\" ) def import_submodules ( path : list [ str ], # module.__path__ is a list[str] name : str , * , lock : threading . Lock = threading . Lock (), ) -> dict [ str , ModuleType ]: \"\"\"Recursively import submodules. This can be useful with registry patterns to automatically discover and import submodules defining additional implementations. `path` and `name` are usually provided from an existing module's `__path__` and `__name__`. This function is thread-safe and supports namespace modules. NOTE: This inherently triggers eager imports, which has performance impacts and may cause import cycles. To reduce these issues, avoid calling during module definition. \"\"\" # pkgutil.iter_modules is not recursive and pkgutil.walk_packages does not handle namespace # packages... however we can leverage setuptools.find_namespace_packages, which was built for # exactly this. path_names = { p : name for p in path } path_names . update ( { str ( Path ( path ) . joinpath ( * name . split ( \".\" ))): f \" { root_name } . { name } \" for path , root_name in path_names . items () for name in find_namespace_packages ( path ) } ) with lock : return { name : importlib . import_module ( name ) for path , name in path_names . items () for _ , name , _ in pkgutil . iter_modules ([ path ], prefix = f \" { name } .\" ) } _int_sub = TypeVar ( \"_int_sub\" , bound = \"_int\" ) class _int ( int ): def __repr__ ( self ) -> str : return f \" { qname ( self ) } ( { int ( self ) } )\" def __str__ ( self ) -> str : return str ( int ( self )) # Stock magics. def __add__ ( self , x : int ) -> Self : return type ( self )( super () . __add__ ( x )) def __and__ ( self , n : int ) -> Self : return type ( self )( super () . __and__ ( n )) def __ceil__ ( self ) -> Self : return type ( self )( super () . __ceil__ ()) def __floor__ ( self ) -> Self : return type ( self )( super () . __floor__ ()) def __floordiv__ ( self , x : int ) -> Self : return type ( self )( super () . __floordiv__ ( x )) def __invert__ ( self ) -> Self : return type ( self )( super () . __invert__ ()) def __lshift__ ( self , n : int ) -> Self : return type ( self )( super () . __lshift__ ( n )) def __mod__ ( self , x : int ) -> Self : return type ( self )( super () . __mod__ ( x )) def __mul__ ( self , x : int ) -> Self : return type ( self )( super () . __mul__ ( x )) def __neg__ ( self ) -> Self : return type ( self )( super () . __neg__ ()) def __or__ ( self , n : int ) -> Self : return type ( self )( super () . __or__ ( n )) def __pos__ ( self ) -> Self : return type ( self )( super () . __pos__ ()) def __radd__ ( self , x : int ) -> Self : return type ( self )( super () . __radd__ ( x )) def __rand__ ( self , n : int ) -> Self : return type ( self )( super () . __rand__ ( n )) def __rfloordiv__ ( self , x : int ) -> Self : return type ( self )( super () . __rfloordiv__ ( x )) def __rlshift__ ( self , n : int ) -> Self : return type ( self )( super () . __rlshift__ ( n )) def __rmod__ ( self , x : int ) -> Self : return type ( self )( super () . __rmod__ ( x )) def __rmul__ ( self , x : int ) -> Self : return type ( self )( super () . __rmul__ ( x )) def __ror__ ( self , n : int ) -> Self : return type ( self )( super () . __ror__ ( n )) def __round__ ( self , ndigits : SupportsIndex = 0 ) -> Self : return type ( self )( super () . __round__ ( ndigits )) def __rrshift__ ( self , n : int ) -> Self : return type ( self )( super () . __rrshift__ ( n )) def __rshift__ ( self , n : int ) -> Self : return type ( self )( super () . __rshift__ ( n )) def __rsub__ ( self , x : int ) -> Self : return type ( self )( super () . __rsub__ ( x )) def __rxor__ ( self , n : int ) -> Self : return type ( self )( super () . __rxor__ ( n )) def __sub__ ( self , x : int ) -> Self : return type ( self )( super () . __sub__ ( x )) def __trunc__ ( self ) -> Self : return type ( self )( super () . __trunc__ ()) def __xor__ ( self , n : int ) -> Self : return type ( self )( super () . __xor__ ( n )) class int64 ( _int ): _min , _max = - ( 2 ** 63 ), ( 2 ** 63 ) - 1 def __new__ ( cls , i : Union [ int , int64 , uint64 ]) -> int64 : if i > cls . _max : if isinstance ( i , uint64 ): i = int ( i ) - uint64 . _max - 1 else : raise ValueError ( f \" { i } is too large for int64. Hint: cast to uint64 first.\" ) if i < cls . _min : raise ValueError ( f \" { i } is too small for int64.\" ) return super () . __new__ ( cls , i ) class uint64 ( _int ): _min , _max = 0 , ( 2 ** 64 ) - 1 def __new__ ( cls , i : Union [ int , int64 , uint64 ]) -> uint64 : if i > cls . _max : raise ValueError ( f \" { i } is too large for uint64.\" ) if i < cls . _min : if isinstance ( i , int64 ): i = int ( i ) + cls . _max + 1 else : raise ValueError ( f \" { i } is negative. Hint: cast to int64 first.\" ) return super () . __new__ ( cls , i ) @contextmanager def named_temporary_file ( mode : str = \"w+b\" ) -> Generator [ IO [ Any ], None , None ]: \"\"\"Minimal alternative to tempfile.NamedTemporaryFile that can be re-opened on Windows.\"\"\" with TemporaryDirectory () as d , ( Path ( d ) / \"contents\" ) . open ( mode = mode ) as f : yield f def one_or_none ( values : Optional [ list [ _V ]], * , item_name : str ) -> Optional [ _V ]: if values is None or len ( values ) == 0 : return None if len ( values ) > 1 : raise ValueError ( f \"multiple { item_name } values found: { values } \" ) return values [ 0 ] def ordinal ( n : int ) -> str : \"\"\"Convert an integer into its ordinal representation.\"\"\" n = int ( n ) suffix = [ \"th\" , \"st\" , \"nd\" , \"rd\" , \"th\" ][ min ( n % 10 , 4 )] if 11 <= ( n % 100 ) <= 13 : suffix = \"th\" return str ( n ) + suffix def register ( registry : dict [ _K , _V ], key : _K , value : _V , get_priority : Optional [ Callable [[ _V ], int ]] = None , ) -> _V : if key in registry : existing = registry [ key ] if get_priority is None : raise ValueError ( f \" { key } is already registered with: { existing } !\" ) existing_priority , new_priority = get_priority ( existing ), get_priority ( value ) if existing_priority > new_priority : return value if existing_priority == new_priority : raise ValueError ( f \" { key } with matching priority ( { existing_priority } ) is already registered with: { existing } !\" ) registry [ key ] = value return value def qname ( val : Union [ object , type ]) -> str : if isinstance ( val , type ): return val . __qualname__ return type ( val ) . __qualname__ class NoCopyMixin : \"\"\"Mixin to bypass (deep)copying. This is useful for objects that are *intended* to be stateful and preserved, despite usually preferring immutable data structures and Pydantic models, which (deep)copy often. \"\"\" def __copy__ ( self ) -> Self : return self # pragma: no cover def __deepcopy__ ( self , memo : Any ) -> Self : return self # pragma: no cover class NoCopyDict ( dict [ _K , _V ], NoCopyMixin ): pass class TypedBox ( Box , MutableMapping [ str , Union [ _V , MutableMapping [ str , _V ]]]): \"\"\"TypedBox holds a collection of typed values. Subclasses must set the __target_type__ to a base class for the contained values. \"\"\" __target_type__ : ClassVar [ type [ _V ]] # type: ignore[misc] @classmethod def __class_getitem__ ( cls , item : type [ _V ]) -> GenericAlias : if isinstance ( item , tuple ): raise TypeError ( f \" { cls . __name__ } expects a single value type\" ) value_type = item return GenericAlias ( type ( cls . __name__ , ( cls ,), { \"__module__\" : get_module_name ( depth = 2 ), # Set to our caller's module \"__target_type__\" : value_type , }, ), item , ) def __setattr__ ( self , key : str , value : Any ) -> None : # GenericAlias sets __orig_class__ after __init__, so preempt Box from storing that (or # erroring if frozen). if key == \"__orig_class__\" : return object . __setattr__ ( self , key , value ) super () . __setattr__ ( key , value ) return None def __cast_value ( self , item : str , value : Any ) -> _V : if isinstance ( value , self . __target_type__ ): return value tgt_name = self . __target_type__ . __name__ if hasattr ( self . __target_type__ , \"cast\" ): casted = cast ( Any , self . __target_type__ ) . cast ( value ) if isinstance ( casted , self . __target_type__ ): return casted raise TypeError ( f \"Expected { tgt_name } .cast( { value } ) to return an instance of { tgt_name } , got: { casted } \" ) raise TypeError ( f \"Expected an instance of { tgt_name } , got: { value } \" ) # NOTE: Box uses name mangling (double __) to prevent conflicts with contained values. def _Box__convert_and_store ( self , item : str , value : _V ) -> None : if isinstance ( value , dict ): super () . _Box__convert_and_store ( item , value ) # pylint: disable=no-member elif item in self : raise ValueError ( f \" { item } is already set!\" ) else : super () . _Box__convert_and_store ( item , self . __cast_value ( item , value )) def walk ( self , root : tuple [ str , ... ] = ()) -> Iterator [ tuple [ str , _V ]]: for k , v in self . items (): subroot = ( * root , k ) if isinstance ( v , TypedBox ): yield from v . walk ( root = subroot ) else : yield \".\" . join ( subroot ), v # type: ignore[misc] Variables PropReturn Functions classproperty def classproperty ( meth : 'Callable[..., PropReturn]' ) -> 'PropReturn' Access a @classmethod like a @property. View Source def classproperty ( meth : Callable [ ..., PropReturn ] ) -> PropReturn : \"\"\"Access a @classmethod like a @property.\"\"\" # mypy doesn ' t understand class properties yet : https : // github . com / python / mypy / issues / 2563 return classmethod ( property ( meth )) # type : ignore [ arg-type,return-value ] get_module_name def get_module_name ( depth : 'int' = 1 ) -> 'Optional[str]' Return the module name of a specific level in the stack. Depth describes how many levels to traverse, for example: - depth=0: return get_module_name's module - depth=1 (default): return the caller's module - depth=2: return the caller's calling module - ... View Source def get_module_name ( depth : int = 1 ) -> Optional [ str ] : \"\"\"Return the module name of a specific level in the stack. Depth describes how many levels to traverse, for example: - depth=0: return get_module_name's module - depth=1 (default): return the caller's module - depth=2: return the caller's calling module - ... \"\"\" frame = inspect . currentframe () if frame is None : # the interpreter doesn ' t support frame inspection return None # pragma : no cover for _ in range ( depth ) : frame = frame . f_back if frame is None : return None return frame . f_globals . get ( \"__name__\" , \"__main__\" ) import_submodules def import_submodules ( path : 'list[str]' , name : 'str' , * , lock : 'threading.Lock' = < unlocked _thread . lock object at 0x7f1a4d9138c0 > ) -> 'dict[str, ModuleType]' Recursively import submodules. This can be useful with registry patterns to automatically discover and import submodules defining additional implementations. path and name are usually provided from an existing module's __path__ and __name__ . This function is thread-safe and supports namespace modules. NOTE: This inherently triggers eager imports, which has performance impacts and may cause import cycles. To reduce these issues, avoid calling during module definition. View Source def import_submodules ( path : list [ str ], # module.__path__ is a list[str] name : str , * , lock : threading . Lock = threading . Lock (), ) -> dict [ str , ModuleType ]: \"\"\"Recursively import submodules. This can be useful with registry patterns to automatically discover and import submodules defining additional implementations. `path` and `name` are usually provided from an existing module's `__path__` and `__name__`. This function is thread-safe and supports namespace modules. NOTE: This inherently triggers eager imports, which has performance impacts and may cause import cycles. To reduce these issues, avoid calling during module definition. \"\"\" # pkgutil.iter_modules is not recursive and pkgutil.walk_packages does not handle namespace # packages... however we can leverage setuptools.find_namespace_packages, which was built for # exactly this. path_names = { p : name for p in path } path_names . update ( { str ( Path ( path ) . joinpath ( * name . split ( \".\" ))): f \" { root_name } . { name } \" for path , root_name in path_names . items () for name in find_namespace_packages ( path ) } ) with lock : return { name : importlib . import_module ( name ) for path , name in path_names . items () for _ , name , _ in pkgutil . iter_modules ([ path ], prefix = f \" { name } .\" ) } named_temporary_file def named_temporary_file ( mode : 'str' = 'w+b' ) -> 'Generator[IO[Any], None, None]' Minimal alternative to tempfile.NamedTemporaryFile that can be re-opened on Windows. View Source @contextmanager def named_temporary_file ( mode : str = \"w+b\" ) -> Generator [ IO[Any ] , None , None ]: \"\"\"Minimal alternative to tempfile.NamedTemporaryFile that can be re-opened on Windows.\"\"\" with TemporaryDirectory () as d , ( Path ( d ) / \"contents\" ). open ( mode = mode ) as f : yield f one_or_none def one_or_none ( values : 'Optional[list[_V]]' , * , item_name : 'str' ) -> 'Optional[_V]' View Source def one_or_none ( values : Optional [ list[_V ] ] , * , item_name : str ) -> Optional [ _V ] : if values is None or len ( values ) == 0 : return None if len ( values ) > 1 : raise ValueError ( f \"multiple {item_name} values found: {values}\" ) return values [ 0 ] ordinal def ordinal ( n : 'int' ) -> 'str' Convert an integer into its ordinal representation. View Source def ordinal ( n : int ) -> str : \"\"\"Convert an integer into its ordinal representation.\"\"\" n = int ( n ) suffix = [ \"th\" , \"st\" , \"nd\" , \"rd\" , \"th\" ][ min ( n % 10 , 4 )] if 11 <= ( n % 100 ) <= 13 : suffix = \"th\" return str ( n ) + suffix qname def qname ( val : 'Union[object, type]' ) -> 'str' View Source def qname ( val : Union [ object , type ]) -> str : if isinstance ( val , type ) : return val . __qualname__ return type ( val ). __qualname__ register def register ( registry : 'dict[_K, _V]' , key : '_K' , value : '_V' , get_priority : 'Optional[Callable[[_V], int]]' = None ) -> '_V' View Source def register ( registry : dict [ _K, _V ] , key : _K , value : _V , get_priority : Optional [ Callable[[_V ] , int ]] = None , ) -> _V : if key in registry : existing = registry [ key ] if get_priority is None : raise ValueError ( f \"{key} is already registered with: {existing}!\" ) existing_priority , new_priority = get_priority ( existing ), get_priority ( value ) if existing_priority > new_priority : return value if existing_priority == new_priority : raise ValueError ( f \"{key} with matching priority ({existing_priority}) is already registered with: {existing}!\" ) registry [ key ] = value return value Classes ClassName class ClassName ( / , * args , ** kwargs ) View Source class ClassName : def __get__ ( self , obj : Any , type_ : type [ Any ] ) -> str : return type_ . __name__ NoCopyDict class NoCopyDict ( / , * args , ** kwargs ) View Source class NoCopyDict ( dict [ _K , _V ], NoCopyMixin ): pass Ancestors (in MRO) builtins.dict arti.internal.utils.NoCopyMixin Methods clear def clear ( ... ) D.clear() -> None. Remove all items from D. copy def copy ( ... ) D.copy() -> a shallow copy of D fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If the key is not found, return the default if given; otherwise, raise a KeyError. popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. update def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] values def values ( ... ) D.values() -> an object providing a view on D's values NoCopyMixin class NoCopyMixin ( / , * args , ** kwargs ) View Source class NoCopyMixin: \"\"\"Mixin to bypass (deep)copying. This is useful for objects that are *intended* to be stateful and preserved, despite usually preferring immutable data structures and Pydantic models, which (deep)copy often. \"\"\" def __copy__ ( self ) -> Self: return self # pragma: no cover def __deepcopy__ ( self , memo: Any ) -> Self: return self # pragma: no cover Descendants arti.internal.utils.NoCopyDict arti.backends.memory._NoCopyContainer TypedBox class TypedBox ( * args : Any , default_box : bool = False , default_box_attr : Any = < object object at 0x7f1a4d988080 > , default_box_none_transform : bool = True , default_box_create_on_get : bool = True , frozen_box : bool = False , camel_killer_box : bool = False , conversion_box : bool = True , modify_tuples_box : bool = False , box_safe_prefix : str = 'x' , box_duplicates : str = 'ignore' , box_intact_types : Union [ Tuple , List ] = (), box_recast : Optional [ Dict ] = None , box_dots : bool = False , box_class : Union [ Dict , Type [ ForwardRef ( 'Box' )], NoneType ] = None , box_namespace : Tuple [ str , ... ] = (), ** kwargs : Any ) View Source class TypedBox ( Box , MutableMapping [ str, Union[_V, MutableMapping[str, _V ] ]] ) : \"\"\"TypedBox holds a collection of typed values. Subclasses must set the __target_type__ to a base class for the contained values. \"\"\" __target_type__ : ClassVar [ type[_V ] ] # type : ignore [ misc ] @classmethod def __class_getitem__ ( cls , item : type [ _V ] ) -> GenericAlias : if isinstance ( item , tuple ) : raise TypeError ( f \"{cls.__name__} expects a single value type\" ) value_type = item return GenericAlias ( type ( cls . __name__ , ( cls ,), { \"__module__\" : get_module_name ( depth = 2 ), # Set to our caller ' s module \"__target_type__\" : value_type , } , ), item , ) def __setattr__ ( self , key : str , value : Any ) -> None : # GenericAlias sets __orig_class__ after __init__ , so preempt Box from storing that ( or # erroring if frozen ). if key == \"__orig_class__\" : return object . __setattr__ ( self , key , value ) super (). __setattr__ ( key , value ) return None def __cast_value ( self , item : str , value : Any ) -> _V : if isinstance ( value , self . __target_type__ ) : return value tgt_name = self . __target_type__ . __name__ if hasattr ( self . __target_type__ , \"cast\" ) : casted = cast ( Any , self . __target_type__ ). cast ( value ) if isinstance ( casted , self . __target_type__ ) : return casted raise TypeError ( f \"Expected {tgt_name}.cast({value}) to return an instance of {tgt_name}, got: {casted}\" ) raise TypeError ( f \"Expected an instance of {tgt_name}, got: {value}\" ) # NOTE : Box uses name mangling ( double __ ) to prevent conflicts with contained values . def _Box__convert_and_store ( self , item : str , value : _V ) -> None : if isinstance ( value , dict ) : super (). _Box__convert_and_store ( item , value ) # pylint : disable = no - member elif item in self : raise ValueError ( f \"{item} is already set!\" ) else : super (). _Box__convert_and_store ( item , self . __cast_value ( item , value )) def walk ( self , root : tuple [ str, ... ] = ()) -> Iterator [ tuple[str, _V ] ]: for k , v in self . items () : subroot = ( * root , k ) if isinstance ( v , TypedBox ) : yield from v . walk ( root = subroot ) else : yield \".\" . join ( subroot ), v # type : ignore [ misc ] Ancestors (in MRO) box.box.Box builtins.dict collections.abc.MutableMapping collections.abc.Mapping collections.abc.Collection collections.abc.Sized collections.abc.Iterable collections.abc.Container Descendants arti.graphs.TypedBox Static methods from_json def from_json ( json_string : Optional [ str ] = None , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transform a json object string into a Box object. If the incoming json is a list, you must use BoxList.from_json. Parameters: Name Type Description Default json_string None string to pass to json.loads None filename None filename to open and pass to json.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() or json.loads None Returns: Type Description None Box object from json data View Source @classmethod def from_json ( cls , json_string : Optional [ str ] = None , filename : Optional [ Union[str, PathLike ] ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transform a json object string into a Box object. If the incoming json is a list, you must use BoxList.from_json. :param json_string: string to pass to `json.loads` :param filename: filename to open and pass to `json.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` or `json.loads` :return: Box object from json data \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_json ( json_string , filename = filename , encoding = encoding , errors = errors , ** kwargs ) if not isinstance ( data , dict ) : raise BoxError ( f \"json data not returned as a dictionary, but rather a {type(data).__name__}\" ) return cls ( data , ** box_args ) from_msgpack def from_msgpack ( msgpack_bytes : Optional [ bytes ] = None , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' View Source @classmethod def from_msgpack ( cls , msgpack_bytes : Optional [ bytes ] = None , filename : Optional [ Union[str, PathLike ] ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : raise BoxError ( 'msgpack is unavailable on this system, please install the \"msgpack\" package' ) from_toml def from_toml ( toml_string : Optional [ str ] = None , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transforms a toml string or file into a Box object Parameters: Name Type Description Default toml_string None string to pass to toml.load None filename None filename to open and pass to toml.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() None Returns: Type Description None Box object View Source @classmethod def from_toml ( cls , toml_string : Optional [ str ] = None , filename : Optional [ Union[str, PathLike ] ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transforms a toml string or file into a Box object :param toml_string: string to pass to `toml.load` :param filename: filename to open and pass to `toml.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` :return: Box object \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_toml ( toml_string = toml_string , filename = filename , encoding = encoding , errors = errors ) return cls ( data , ** box_args ) from_yaml def from_yaml ( yaml_string : Optional [ str ] = None , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transform a yaml object string into a Box object. By default will use SafeLoader. Parameters: Name Type Description Default yaml_string None string to pass to yaml.load None filename None filename to open and pass to yaml.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() or yaml.load None Returns: Type Description None Box object from yaml data View Source @classmethod def from_yaml ( cls , yaml_string : Optional [ str ] = None , filename : Optional [ Union[str, PathLike ] ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transform a yaml object string into a Box object. By default will use SafeLoader. :param yaml_string: string to pass to `yaml.load` :param filename: filename to open and pass to `yaml.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` or `yaml.load` :return: Box object from yaml data \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_yaml ( yaml_string = yaml_string , filename = filename , encoding = encoding , errors = errors , ** kwargs ) if not data : return cls ( ** box_args ) if not isinstance ( data , dict ) : raise BoxError ( f \"yaml data not returned as a dictionary but rather a {type(data).__name__}\" ) return cls ( data , ** box_args ) Methods clear def clear ( self ) D.clear() -> None. Remove all items from D. View Source def clear ( self ) : if self . _box_config [ \"frozen_box\" ]: raise BoxError ( \"Box is frozen\" ) super () . clear () self . _box_config [ \"__safe_keys\" ]. clear () copy def copy ( self ) -> 'Box' D.copy() -> a shallow copy of D View Source def copy ( self ) -> \"Box\" : config = self . __box_config () config . pop ( \"box_namespace\" ) # Detach namespace ; it will be reassigned if we nest again return Box ( super (). copy (), ** config ) fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default =< object object at 0x7f1a4d988080 > ) Return the value for key if key is in the dictionary, else default. View Source def get ( self , key , default = NO_DEFAULT ) : if key not in self : if default is NO_DEFAULT : if self . _box_config [ \"default_box\" ] and self . _box_config [ \"default_box_none_transform\" ] : return self . __get_default ( key ) else : return None if isinstance ( default , dict ) and not isinstance ( default , Box ) : return Box ( default ) if isinstance ( default , list ) and not isinstance ( default , box . BoxList ) : return box . BoxList ( default ) return default return self [ key ] items def items ( self , dotted : bool = False ) D.items() -> a set-like object providing a view on D's items View Source def items ( self , dotted : Union [ bool ] = False ) : if not dotted : return super (). items () if not self . _box_config [ \"box_dots\" ] : raise BoxError ( \"Cannot return dotted keys as this Box does not have `box_dots` enabled\" ) return [ (k, self[k ] ) for k in self . keys ( dotted = True ) ] keys def keys ( self , dotted : bool = False ) D.keys() -> a set-like object providing a view on D's keys View Source def keys ( self , dotted : Union [ bool ] = False ) : if not dotted : return super (). keys () if not self . _box_config [ \"box_dots\" ] : raise BoxError ( \"Cannot return dotted keys as this Box does not have `box_dots` enabled\" ) keys = set () for key , value in self . items () : added = False if isinstance ( key , str ) : if isinstance ( value , Box ) : for sub_key in value . keys ( dotted = True ) : keys . add ( f \"{key}.{sub_key}\" ) added = True elif isinstance ( value , box . BoxList ) : for pos in value . _dotted_helper () : keys . add ( f \"{key}{pos}\" ) added = True if not added : keys . add ( key ) return sorted ( keys , key = lambda x : str ( x )) merge_update def merge_update ( self , * args , ** kwargs ) View Source def merge_update ( self , * args , ** kwargs ) : merge_type = None if \"box_merge_lists\" in kwargs : merge_type = kwargs . pop ( \"box_merge_lists\" ) def convert_and_set ( k , v ) : intact_type = self . _box_config [ \"box_intact_types\" ] and isinstance ( v , self . _box_config [ \"box_intact_types\" ] ) if isinstance ( v , dict ) and not intact_type : # Box objects must be created in case they are already # in the ` converted ` box_config set v = self . _box_config [ \"box_class\" ] ( v , ** self . __box_config ( extra_namespace = k )) if k in self and isinstance ( self [ k ] , dict ) : self [ k ] . merge_update ( v ) return if isinstance ( v , list ) and not intact_type : v = box . BoxList ( v , ** self . __box_config ( extra_namespace = k )) if merge_type == \"extend\" and k in self and isinstance ( self [ k ] , list ) : self [ k ] . extend ( v ) return if merge_type == \"unique\" and k in self and isinstance ( self [ k ] , list ) : for item in v : if item not in self [ k ] : self [ k ] . append ( item ) return self . __setitem__ ( k , v ) if ( len ( args ) + int ( bool ( kwargs ))) > 1 : raise BoxTypeError ( f \"merge_update expected at most 1 argument, got {len(args) + int(bool(kwargs))}\" ) single_arg = next ( iter ( args ), None ) if single_arg : if hasattr ( single_arg , \"keys\" ) : for k in single_arg : convert_and_set ( k , single_arg [ k ] ) else : for k , v in single_arg : convert_and_set ( k , v ) for key in kwargs : convert_and_set ( key , kwargs [ key ] ) pop def pop ( self , key , * args ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If the key is not found, return the default if given; otherwise, raise a KeyError. View Source def pop ( self , key , * args ) : if self . _box_config [ \"frozen_box\" ] : raise BoxError ( \"Box is frozen\" ) if args : if len ( args ) != 1 : raise BoxError ( 'pop() takes only one optional argument \"default\"' ) try : item = self [ key ] except KeyError : return args [ 0 ] else : del self [ key ] return item try : item = self [ key ] except KeyError : raise BoxKeyError ( f \"{key}\" ) from None else : del self [ key ] return item popitem def popitem ( self ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. View Source def popitem ( self ) : if self . _box_config [ \"frozen_box\" ]: raise BoxError ( \"Box is frozen\" ) try : key = next ( self . __iter__ ()) except StopIteration : raise BoxKeyError ( \"Empty box\" ) from None return key , self . pop ( key ) setdefault def setdefault ( self , item , default = None ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. View Source def setdefault ( self , item , default = None ) : if item in self : return self [ item ] if self . _box_config [ \"box_dots\" ] : if item in _get_dot_paths ( self ) : return self [ item ] if isinstance ( default , dict ) : default = self . _box_config [ \"box_class\" ] ( default , ** self . __box_config ( extra_namespace = item )) if isinstance ( default , list ) : default = box . BoxList ( default , ** self . __box_config ( extra_namespace = item )) self [ item ] = default return self [ item ] to_dict def to_dict ( self ) -> Dict Turn the Box and sub Boxes back into a native python dictionary. Returns: Type Description None python dictionary of this Box View Source def to_dict ( self ) -> Dict : \"\"\" Turn the Box and sub Boxes back into a native python dictionary. :return: python dictionary of this Box \"\"\" out_dict = dict ( self ) for k , v in out_dict . items () : if v is self : out_dict [ k ] = out_dict elif isinstance ( v , Box ) : out_dict [ k ] = v . to_dict () elif isinstance ( v , box . BoxList ) : out_dict [ k ] = v . to_list () return out_dict to_json def to_json ( self , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** json_kwargs ) Transform the Box object into a JSON string. Parameters: Name Type Description Default filename None If provided will save to file None encoding None File encoding None errors None How to handle encoding errors None json_kwargs None additional arguments to pass to json.dump(s) None Returns: Type Description None string of JSON (if no filename provided) View Source def to_json ( self , filename : Optional [ Union [ str , PathLike ]] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** json_kwargs , ) : \"\" \" Transform the Box object into a JSON string. :param filename: If provided will save to file :param encoding: File encoding :param errors: How to handle encoding errors :param json_kwargs: additional arguments to pass to json.dump(s) :return: string of JSON (if no filename provided) \"\" \" return _to_json(self.to_dict(), filename=filename, encoding=encoding, errors=errors, **json_kwargs) to_msgpack def to_msgpack ( self , filename : Union [ str , os . PathLike , NoneType ] = None , ** kwargs ) View Source def to_msgpack(self, filename: Optional[Union[str, PathLike]] = None, **kwargs): raise BoxError('msgpack is unavailable on this system, please install the \"msgpack\" package') to_toml def to_toml ( self , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' ) Transform the Box object into a toml string. Parameters: Name Type Description Default filename None File to write toml object too None encoding None File encoding None errors None How to handle encoding errors None Returns: Type Description None string of TOML (if no filename provided) View Source def to_toml ( self , filename : Optional [ Union [ str , PathLike ]] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" ) : \"\" \" Transform the Box object into a toml string. :param filename: File to write toml object too :param encoding: File encoding :param errors: How to handle encoding errors :return: string of TOML (if no filename provided) \"\" \" return _to_toml(self.to_dict(), filename=filename, encoding=encoding, errors=errors) to_yaml def to_yaml ( self , filename : Union [ str , os . PathLike , NoneType ] = None , default_flow_style : bool = False , encoding : str = 'utf-8' , errors : str = 'strict' , ** yaml_kwargs ) Transform the Box object into a YAML string. Parameters: Name Type Description Default filename None If provided will save to file None default_flow_style None False will recursively dump dicts None encoding None File encoding None errors None How to handle encoding errors None yaml_kwargs None additional arguments to pass to yaml.dump None Returns: Type Description None string of YAML (if no filename provided) View Source def to_yaml ( self , filename : Optional [ Union [ str , PathLike ]] = None , default_flow_style : bool = False , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** yaml_kwargs , ) : \"\" \" Transform the Box object into a YAML string. :param filename: If provided will save to file :param default_flow_style: False will recursively dump dicts :param encoding: File encoding :param errors: How to handle encoding errors :param yaml_kwargs: additional arguments to pass to yaml.dump :return: string of YAML (if no filename provided) \"\" \" return _to_yaml( self.to_dict(), filename=filename, default_flow_style=default_flow_style, encoding=encoding, errors=errors, **yaml_kwargs, ) update def update ( self , * args , ** kwargs ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] View Source def update ( self , * args , ** kwargs ) : if self . _box_config [ \"frozen_box\" ] : raise BoxError ( \"Box is frozen\" ) if ( len ( args ) + int ( bool ( kwargs ))) > 1 : raise BoxTypeError ( f \"update expected at most 1 argument, got {len(args) + int(bool(kwargs))}\" ) single_arg = next ( iter ( args ), None ) if single_arg : if hasattr ( single_arg , \"keys\" ) : for k in single_arg : self . __convert_and_store ( k , single_arg [ k ] ) else : for k , v in single_arg : self . __convert_and_store ( k , v ) for k in kwargs : self . __convert_and_store ( k , kwargs [ k ] ) values def values ( ... ) D.values() -> an object providing a view on D's values walk def walk ( self , root : 'tuple[str, ...]' = () ) -> 'Iterator[tuple[str, _V]]' View Source def walk ( self , root : tuple [ str, ... ] = ()) -> Iterator [ tuple[str, _V ] ]: for k , v in self . items () : subroot = ( * root , k ) if isinstance ( v , TypedBox ) : yield from v . walk ( root = subroot ) else : yield \".\" . join ( subroot ), v # type : ignore [ misc ] class_name class class_name ( / , * args , ** kwargs ) View Source class ClassName : def __get__ ( self , obj : Any , type_ : type [ Any ] ) -> str : return type_ . __name__ frozendict class frozendict ( arg : 'Union[Mapping[_K, _V], Iterable[tuple[_K, _V]]]' = (), ** kwargs : '_V' ) View Source class frozendict ( Mapping [ _K, _V ] ) : def __init__ ( self , arg : Union [ Mapping[_K, _V ] , Iterable [ tuple[_K, _V ] ]] = (), ** kwargs : _V ) -> None : self . _data = dict [ _K, _V ] ( arg , ** kwargs ) # Eagerly evaluate the hash to confirm elements are also frozen ( via frozenset ) at # creation time , not just when hashed . self . _hash = hash ( frozenset ( self . _data . items ())) def __getitem__ ( self , key : _K ) -> _V : return self . _data [ key ] def __hash__ ( self ) -> int : return self . _hash def __iter__ ( self ) -> Iterator [ _K ] : return iter ( self . _data ) def __len__ ( self ) -> int : return len ( self . _data ) def __or__ ( self , other : Mapping [ _K, _V ] ) -> frozendict [ _K, _V ] : return type ( self )( { ** self , ** other } ) __ror__ = __or__ def __repr__ ( self ) -> str : return repr ( self . _data ) Ancestors (in MRO) collections.abc.Mapping collections.abc.Collection collections.abc.Sized collections.abc.Iterable collections.abc.Container Methods get def get ( self , key , default = None ) D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. items def items ( self ) D.items() -> a set-like object providing a view on D's items keys def keys ( self ) D.keys() -> a set-like object providing a view on D's keys values def values ( self ) D.values() -> an object providing a view on D's values int64 class int64 ( / , * args , ** kwargs ) View Source class int64 ( _int ): _min , _max = -( 2 ** 63 ), ( 2 ** 63 ) - 1 def __new__ ( cls , i: Union [ int , int64 , uint64 ]) -> int64: if i > cls . _max: if isinstance ( i , uint64 ): i = int ( i ) - uint64 . _max - 1 else: raise ValueError ( f \"{i} is too large for int64. Hint: cast to uint64 first.\" ) if i < cls . _min: raise ValueError ( f \"{i} is too small for int64.\" ) return super (). __new__ ( cls , i ) Ancestors (in MRO) arti.internal.utils._int builtins.int Class variables denominator imag numerator real Methods as_integer_ratio def as_integer_ratio ( self , / ) Return integer ratio. Return a pair of integers, whose ratio is exactly equal to the original int and with a positive denominator. (10).as_integer_ratio() (10, 1) (-10).as_integer_ratio() (-10, 1) (0).as_integer_ratio() (0, 1) bit_count def bit_count ( self , / ) Number of ones in the binary representation of the absolute value of self. Also known as the population count. bin(13) '0b1101' (13).bit_count() 3 bit_length def bit_length ( self , / ) Number of bits necessary to represent self in binary. bin(37) '0b100101' (37).bit_length() 6 conjugate def conjugate ( ... ) Returns self, the complex conjugate of any int. from_bytes def from_bytes ( bytes , byteorder = 'big' , * , signed = False ) Return the integer represented by the given array of bytes. bytes Holds the array of bytes to convert. The argument must either support the buffer protocol or be an iterable object producing bytes. Bytes and bytearray are examples of built-in objects that support the buffer protocol. byteorder The byte order used to represent the integer. If byteorder is 'big', the most significant byte is at the beginning of the byte array. If byteorder is 'little', the most significant byte is at the end of the byte array. To request the native byte order of the host system, use `sys.byteorder' as the byte order value. Default is to use 'big'. signed Indicates whether two's complement is used to represent the integer. to_bytes def to_bytes ( self , / , length = 1 , byteorder = 'big' , * , signed = False ) Return an array of bytes representing an integer. length Length of bytes object to use. An OverflowError is raised if the integer is not representable with the given number of bytes. Default is length 1. byteorder The byte order used to represent the integer. If byteorder is 'big', the most significant byte is at the beginning of the byte array. If byteorder is 'little', the most significant byte is at the end of the byte array. To request the native byte order of the host system, use `sys.byteorder' as the byte order value. Default is to use 'big'. signed Determines whether two's complement is used to represent the integer. If signed is False and a negative integer is given, an OverflowError is raised. uint64 class uint64 ( / , * args , ** kwargs ) View Source class uint64 ( _int ): _min , _max = 0 , ( 2 ** 64 ) - 1 def __new__ ( cls , i: Union [ int , int64 , uint64 ]) -> uint64: if i > cls . _max: raise ValueError ( f \"{i} is too large for uint64.\" ) if i < cls . _min: if isinstance ( i , int64 ): i = int ( i ) + cls . _max + 1 else: raise ValueError ( f \"{i} is negative. Hint: cast to int64 first.\" ) return super (). __new__ ( cls , i ) Ancestors (in MRO) arti.internal.utils._int builtins.int Class variables denominator imag numerator real Methods as_integer_ratio def as_integer_ratio ( self , / ) Return integer ratio. Return a pair of integers, whose ratio is exactly equal to the original int and with a positive denominator. (10).as_integer_ratio() (10, 1) (-10).as_integer_ratio() (-10, 1) (0).as_integer_ratio() (0, 1) bit_count def bit_count ( self , / ) Number of ones in the binary representation of the absolute value of self. Also known as the population count. bin(13) '0b1101' (13).bit_count() 3 bit_length def bit_length ( self , / ) Number of bits necessary to represent self in binary. bin(37) '0b100101' (37).bit_length() 6 conjugate def conjugate ( ... ) Returns self, the complex conjugate of any int. from_bytes def from_bytes ( bytes , byteorder = 'big' , * , signed = False ) Return the integer represented by the given array of bytes. bytes Holds the array of bytes to convert. The argument must either support the buffer protocol or be an iterable object producing bytes. Bytes and bytearray are examples of built-in objects that support the buffer protocol. byteorder The byte order used to represent the integer. If byteorder is 'big', the most significant byte is at the beginning of the byte array. If byteorder is 'little', the most significant byte is at the end of the byte array. To request the native byte order of the host system, use `sys.byteorder' as the byte order value. Default is to use 'big'. signed Indicates whether two's complement is used to represent the integer. to_bytes def to_bytes ( self , / , length = 1 , byteorder = 'big' , * , signed = False ) Return an array of bytes representing an integer. length Length of bytes object to use. An OverflowError is raised if the integer is not representable with the given number of bytes. Default is length 1. byteorder The byte order used to represent the integer. If byteorder is 'big', the most significant byte is at the beginning of the byte array. If byteorder is 'little', the most significant byte is at the end of the byte array. To request the native byte order of the host system, use `sys.byteorder' as the byte order value. Default is to use 'big'. signed Determines whether two's complement is used to represent the integer. If signed is False and a negative integer is given, an OverflowError is raised.","title":"Utils"},{"location":"reference/arti/internal/utils/#module-artiinternalutils","text":"None None View Source from __future__ import annotations import importlib import inspect import pkgutil import threading from collections.abc import Callable , Generator , Iterable , Iterator , Mapping , MutableMapping from contextlib import contextmanager from pathlib import Path from tempfile import TemporaryDirectory from types import GenericAlias , ModuleType from typing import IO , Any , ClassVar , Optional , SupportsIndex , TypeVar , Union , cast from box import Box from arti.internal.type_hints import Self from arti.internal.vendored.setuptools import find_namespace_packages _K = TypeVar ( \"_K\" ) _V = TypeVar ( \"_V\" ) class ClassName : def __get__ ( self , obj : Any , type_ : type [ Any ]) -> str : return type_ . __name__ class_name = cast ( Callable [[], str ], ClassName ) PropReturn = TypeVar ( \"PropReturn\" ) def classproperty ( meth : Callable [ ... , PropReturn ]) -> PropReturn : \"\"\"Access a @classmethod like a @property.\"\"\" # mypy doesn't understand class properties yet: https://github.com/python/mypy/issues/2563 return classmethod ( property ( meth )) # type: ignore[arg-type,return-value] class frozendict ( Mapping [ _K , _V ]): def __init__ ( self , arg : Union [ Mapping [ _K , _V ], Iterable [ tuple [ _K , _V ]]] = (), ** kwargs : _V ) -> None : self . _data = dict [ _K , _V ]( arg , ** kwargs ) # Eagerly evaluate the hash to confirm elements are also frozen (via frozenset) at # creation time, not just when hashed. self . _hash = hash ( frozenset ( self . _data . items ())) def __getitem__ ( self , key : _K ) -> _V : return self . _data [ key ] def __hash__ ( self ) -> int : return self . _hash def __iter__ ( self ) -> Iterator [ _K ]: return iter ( self . _data ) def __len__ ( self ) -> int : return len ( self . _data ) def __or__ ( self , other : Mapping [ _K , _V ]) -> frozendict [ _K , _V ]: return type ( self )({ ** self , ** other }) __ror__ = __or__ def __repr__ ( self ) -> str : return repr ( self . _data ) def get_module_name ( depth : int = 1 ) -> Optional [ str ]: \"\"\"Return the module name of a specific level in the stack. Depth describes how many levels to traverse, for example: - depth=0: return get_module_name's module - depth=1 (default): return the caller's module - depth=2: return the caller's calling module - ... \"\"\" frame = inspect . currentframe () if frame is None : # the interpreter doesn't support frame inspection return None # pragma: no cover for _ in range ( depth ): frame = frame . f_back if frame is None : return None return frame . f_globals . get ( \"__name__\" , \"__main__\" ) def import_submodules ( path : list [ str ], # module.__path__ is a list[str] name : str , * , lock : threading . Lock = threading . Lock (), ) -> dict [ str , ModuleType ]: \"\"\"Recursively import submodules. This can be useful with registry patterns to automatically discover and import submodules defining additional implementations. `path` and `name` are usually provided from an existing module's `__path__` and `__name__`. This function is thread-safe and supports namespace modules. NOTE: This inherently triggers eager imports, which has performance impacts and may cause import cycles. To reduce these issues, avoid calling during module definition. \"\"\" # pkgutil.iter_modules is not recursive and pkgutil.walk_packages does not handle namespace # packages... however we can leverage setuptools.find_namespace_packages, which was built for # exactly this. path_names = { p : name for p in path } path_names . update ( { str ( Path ( path ) . joinpath ( * name . split ( \".\" ))): f \" { root_name } . { name } \" for path , root_name in path_names . items () for name in find_namespace_packages ( path ) } ) with lock : return { name : importlib . import_module ( name ) for path , name in path_names . items () for _ , name , _ in pkgutil . iter_modules ([ path ], prefix = f \" { name } .\" ) } _int_sub = TypeVar ( \"_int_sub\" , bound = \"_int\" ) class _int ( int ): def __repr__ ( self ) -> str : return f \" { qname ( self ) } ( { int ( self ) } )\" def __str__ ( self ) -> str : return str ( int ( self )) # Stock magics. def __add__ ( self , x : int ) -> Self : return type ( self )( super () . __add__ ( x )) def __and__ ( self , n : int ) -> Self : return type ( self )( super () . __and__ ( n )) def __ceil__ ( self ) -> Self : return type ( self )( super () . __ceil__ ()) def __floor__ ( self ) -> Self : return type ( self )( super () . __floor__ ()) def __floordiv__ ( self , x : int ) -> Self : return type ( self )( super () . __floordiv__ ( x )) def __invert__ ( self ) -> Self : return type ( self )( super () . __invert__ ()) def __lshift__ ( self , n : int ) -> Self : return type ( self )( super () . __lshift__ ( n )) def __mod__ ( self , x : int ) -> Self : return type ( self )( super () . __mod__ ( x )) def __mul__ ( self , x : int ) -> Self : return type ( self )( super () . __mul__ ( x )) def __neg__ ( self ) -> Self : return type ( self )( super () . __neg__ ()) def __or__ ( self , n : int ) -> Self : return type ( self )( super () . __or__ ( n )) def __pos__ ( self ) -> Self : return type ( self )( super () . __pos__ ()) def __radd__ ( self , x : int ) -> Self : return type ( self )( super () . __radd__ ( x )) def __rand__ ( self , n : int ) -> Self : return type ( self )( super () . __rand__ ( n )) def __rfloordiv__ ( self , x : int ) -> Self : return type ( self )( super () . __rfloordiv__ ( x )) def __rlshift__ ( self , n : int ) -> Self : return type ( self )( super () . __rlshift__ ( n )) def __rmod__ ( self , x : int ) -> Self : return type ( self )( super () . __rmod__ ( x )) def __rmul__ ( self , x : int ) -> Self : return type ( self )( super () . __rmul__ ( x )) def __ror__ ( self , n : int ) -> Self : return type ( self )( super () . __ror__ ( n )) def __round__ ( self , ndigits : SupportsIndex = 0 ) -> Self : return type ( self )( super () . __round__ ( ndigits )) def __rrshift__ ( self , n : int ) -> Self : return type ( self )( super () . __rrshift__ ( n )) def __rshift__ ( self , n : int ) -> Self : return type ( self )( super () . __rshift__ ( n )) def __rsub__ ( self , x : int ) -> Self : return type ( self )( super () . __rsub__ ( x )) def __rxor__ ( self , n : int ) -> Self : return type ( self )( super () . __rxor__ ( n )) def __sub__ ( self , x : int ) -> Self : return type ( self )( super () . __sub__ ( x )) def __trunc__ ( self ) -> Self : return type ( self )( super () . __trunc__ ()) def __xor__ ( self , n : int ) -> Self : return type ( self )( super () . __xor__ ( n )) class int64 ( _int ): _min , _max = - ( 2 ** 63 ), ( 2 ** 63 ) - 1 def __new__ ( cls , i : Union [ int , int64 , uint64 ]) -> int64 : if i > cls . _max : if isinstance ( i , uint64 ): i = int ( i ) - uint64 . _max - 1 else : raise ValueError ( f \" { i } is too large for int64. Hint: cast to uint64 first.\" ) if i < cls . _min : raise ValueError ( f \" { i } is too small for int64.\" ) return super () . __new__ ( cls , i ) class uint64 ( _int ): _min , _max = 0 , ( 2 ** 64 ) - 1 def __new__ ( cls , i : Union [ int , int64 , uint64 ]) -> uint64 : if i > cls . _max : raise ValueError ( f \" { i } is too large for uint64.\" ) if i < cls . _min : if isinstance ( i , int64 ): i = int ( i ) + cls . _max + 1 else : raise ValueError ( f \" { i } is negative. Hint: cast to int64 first.\" ) return super () . __new__ ( cls , i ) @contextmanager def named_temporary_file ( mode : str = \"w+b\" ) -> Generator [ IO [ Any ], None , None ]: \"\"\"Minimal alternative to tempfile.NamedTemporaryFile that can be re-opened on Windows.\"\"\" with TemporaryDirectory () as d , ( Path ( d ) / \"contents\" ) . open ( mode = mode ) as f : yield f def one_or_none ( values : Optional [ list [ _V ]], * , item_name : str ) -> Optional [ _V ]: if values is None or len ( values ) == 0 : return None if len ( values ) > 1 : raise ValueError ( f \"multiple { item_name } values found: { values } \" ) return values [ 0 ] def ordinal ( n : int ) -> str : \"\"\"Convert an integer into its ordinal representation.\"\"\" n = int ( n ) suffix = [ \"th\" , \"st\" , \"nd\" , \"rd\" , \"th\" ][ min ( n % 10 , 4 )] if 11 <= ( n % 100 ) <= 13 : suffix = \"th\" return str ( n ) + suffix def register ( registry : dict [ _K , _V ], key : _K , value : _V , get_priority : Optional [ Callable [[ _V ], int ]] = None , ) -> _V : if key in registry : existing = registry [ key ] if get_priority is None : raise ValueError ( f \" { key } is already registered with: { existing } !\" ) existing_priority , new_priority = get_priority ( existing ), get_priority ( value ) if existing_priority > new_priority : return value if existing_priority == new_priority : raise ValueError ( f \" { key } with matching priority ( { existing_priority } ) is already registered with: { existing } !\" ) registry [ key ] = value return value def qname ( val : Union [ object , type ]) -> str : if isinstance ( val , type ): return val . __qualname__ return type ( val ) . __qualname__ class NoCopyMixin : \"\"\"Mixin to bypass (deep)copying. This is useful for objects that are *intended* to be stateful and preserved, despite usually preferring immutable data structures and Pydantic models, which (deep)copy often. \"\"\" def __copy__ ( self ) -> Self : return self # pragma: no cover def __deepcopy__ ( self , memo : Any ) -> Self : return self # pragma: no cover class NoCopyDict ( dict [ _K , _V ], NoCopyMixin ): pass class TypedBox ( Box , MutableMapping [ str , Union [ _V , MutableMapping [ str , _V ]]]): \"\"\"TypedBox holds a collection of typed values. Subclasses must set the __target_type__ to a base class for the contained values. \"\"\" __target_type__ : ClassVar [ type [ _V ]] # type: ignore[misc] @classmethod def __class_getitem__ ( cls , item : type [ _V ]) -> GenericAlias : if isinstance ( item , tuple ): raise TypeError ( f \" { cls . __name__ } expects a single value type\" ) value_type = item return GenericAlias ( type ( cls . __name__ , ( cls ,), { \"__module__\" : get_module_name ( depth = 2 ), # Set to our caller's module \"__target_type__\" : value_type , }, ), item , ) def __setattr__ ( self , key : str , value : Any ) -> None : # GenericAlias sets __orig_class__ after __init__, so preempt Box from storing that (or # erroring if frozen). if key == \"__orig_class__\" : return object . __setattr__ ( self , key , value ) super () . __setattr__ ( key , value ) return None def __cast_value ( self , item : str , value : Any ) -> _V : if isinstance ( value , self . __target_type__ ): return value tgt_name = self . __target_type__ . __name__ if hasattr ( self . __target_type__ , \"cast\" ): casted = cast ( Any , self . __target_type__ ) . cast ( value ) if isinstance ( casted , self . __target_type__ ): return casted raise TypeError ( f \"Expected { tgt_name } .cast( { value } ) to return an instance of { tgt_name } , got: { casted } \" ) raise TypeError ( f \"Expected an instance of { tgt_name } , got: { value } \" ) # NOTE: Box uses name mangling (double __) to prevent conflicts with contained values. def _Box__convert_and_store ( self , item : str , value : _V ) -> None : if isinstance ( value , dict ): super () . _Box__convert_and_store ( item , value ) # pylint: disable=no-member elif item in self : raise ValueError ( f \" { item } is already set!\" ) else : super () . _Box__convert_and_store ( item , self . __cast_value ( item , value )) def walk ( self , root : tuple [ str , ... ] = ()) -> Iterator [ tuple [ str , _V ]]: for k , v in self . items (): subroot = ( * root , k ) if isinstance ( v , TypedBox ): yield from v . walk ( root = subroot ) else : yield \".\" . join ( subroot ), v # type: ignore[misc]","title":"Module arti.internal.utils"},{"location":"reference/arti/internal/utils/#variables","text":"PropReturn","title":"Variables"},{"location":"reference/arti/internal/utils/#functions","text":"","title":"Functions"},{"location":"reference/arti/internal/utils/#classproperty","text":"def classproperty ( meth : 'Callable[..., PropReturn]' ) -> 'PropReturn' Access a @classmethod like a @property. View Source def classproperty ( meth : Callable [ ..., PropReturn ] ) -> PropReturn : \"\"\"Access a @classmethod like a @property.\"\"\" # mypy doesn ' t understand class properties yet : https : // github . com / python / mypy / issues / 2563 return classmethod ( property ( meth )) # type : ignore [ arg-type,return-value ]","title":"classproperty"},{"location":"reference/arti/internal/utils/#get_module_name","text":"def get_module_name ( depth : 'int' = 1 ) -> 'Optional[str]' Return the module name of a specific level in the stack. Depth describes how many levels to traverse, for example: - depth=0: return get_module_name's module - depth=1 (default): return the caller's module - depth=2: return the caller's calling module - ... View Source def get_module_name ( depth : int = 1 ) -> Optional [ str ] : \"\"\"Return the module name of a specific level in the stack. Depth describes how many levels to traverse, for example: - depth=0: return get_module_name's module - depth=1 (default): return the caller's module - depth=2: return the caller's calling module - ... \"\"\" frame = inspect . currentframe () if frame is None : # the interpreter doesn ' t support frame inspection return None # pragma : no cover for _ in range ( depth ) : frame = frame . f_back if frame is None : return None return frame . f_globals . get ( \"__name__\" , \"__main__\" )","title":"get_module_name"},{"location":"reference/arti/internal/utils/#import_submodules","text":"def import_submodules ( path : 'list[str]' , name : 'str' , * , lock : 'threading.Lock' = < unlocked _thread . lock object at 0x7f1a4d9138c0 > ) -> 'dict[str, ModuleType]' Recursively import submodules. This can be useful with registry patterns to automatically discover and import submodules defining additional implementations. path and name are usually provided from an existing module's __path__ and __name__ . This function is thread-safe and supports namespace modules. NOTE: This inherently triggers eager imports, which has performance impacts and may cause import cycles. To reduce these issues, avoid calling during module definition. View Source def import_submodules ( path : list [ str ], # module.__path__ is a list[str] name : str , * , lock : threading . Lock = threading . Lock (), ) -> dict [ str , ModuleType ]: \"\"\"Recursively import submodules. This can be useful with registry patterns to automatically discover and import submodules defining additional implementations. `path` and `name` are usually provided from an existing module's `__path__` and `__name__`. This function is thread-safe and supports namespace modules. NOTE: This inherently triggers eager imports, which has performance impacts and may cause import cycles. To reduce these issues, avoid calling during module definition. \"\"\" # pkgutil.iter_modules is not recursive and pkgutil.walk_packages does not handle namespace # packages... however we can leverage setuptools.find_namespace_packages, which was built for # exactly this. path_names = { p : name for p in path } path_names . update ( { str ( Path ( path ) . joinpath ( * name . split ( \".\" ))): f \" { root_name } . { name } \" for path , root_name in path_names . items () for name in find_namespace_packages ( path ) } ) with lock : return { name : importlib . import_module ( name ) for path , name in path_names . items () for _ , name , _ in pkgutil . iter_modules ([ path ], prefix = f \" { name } .\" ) }","title":"import_submodules"},{"location":"reference/arti/internal/utils/#named_temporary_file","text":"def named_temporary_file ( mode : 'str' = 'w+b' ) -> 'Generator[IO[Any], None, None]' Minimal alternative to tempfile.NamedTemporaryFile that can be re-opened on Windows. View Source @contextmanager def named_temporary_file ( mode : str = \"w+b\" ) -> Generator [ IO[Any ] , None , None ]: \"\"\"Minimal alternative to tempfile.NamedTemporaryFile that can be re-opened on Windows.\"\"\" with TemporaryDirectory () as d , ( Path ( d ) / \"contents\" ). open ( mode = mode ) as f : yield f","title":"named_temporary_file"},{"location":"reference/arti/internal/utils/#one_or_none","text":"def one_or_none ( values : 'Optional[list[_V]]' , * , item_name : 'str' ) -> 'Optional[_V]' View Source def one_or_none ( values : Optional [ list[_V ] ] , * , item_name : str ) -> Optional [ _V ] : if values is None or len ( values ) == 0 : return None if len ( values ) > 1 : raise ValueError ( f \"multiple {item_name} values found: {values}\" ) return values [ 0 ]","title":"one_or_none"},{"location":"reference/arti/internal/utils/#ordinal","text":"def ordinal ( n : 'int' ) -> 'str' Convert an integer into its ordinal representation. View Source def ordinal ( n : int ) -> str : \"\"\"Convert an integer into its ordinal representation.\"\"\" n = int ( n ) suffix = [ \"th\" , \"st\" , \"nd\" , \"rd\" , \"th\" ][ min ( n % 10 , 4 )] if 11 <= ( n % 100 ) <= 13 : suffix = \"th\" return str ( n ) + suffix","title":"ordinal"},{"location":"reference/arti/internal/utils/#qname","text":"def qname ( val : 'Union[object, type]' ) -> 'str' View Source def qname ( val : Union [ object , type ]) -> str : if isinstance ( val , type ) : return val . __qualname__ return type ( val ). __qualname__","title":"qname"},{"location":"reference/arti/internal/utils/#register","text":"def register ( registry : 'dict[_K, _V]' , key : '_K' , value : '_V' , get_priority : 'Optional[Callable[[_V], int]]' = None ) -> '_V' View Source def register ( registry : dict [ _K, _V ] , key : _K , value : _V , get_priority : Optional [ Callable[[_V ] , int ]] = None , ) -> _V : if key in registry : existing = registry [ key ] if get_priority is None : raise ValueError ( f \"{key} is already registered with: {existing}!\" ) existing_priority , new_priority = get_priority ( existing ), get_priority ( value ) if existing_priority > new_priority : return value if existing_priority == new_priority : raise ValueError ( f \"{key} with matching priority ({existing_priority}) is already registered with: {existing}!\" ) registry [ key ] = value return value","title":"register"},{"location":"reference/arti/internal/utils/#classes","text":"","title":"Classes"},{"location":"reference/arti/internal/utils/#classname","text":"class ClassName ( / , * args , ** kwargs ) View Source class ClassName : def __get__ ( self , obj : Any , type_ : type [ Any ] ) -> str : return type_ . __name__","title":"ClassName"},{"location":"reference/arti/internal/utils/#nocopydict","text":"class NoCopyDict ( / , * args , ** kwargs ) View Source class NoCopyDict ( dict [ _K , _V ], NoCopyMixin ): pass","title":"NoCopyDict"},{"location":"reference/arti/internal/utils/#ancestors-in-mro","text":"builtins.dict arti.internal.utils.NoCopyMixin","title":"Ancestors (in MRO)"},{"location":"reference/arti/internal/utils/#methods","text":"","title":"Methods"},{"location":"reference/arti/internal/utils/#clear","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/arti/internal/utils/#copy","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/arti/internal/utils/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/arti/internal/utils/#get","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/arti/internal/utils/#items","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/arti/internal/utils/#keys","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/arti/internal/utils/#pop","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If the key is not found, return the default if given; otherwise, raise a KeyError.","title":"pop"},{"location":"reference/arti/internal/utils/#popitem","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/arti/internal/utils/#setdefault","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/arti/internal/utils/#update","text":"def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k]","title":"update"},{"location":"reference/arti/internal/utils/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/arti/internal/utils/#nocopymixin","text":"class NoCopyMixin ( / , * args , ** kwargs ) View Source class NoCopyMixin: \"\"\"Mixin to bypass (deep)copying. This is useful for objects that are *intended* to be stateful and preserved, despite usually preferring immutable data structures and Pydantic models, which (deep)copy often. \"\"\" def __copy__ ( self ) -> Self: return self # pragma: no cover def __deepcopy__ ( self , memo: Any ) -> Self: return self # pragma: no cover","title":"NoCopyMixin"},{"location":"reference/arti/internal/utils/#descendants","text":"arti.internal.utils.NoCopyDict arti.backends.memory._NoCopyContainer","title":"Descendants"},{"location":"reference/arti/internal/utils/#typedbox","text":"class TypedBox ( * args : Any , default_box : bool = False , default_box_attr : Any = < object object at 0x7f1a4d988080 > , default_box_none_transform : bool = True , default_box_create_on_get : bool = True , frozen_box : bool = False , camel_killer_box : bool = False , conversion_box : bool = True , modify_tuples_box : bool = False , box_safe_prefix : str = 'x' , box_duplicates : str = 'ignore' , box_intact_types : Union [ Tuple , List ] = (), box_recast : Optional [ Dict ] = None , box_dots : bool = False , box_class : Union [ Dict , Type [ ForwardRef ( 'Box' )], NoneType ] = None , box_namespace : Tuple [ str , ... ] = (), ** kwargs : Any ) View Source class TypedBox ( Box , MutableMapping [ str, Union[_V, MutableMapping[str, _V ] ]] ) : \"\"\"TypedBox holds a collection of typed values. Subclasses must set the __target_type__ to a base class for the contained values. \"\"\" __target_type__ : ClassVar [ type[_V ] ] # type : ignore [ misc ] @classmethod def __class_getitem__ ( cls , item : type [ _V ] ) -> GenericAlias : if isinstance ( item , tuple ) : raise TypeError ( f \"{cls.__name__} expects a single value type\" ) value_type = item return GenericAlias ( type ( cls . __name__ , ( cls ,), { \"__module__\" : get_module_name ( depth = 2 ), # Set to our caller ' s module \"__target_type__\" : value_type , } , ), item , ) def __setattr__ ( self , key : str , value : Any ) -> None : # GenericAlias sets __orig_class__ after __init__ , so preempt Box from storing that ( or # erroring if frozen ). if key == \"__orig_class__\" : return object . __setattr__ ( self , key , value ) super (). __setattr__ ( key , value ) return None def __cast_value ( self , item : str , value : Any ) -> _V : if isinstance ( value , self . __target_type__ ) : return value tgt_name = self . __target_type__ . __name__ if hasattr ( self . __target_type__ , \"cast\" ) : casted = cast ( Any , self . __target_type__ ). cast ( value ) if isinstance ( casted , self . __target_type__ ) : return casted raise TypeError ( f \"Expected {tgt_name}.cast({value}) to return an instance of {tgt_name}, got: {casted}\" ) raise TypeError ( f \"Expected an instance of {tgt_name}, got: {value}\" ) # NOTE : Box uses name mangling ( double __ ) to prevent conflicts with contained values . def _Box__convert_and_store ( self , item : str , value : _V ) -> None : if isinstance ( value , dict ) : super (). _Box__convert_and_store ( item , value ) # pylint : disable = no - member elif item in self : raise ValueError ( f \"{item} is already set!\" ) else : super (). _Box__convert_and_store ( item , self . __cast_value ( item , value )) def walk ( self , root : tuple [ str, ... ] = ()) -> Iterator [ tuple[str, _V ] ]: for k , v in self . items () : subroot = ( * root , k ) if isinstance ( v , TypedBox ) : yield from v . walk ( root = subroot ) else : yield \".\" . join ( subroot ), v # type : ignore [ misc ]","title":"TypedBox"},{"location":"reference/arti/internal/utils/#ancestors-in-mro_1","text":"box.box.Box builtins.dict collections.abc.MutableMapping collections.abc.Mapping collections.abc.Collection collections.abc.Sized collections.abc.Iterable collections.abc.Container","title":"Ancestors (in MRO)"},{"location":"reference/arti/internal/utils/#descendants_1","text":"arti.graphs.TypedBox","title":"Descendants"},{"location":"reference/arti/internal/utils/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/internal/utils/#from_json","text":"def from_json ( json_string : Optional [ str ] = None , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transform a json object string into a Box object. If the incoming json is a list, you must use BoxList.from_json. Parameters: Name Type Description Default json_string None string to pass to json.loads None filename None filename to open and pass to json.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() or json.loads None Returns: Type Description None Box object from json data View Source @classmethod def from_json ( cls , json_string : Optional [ str ] = None , filename : Optional [ Union[str, PathLike ] ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transform a json object string into a Box object. If the incoming json is a list, you must use BoxList.from_json. :param json_string: string to pass to `json.loads` :param filename: filename to open and pass to `json.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` or `json.loads` :return: Box object from json data \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_json ( json_string , filename = filename , encoding = encoding , errors = errors , ** kwargs ) if not isinstance ( data , dict ) : raise BoxError ( f \"json data not returned as a dictionary, but rather a {type(data).__name__}\" ) return cls ( data , ** box_args )","title":"from_json"},{"location":"reference/arti/internal/utils/#from_msgpack","text":"def from_msgpack ( msgpack_bytes : Optional [ bytes ] = None , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' View Source @classmethod def from_msgpack ( cls , msgpack_bytes : Optional [ bytes ] = None , filename : Optional [ Union[str, PathLike ] ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : raise BoxError ( 'msgpack is unavailable on this system, please install the \"msgpack\" package' )","title":"from_msgpack"},{"location":"reference/arti/internal/utils/#from_toml","text":"def from_toml ( toml_string : Optional [ str ] = None , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transforms a toml string or file into a Box object Parameters: Name Type Description Default toml_string None string to pass to toml.load None filename None filename to open and pass to toml.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() None Returns: Type Description None Box object View Source @classmethod def from_toml ( cls , toml_string : Optional [ str ] = None , filename : Optional [ Union[str, PathLike ] ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transforms a toml string or file into a Box object :param toml_string: string to pass to `toml.load` :param filename: filename to open and pass to `toml.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` :return: Box object \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_toml ( toml_string = toml_string , filename = filename , encoding = encoding , errors = errors ) return cls ( data , ** box_args )","title":"from_toml"},{"location":"reference/arti/internal/utils/#from_yaml","text":"def from_yaml ( yaml_string : Optional [ str ] = None , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transform a yaml object string into a Box object. By default will use SafeLoader. Parameters: Name Type Description Default yaml_string None string to pass to yaml.load None filename None filename to open and pass to yaml.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() or yaml.load None Returns: Type Description None Box object from yaml data View Source @classmethod def from_yaml ( cls , yaml_string : Optional [ str ] = None , filename : Optional [ Union[str, PathLike ] ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transform a yaml object string into a Box object. By default will use SafeLoader. :param yaml_string: string to pass to `yaml.load` :param filename: filename to open and pass to `yaml.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` or `yaml.load` :return: Box object from yaml data \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_yaml ( yaml_string = yaml_string , filename = filename , encoding = encoding , errors = errors , ** kwargs ) if not data : return cls ( ** box_args ) if not isinstance ( data , dict ) : raise BoxError ( f \"yaml data not returned as a dictionary but rather a {type(data).__name__}\" ) return cls ( data , ** box_args )","title":"from_yaml"},{"location":"reference/arti/internal/utils/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/internal/utils/#clear_1","text":"def clear ( self ) D.clear() -> None. Remove all items from D. View Source def clear ( self ) : if self . _box_config [ \"frozen_box\" ]: raise BoxError ( \"Box is frozen\" ) super () . clear () self . _box_config [ \"__safe_keys\" ]. clear ()","title":"clear"},{"location":"reference/arti/internal/utils/#copy_1","text":"def copy ( self ) -> 'Box' D.copy() -> a shallow copy of D View Source def copy ( self ) -> \"Box\" : config = self . __box_config () config . pop ( \"box_namespace\" ) # Detach namespace ; it will be reassigned if we nest again return Box ( super (). copy (), ** config )","title":"copy"},{"location":"reference/arti/internal/utils/#fromkeys_1","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/arti/internal/utils/#get_1","text":"def get ( self , key , default =< object object at 0x7f1a4d988080 > ) Return the value for key if key is in the dictionary, else default. View Source def get ( self , key , default = NO_DEFAULT ) : if key not in self : if default is NO_DEFAULT : if self . _box_config [ \"default_box\" ] and self . _box_config [ \"default_box_none_transform\" ] : return self . __get_default ( key ) else : return None if isinstance ( default , dict ) and not isinstance ( default , Box ) : return Box ( default ) if isinstance ( default , list ) and not isinstance ( default , box . BoxList ) : return box . BoxList ( default ) return default return self [ key ]","title":"get"},{"location":"reference/arti/internal/utils/#items_1","text":"def items ( self , dotted : bool = False ) D.items() -> a set-like object providing a view on D's items View Source def items ( self , dotted : Union [ bool ] = False ) : if not dotted : return super (). items () if not self . _box_config [ \"box_dots\" ] : raise BoxError ( \"Cannot return dotted keys as this Box does not have `box_dots` enabled\" ) return [ (k, self[k ] ) for k in self . keys ( dotted = True ) ]","title":"items"},{"location":"reference/arti/internal/utils/#keys_1","text":"def keys ( self , dotted : bool = False ) D.keys() -> a set-like object providing a view on D's keys View Source def keys ( self , dotted : Union [ bool ] = False ) : if not dotted : return super (). keys () if not self . _box_config [ \"box_dots\" ] : raise BoxError ( \"Cannot return dotted keys as this Box does not have `box_dots` enabled\" ) keys = set () for key , value in self . items () : added = False if isinstance ( key , str ) : if isinstance ( value , Box ) : for sub_key in value . keys ( dotted = True ) : keys . add ( f \"{key}.{sub_key}\" ) added = True elif isinstance ( value , box . BoxList ) : for pos in value . _dotted_helper () : keys . add ( f \"{key}{pos}\" ) added = True if not added : keys . add ( key ) return sorted ( keys , key = lambda x : str ( x ))","title":"keys"},{"location":"reference/arti/internal/utils/#merge_update","text":"def merge_update ( self , * args , ** kwargs ) View Source def merge_update ( self , * args , ** kwargs ) : merge_type = None if \"box_merge_lists\" in kwargs : merge_type = kwargs . pop ( \"box_merge_lists\" ) def convert_and_set ( k , v ) : intact_type = self . _box_config [ \"box_intact_types\" ] and isinstance ( v , self . _box_config [ \"box_intact_types\" ] ) if isinstance ( v , dict ) and not intact_type : # Box objects must be created in case they are already # in the ` converted ` box_config set v = self . _box_config [ \"box_class\" ] ( v , ** self . __box_config ( extra_namespace = k )) if k in self and isinstance ( self [ k ] , dict ) : self [ k ] . merge_update ( v ) return if isinstance ( v , list ) and not intact_type : v = box . BoxList ( v , ** self . __box_config ( extra_namespace = k )) if merge_type == \"extend\" and k in self and isinstance ( self [ k ] , list ) : self [ k ] . extend ( v ) return if merge_type == \"unique\" and k in self and isinstance ( self [ k ] , list ) : for item in v : if item not in self [ k ] : self [ k ] . append ( item ) return self . __setitem__ ( k , v ) if ( len ( args ) + int ( bool ( kwargs ))) > 1 : raise BoxTypeError ( f \"merge_update expected at most 1 argument, got {len(args) + int(bool(kwargs))}\" ) single_arg = next ( iter ( args ), None ) if single_arg : if hasattr ( single_arg , \"keys\" ) : for k in single_arg : convert_and_set ( k , single_arg [ k ] ) else : for k , v in single_arg : convert_and_set ( k , v ) for key in kwargs : convert_and_set ( key , kwargs [ key ] )","title":"merge_update"},{"location":"reference/arti/internal/utils/#pop_1","text":"def pop ( self , key , * args ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If the key is not found, return the default if given; otherwise, raise a KeyError. View Source def pop ( self , key , * args ) : if self . _box_config [ \"frozen_box\" ] : raise BoxError ( \"Box is frozen\" ) if args : if len ( args ) != 1 : raise BoxError ( 'pop() takes only one optional argument \"default\"' ) try : item = self [ key ] except KeyError : return args [ 0 ] else : del self [ key ] return item try : item = self [ key ] except KeyError : raise BoxKeyError ( f \"{key}\" ) from None else : del self [ key ] return item","title":"pop"},{"location":"reference/arti/internal/utils/#popitem_1","text":"def popitem ( self ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. View Source def popitem ( self ) : if self . _box_config [ \"frozen_box\" ]: raise BoxError ( \"Box is frozen\" ) try : key = next ( self . __iter__ ()) except StopIteration : raise BoxKeyError ( \"Empty box\" ) from None return key , self . pop ( key )","title":"popitem"},{"location":"reference/arti/internal/utils/#setdefault_1","text":"def setdefault ( self , item , default = None ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. View Source def setdefault ( self , item , default = None ) : if item in self : return self [ item ] if self . _box_config [ \"box_dots\" ] : if item in _get_dot_paths ( self ) : return self [ item ] if isinstance ( default , dict ) : default = self . _box_config [ \"box_class\" ] ( default , ** self . __box_config ( extra_namespace = item )) if isinstance ( default , list ) : default = box . BoxList ( default , ** self . __box_config ( extra_namespace = item )) self [ item ] = default return self [ item ]","title":"setdefault"},{"location":"reference/arti/internal/utils/#to_dict","text":"def to_dict ( self ) -> Dict Turn the Box and sub Boxes back into a native python dictionary. Returns: Type Description None python dictionary of this Box View Source def to_dict ( self ) -> Dict : \"\"\" Turn the Box and sub Boxes back into a native python dictionary. :return: python dictionary of this Box \"\"\" out_dict = dict ( self ) for k , v in out_dict . items () : if v is self : out_dict [ k ] = out_dict elif isinstance ( v , Box ) : out_dict [ k ] = v . to_dict () elif isinstance ( v , box . BoxList ) : out_dict [ k ] = v . to_list () return out_dict","title":"to_dict"},{"location":"reference/arti/internal/utils/#to_json","text":"def to_json ( self , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** json_kwargs ) Transform the Box object into a JSON string. Parameters: Name Type Description Default filename None If provided will save to file None encoding None File encoding None errors None How to handle encoding errors None json_kwargs None additional arguments to pass to json.dump(s) None Returns: Type Description None string of JSON (if no filename provided) View Source def to_json ( self , filename : Optional [ Union [ str , PathLike ]] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** json_kwargs , ) : \"\" \" Transform the Box object into a JSON string. :param filename: If provided will save to file :param encoding: File encoding :param errors: How to handle encoding errors :param json_kwargs: additional arguments to pass to json.dump(s) :return: string of JSON (if no filename provided) \"\" \" return _to_json(self.to_dict(), filename=filename, encoding=encoding, errors=errors, **json_kwargs)","title":"to_json"},{"location":"reference/arti/internal/utils/#to_msgpack","text":"def to_msgpack ( self , filename : Union [ str , os . PathLike , NoneType ] = None , ** kwargs ) View Source def to_msgpack(self, filename: Optional[Union[str, PathLike]] = None, **kwargs): raise BoxError('msgpack is unavailable on this system, please install the \"msgpack\" package')","title":"to_msgpack"},{"location":"reference/arti/internal/utils/#to_toml","text":"def to_toml ( self , filename : Union [ str , os . PathLike , NoneType ] = None , encoding : str = 'utf-8' , errors : str = 'strict' ) Transform the Box object into a toml string. Parameters: Name Type Description Default filename None File to write toml object too None encoding None File encoding None errors None How to handle encoding errors None Returns: Type Description None string of TOML (if no filename provided) View Source def to_toml ( self , filename : Optional [ Union [ str , PathLike ]] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" ) : \"\" \" Transform the Box object into a toml string. :param filename: File to write toml object too :param encoding: File encoding :param errors: How to handle encoding errors :return: string of TOML (if no filename provided) \"\" \" return _to_toml(self.to_dict(), filename=filename, encoding=encoding, errors=errors)","title":"to_toml"},{"location":"reference/arti/internal/utils/#to_yaml","text":"def to_yaml ( self , filename : Union [ str , os . PathLike , NoneType ] = None , default_flow_style : bool = False , encoding : str = 'utf-8' , errors : str = 'strict' , ** yaml_kwargs ) Transform the Box object into a YAML string. Parameters: Name Type Description Default filename None If provided will save to file None default_flow_style None False will recursively dump dicts None encoding None File encoding None errors None How to handle encoding errors None yaml_kwargs None additional arguments to pass to yaml.dump None Returns: Type Description None string of YAML (if no filename provided) View Source def to_yaml ( self , filename : Optional [ Union [ str , PathLike ]] = None , default_flow_style : bool = False , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** yaml_kwargs , ) : \"\" \" Transform the Box object into a YAML string. :param filename: If provided will save to file :param default_flow_style: False will recursively dump dicts :param encoding: File encoding :param errors: How to handle encoding errors :param yaml_kwargs: additional arguments to pass to yaml.dump :return: string of YAML (if no filename provided) \"\" \" return _to_yaml( self.to_dict(), filename=filename, default_flow_style=default_flow_style, encoding=encoding, errors=errors, **yaml_kwargs, )","title":"to_yaml"},{"location":"reference/arti/internal/utils/#update_1","text":"def update ( self , * args , ** kwargs ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] View Source def update ( self , * args , ** kwargs ) : if self . _box_config [ \"frozen_box\" ] : raise BoxError ( \"Box is frozen\" ) if ( len ( args ) + int ( bool ( kwargs ))) > 1 : raise BoxTypeError ( f \"update expected at most 1 argument, got {len(args) + int(bool(kwargs))}\" ) single_arg = next ( iter ( args ), None ) if single_arg : if hasattr ( single_arg , \"keys\" ) : for k in single_arg : self . __convert_and_store ( k , single_arg [ k ] ) else : for k , v in single_arg : self . __convert_and_store ( k , v ) for k in kwargs : self . __convert_and_store ( k , kwargs [ k ] )","title":"update"},{"location":"reference/arti/internal/utils/#values_1","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/arti/internal/utils/#walk","text":"def walk ( self , root : 'tuple[str, ...]' = () ) -> 'Iterator[tuple[str, _V]]' View Source def walk ( self , root : tuple [ str, ... ] = ()) -> Iterator [ tuple[str, _V ] ]: for k , v in self . items () : subroot = ( * root , k ) if isinstance ( v , TypedBox ) : yield from v . walk ( root = subroot ) else : yield \".\" . join ( subroot ), v # type : ignore [ misc ]","title":"walk"},{"location":"reference/arti/internal/utils/#class_name","text":"class class_name ( / , * args , ** kwargs ) View Source class ClassName : def __get__ ( self , obj : Any , type_ : type [ Any ] ) -> str : return type_ . __name__","title":"class_name"},{"location":"reference/arti/internal/utils/#frozendict","text":"class frozendict ( arg : 'Union[Mapping[_K, _V], Iterable[tuple[_K, _V]]]' = (), ** kwargs : '_V' ) View Source class frozendict ( Mapping [ _K, _V ] ) : def __init__ ( self , arg : Union [ Mapping[_K, _V ] , Iterable [ tuple[_K, _V ] ]] = (), ** kwargs : _V ) -> None : self . _data = dict [ _K, _V ] ( arg , ** kwargs ) # Eagerly evaluate the hash to confirm elements are also frozen ( via frozenset ) at # creation time , not just when hashed . self . _hash = hash ( frozenset ( self . _data . items ())) def __getitem__ ( self , key : _K ) -> _V : return self . _data [ key ] def __hash__ ( self ) -> int : return self . _hash def __iter__ ( self ) -> Iterator [ _K ] : return iter ( self . _data ) def __len__ ( self ) -> int : return len ( self . _data ) def __or__ ( self , other : Mapping [ _K, _V ] ) -> frozendict [ _K, _V ] : return type ( self )( { ** self , ** other } ) __ror__ = __or__ def __repr__ ( self ) -> str : return repr ( self . _data )","title":"frozendict"},{"location":"reference/arti/internal/utils/#ancestors-in-mro_2","text":"collections.abc.Mapping collections.abc.Collection collections.abc.Sized collections.abc.Iterable collections.abc.Container","title":"Ancestors (in MRO)"},{"location":"reference/arti/internal/utils/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/internal/utils/#get_2","text":"def get ( self , key , default = None ) D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None.","title":"get"},{"location":"reference/arti/internal/utils/#items_2","text":"def items ( self ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/arti/internal/utils/#keys_2","text":"def keys ( self ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/arti/internal/utils/#values_2","text":"def values ( self ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/arti/internal/utils/#int64","text":"class int64 ( / , * args , ** kwargs ) View Source class int64 ( _int ): _min , _max = -( 2 ** 63 ), ( 2 ** 63 ) - 1 def __new__ ( cls , i: Union [ int , int64 , uint64 ]) -> int64: if i > cls . _max: if isinstance ( i , uint64 ): i = int ( i ) - uint64 . _max - 1 else: raise ValueError ( f \"{i} is too large for int64. Hint: cast to uint64 first.\" ) if i < cls . _min: raise ValueError ( f \"{i} is too small for int64.\" ) return super (). __new__ ( cls , i )","title":"int64"},{"location":"reference/arti/internal/utils/#ancestors-in-mro_3","text":"arti.internal.utils._int builtins.int","title":"Ancestors (in MRO)"},{"location":"reference/arti/internal/utils/#class-variables","text":"denominator imag numerator real","title":"Class variables"},{"location":"reference/arti/internal/utils/#methods_3","text":"","title":"Methods"},{"location":"reference/arti/internal/utils/#as_integer_ratio","text":"def as_integer_ratio ( self , / ) Return integer ratio. Return a pair of integers, whose ratio is exactly equal to the original int and with a positive denominator. (10).as_integer_ratio() (10, 1) (-10).as_integer_ratio() (-10, 1) (0).as_integer_ratio() (0, 1)","title":"as_integer_ratio"},{"location":"reference/arti/internal/utils/#bit_count","text":"def bit_count ( self , / ) Number of ones in the binary representation of the absolute value of self. Also known as the population count. bin(13) '0b1101' (13).bit_count() 3","title":"bit_count"},{"location":"reference/arti/internal/utils/#bit_length","text":"def bit_length ( self , / ) Number of bits necessary to represent self in binary. bin(37) '0b100101' (37).bit_length() 6","title":"bit_length"},{"location":"reference/arti/internal/utils/#conjugate","text":"def conjugate ( ... ) Returns self, the complex conjugate of any int.","title":"conjugate"},{"location":"reference/arti/internal/utils/#from_bytes","text":"def from_bytes ( bytes , byteorder = 'big' , * , signed = False ) Return the integer represented by the given array of bytes. bytes Holds the array of bytes to convert. The argument must either support the buffer protocol or be an iterable object producing bytes. Bytes and bytearray are examples of built-in objects that support the buffer protocol. byteorder The byte order used to represent the integer. If byteorder is 'big', the most significant byte is at the beginning of the byte array. If byteorder is 'little', the most significant byte is at the end of the byte array. To request the native byte order of the host system, use `sys.byteorder' as the byte order value. Default is to use 'big'. signed Indicates whether two's complement is used to represent the integer.","title":"from_bytes"},{"location":"reference/arti/internal/utils/#to_bytes","text":"def to_bytes ( self , / , length = 1 , byteorder = 'big' , * , signed = False ) Return an array of bytes representing an integer. length Length of bytes object to use. An OverflowError is raised if the integer is not representable with the given number of bytes. Default is length 1. byteorder The byte order used to represent the integer. If byteorder is 'big', the most significant byte is at the beginning of the byte array. If byteorder is 'little', the most significant byte is at the end of the byte array. To request the native byte order of the host system, use `sys.byteorder' as the byte order value. Default is to use 'big'. signed Determines whether two's complement is used to represent the integer. If signed is False and a negative integer is given, an OverflowError is raised.","title":"to_bytes"},{"location":"reference/arti/internal/utils/#uint64","text":"class uint64 ( / , * args , ** kwargs ) View Source class uint64 ( _int ): _min , _max = 0 , ( 2 ** 64 ) - 1 def __new__ ( cls , i: Union [ int , int64 , uint64 ]) -> uint64: if i > cls . _max: raise ValueError ( f \"{i} is too large for uint64.\" ) if i < cls . _min: if isinstance ( i , int64 ): i = int ( i ) + cls . _max + 1 else: raise ValueError ( f \"{i} is negative. Hint: cast to int64 first.\" ) return super (). __new__ ( cls , i )","title":"uint64"},{"location":"reference/arti/internal/utils/#ancestors-in-mro_4","text":"arti.internal.utils._int builtins.int","title":"Ancestors (in MRO)"},{"location":"reference/arti/internal/utils/#class-variables_1","text":"denominator imag numerator real","title":"Class variables"},{"location":"reference/arti/internal/utils/#methods_4","text":"","title":"Methods"},{"location":"reference/arti/internal/utils/#as_integer_ratio_1","text":"def as_integer_ratio ( self , / ) Return integer ratio. Return a pair of integers, whose ratio is exactly equal to the original int and with a positive denominator. (10).as_integer_ratio() (10, 1) (-10).as_integer_ratio() (-10, 1) (0).as_integer_ratio() (0, 1)","title":"as_integer_ratio"},{"location":"reference/arti/internal/utils/#bit_count_1","text":"def bit_count ( self , / ) Number of ones in the binary representation of the absolute value of self. Also known as the population count. bin(13) '0b1101' (13).bit_count() 3","title":"bit_count"},{"location":"reference/arti/internal/utils/#bit_length_1","text":"def bit_length ( self , / ) Number of bits necessary to represent self in binary. bin(37) '0b100101' (37).bit_length() 6","title":"bit_length"},{"location":"reference/arti/internal/utils/#conjugate_1","text":"def conjugate ( ... ) Returns self, the complex conjugate of any int.","title":"conjugate"},{"location":"reference/arti/internal/utils/#from_bytes_1","text":"def from_bytes ( bytes , byteorder = 'big' , * , signed = False ) Return the integer represented by the given array of bytes. bytes Holds the array of bytes to convert. The argument must either support the buffer protocol or be an iterable object producing bytes. Bytes and bytearray are examples of built-in objects that support the buffer protocol. byteorder The byte order used to represent the integer. If byteorder is 'big', the most significant byte is at the beginning of the byte array. If byteorder is 'little', the most significant byte is at the end of the byte array. To request the native byte order of the host system, use `sys.byteorder' as the byte order value. Default is to use 'big'. signed Indicates whether two's complement is used to represent the integer.","title":"from_bytes"},{"location":"reference/arti/internal/utils/#to_bytes_1","text":"def to_bytes ( self , / , length = 1 , byteorder = 'big' , * , signed = False ) Return an array of bytes representing an integer. length Length of bytes object to use. An OverflowError is raised if the integer is not representable with the given number of bytes. Default is length 1. byteorder The byte order used to represent the integer. If byteorder is 'big', the most significant byte is at the beginning of the byte array. If byteorder is 'little', the most significant byte is at the end of the byte array. To request the native byte order of the host system, use `sys.byteorder' as the byte order value. Default is to use 'big'. signed Determines whether two's complement is used to represent the integer. If signed is False and a negative integer is given, an OverflowError is raised.","title":"to_bytes"},{"location":"reference/arti/io/","text":"Module arti.io None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from collections.abc import Sequence from types import ModuleType from typing import Any , Optional from arti.formats import Format from arti.internal.dispatch import multipledispatch from arti.internal.utils import import_submodules from arti.storage import StoragePartition , StoragePartitionVar from arti.types import Type , is_partitioned from arti.views import View _submodules : Optional [ dict [ str , ModuleType ]] = None def _discover () -> None : global _submodules if _submodules is None : _submodules = import_submodules ( __path__ , __name__ ) @multipledispatch ( \"io.read\" , discovery_func = _discover ) def _read ( type_ : Type , format : Format , storage_partitions : Sequence [ StoragePartition ], view : View ) -> Any : raise NotImplementedError ( f \"Reading { type ( storage_partitions [ 0 ]) } storage in { type ( format ) } format to { type ( view ) } view is not implemented.\" ) register_reader = _read . register def read ( type_ : Type , format : Format , storage_partitions : Sequence [ StoragePartition ], view : View ) -> Any : if not storage_partitions : # NOTE: Aside from simplifying this check up front, multiple dispatch with unknown list # element type can be ambiguous/error. raise FileNotFoundError ( \"No data\" ) if len ( storage_partitions ) > 1 and not is_partitioned ( type_ ): raise ValueError ( f \"Multiple partitions can only be read into a partitioned Collection, not { type_ } \" ) # TODO Checks that the returned data matches the Type/View # # Likely add a View method that can handle this type + schema checking, filtering to column/row subsets if necessary, etc return _read ( type_ , format , storage_partitions , view ) @multipledispatch ( \"io.write\" , discovery_func = _discover ) # type: ignore[arg-type] def _write ( data : Any , type_ : Type , format : Format , storage_partition : StoragePartitionVar , view : View ) -> Optional [ StoragePartitionVar ]: raise NotImplementedError ( f \"Writing { type ( view ) } view into { type ( format ) } format in { type ( storage_partition ) } storage is not implemented.\" ) register_writer = _write . register def write ( data : Any , type_ : Type , format : Format , storage_partition : StoragePartitionVar , view : View ) -> StoragePartitionVar : if ( updated := _write ( data , type_ , format , storage_partition , view )) is not None : return updated return storage_partition Sub-modules arti.io.json_gcsfile_python arti.io.json_localfile_python arti.io.json_stringliteral_python arti.io.pickle_gcsfile_python arti.io.pickle_localfile_python Functions read def read ( type_ : 'Type' , format : 'Format' , storage_partitions : 'Sequence[StoragePartition]' , view : 'View' ) -> 'Any' View Source def read ( type_ : Type , format : Format , storage_partitions : Sequence [ StoragePartition ] , view : View ) -> Any : if not storage_partitions : # NOTE : Aside from simplifying this check up front , multiple dispatch with unknown list # element type can be ambiguous / error . raise FileNotFoundError ( \"No data\" ) if len ( storage_partitions ) > 1 and not is_partitioned ( type_ ) : raise ValueError ( f \"Multiple partitions can only be read into a partitioned Collection, not {type_}\" ) # TODO Checks that the returned data matches the Type / View # # Likely add a View method that can handle this type + schema checking , filtering to column / row subsets if necessary , etc return _read ( type_ , format , storage_partitions , view ) register_reader def register_reader ( * args : 'Any' ) -> 'Callable[[REGISTERED], REGISTERED]' Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return cast ( Callable [ ..., Any ] , super (). register ( * args )) register_writer def register_writer ( * args : 'Any' ) -> 'Callable[[REGISTERED], REGISTERED]' Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return cast ( Callable [ ..., Any ] , super (). register ( * args )) write def write ( data : 'Any' , type_ : 'Type' , format : 'Format' , storage_partition : 'StoragePartitionVar' , view : 'View' ) -> 'StoragePartitionVar' View Source def write ( data : Any , type_ : Type , for mat : Format , storage_partition : StoragePartitionVar , view : View ) -> StoragePartitionVar : if ( updated := _write ( data , type_ , for mat , storage_partition , view )) is not None : return updated return storage_partition","title":"Index"},{"location":"reference/arti/io/#module-artiio","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from collections.abc import Sequence from types import ModuleType from typing import Any , Optional from arti.formats import Format from arti.internal.dispatch import multipledispatch from arti.internal.utils import import_submodules from arti.storage import StoragePartition , StoragePartitionVar from arti.types import Type , is_partitioned from arti.views import View _submodules : Optional [ dict [ str , ModuleType ]] = None def _discover () -> None : global _submodules if _submodules is None : _submodules = import_submodules ( __path__ , __name__ ) @multipledispatch ( \"io.read\" , discovery_func = _discover ) def _read ( type_ : Type , format : Format , storage_partitions : Sequence [ StoragePartition ], view : View ) -> Any : raise NotImplementedError ( f \"Reading { type ( storage_partitions [ 0 ]) } storage in { type ( format ) } format to { type ( view ) } view is not implemented.\" ) register_reader = _read . register def read ( type_ : Type , format : Format , storage_partitions : Sequence [ StoragePartition ], view : View ) -> Any : if not storage_partitions : # NOTE: Aside from simplifying this check up front, multiple dispatch with unknown list # element type can be ambiguous/error. raise FileNotFoundError ( \"No data\" ) if len ( storage_partitions ) > 1 and not is_partitioned ( type_ ): raise ValueError ( f \"Multiple partitions can only be read into a partitioned Collection, not { type_ } \" ) # TODO Checks that the returned data matches the Type/View # # Likely add a View method that can handle this type + schema checking, filtering to column/row subsets if necessary, etc return _read ( type_ , format , storage_partitions , view ) @multipledispatch ( \"io.write\" , discovery_func = _discover ) # type: ignore[arg-type] def _write ( data : Any , type_ : Type , format : Format , storage_partition : StoragePartitionVar , view : View ) -> Optional [ StoragePartitionVar ]: raise NotImplementedError ( f \"Writing { type ( view ) } view into { type ( format ) } format in { type ( storage_partition ) } storage is not implemented.\" ) register_writer = _write . register def write ( data : Any , type_ : Type , format : Format , storage_partition : StoragePartitionVar , view : View ) -> StoragePartitionVar : if ( updated := _write ( data , type_ , format , storage_partition , view )) is not None : return updated return storage_partition","title":"Module arti.io"},{"location":"reference/arti/io/#sub-modules","text":"arti.io.json_gcsfile_python arti.io.json_localfile_python arti.io.json_stringliteral_python arti.io.pickle_gcsfile_python arti.io.pickle_localfile_python","title":"Sub-modules"},{"location":"reference/arti/io/#functions","text":"","title":"Functions"},{"location":"reference/arti/io/#read","text":"def read ( type_ : 'Type' , format : 'Format' , storage_partitions : 'Sequence[StoragePartition]' , view : 'View' ) -> 'Any' View Source def read ( type_ : Type , format : Format , storage_partitions : Sequence [ StoragePartition ] , view : View ) -> Any : if not storage_partitions : # NOTE : Aside from simplifying this check up front , multiple dispatch with unknown list # element type can be ambiguous / error . raise FileNotFoundError ( \"No data\" ) if len ( storage_partitions ) > 1 and not is_partitioned ( type_ ) : raise ValueError ( f \"Multiple partitions can only be read into a partitioned Collection, not {type_}\" ) # TODO Checks that the returned data matches the Type / View # # Likely add a View method that can handle this type + schema checking , filtering to column / row subsets if necessary , etc return _read ( type_ , format , storage_partitions , view )","title":"read"},{"location":"reference/arti/io/#register_reader","text":"def register_reader ( * args : 'Any' ) -> 'Callable[[REGISTERED], REGISTERED]' Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return cast ( Callable [ ..., Any ] , super (). register ( * args ))","title":"register_reader"},{"location":"reference/arti/io/#register_writer","text":"def register_writer ( * args : 'Any' ) -> 'Callable[[REGISTERED], REGISTERED]' Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return cast ( Callable [ ..., Any ] , super (). register ( * args ))","title":"register_writer"},{"location":"reference/arti/io/#write","text":"def write ( data : 'Any' , type_ : 'Type' , format : 'Format' , storage_partition : 'StoragePartitionVar' , view : 'View' ) -> 'StoragePartitionVar' View Source def write ( data : Any , type_ : Type , for mat : Format , storage_partition : StoragePartitionVar , view : View ) -> StoragePartitionVar : if ( updated := _write ( data , type_ , for mat , storage_partition , view )) is not None : return updated return storage_partition","title":"write"},{"location":"reference/arti/io/json_gcsfile_python/","text":"Module arti.io.json_gcsfile_python None None View Source from __future__ import annotations import json from collections.abc import Sequence from itertools import chain from typing import Any from gcsfs import GCSFileSystem from arti.formats.json import JSON from arti.io import register_reader , register_writer from arti.storage.google.cloud.storage import GCSFilePartition from arti.types import Type , is_partitioned from arti.views.python import PythonBuiltin # TODO: Do I need to inject the partition keys into the returned data? Likely useful... # Maybe a View option? def _read_json_file ( path : str ) -> Any : # TODO: GCSFileSystem needs to be injected somehow with GCSFileSystem () . open ( path , \"r\" ) as file : return json . load ( file ) @register_reader def _read_json_gcsfile_python ( type_ : Type , format : JSON , storage_partitions : Sequence [ GCSFilePartition ], view : PythonBuiltin , ) -> Any : if is_partitioned ( type_ ): return list ( chain . from_iterable ( _read_json_file ( storage_partition . qualified_path ) for storage_partition in storage_partitions ) ) assert len ( storage_partitions ) == 1 # Better error handled in base read return _read_json_file ( storage_partitions [ 0 ] . qualified_path ) @register_writer def _write_json_gcsfile_python ( data : Any , type_ : Type , format : JSON , storage_partition : GCSFilePartition , view : PythonBuiltin ) -> None : with GCSFileSystem () . open ( storage_partition . qualified_path , \"w\" ) as file : json . dump ( data , file )","title":"Json Gcsfile Python"},{"location":"reference/arti/io/json_gcsfile_python/#module-artiiojson_gcsfile_python","text":"None None View Source from __future__ import annotations import json from collections.abc import Sequence from itertools import chain from typing import Any from gcsfs import GCSFileSystem from arti.formats.json import JSON from arti.io import register_reader , register_writer from arti.storage.google.cloud.storage import GCSFilePartition from arti.types import Type , is_partitioned from arti.views.python import PythonBuiltin # TODO: Do I need to inject the partition keys into the returned data? Likely useful... # Maybe a View option? def _read_json_file ( path : str ) -> Any : # TODO: GCSFileSystem needs to be injected somehow with GCSFileSystem () . open ( path , \"r\" ) as file : return json . load ( file ) @register_reader def _read_json_gcsfile_python ( type_ : Type , format : JSON , storage_partitions : Sequence [ GCSFilePartition ], view : PythonBuiltin , ) -> Any : if is_partitioned ( type_ ): return list ( chain . from_iterable ( _read_json_file ( storage_partition . qualified_path ) for storage_partition in storage_partitions ) ) assert len ( storage_partitions ) == 1 # Better error handled in base read return _read_json_file ( storage_partitions [ 0 ] . qualified_path ) @register_writer def _write_json_gcsfile_python ( data : Any , type_ : Type , format : JSON , storage_partition : GCSFilePartition , view : PythonBuiltin ) -> None : with GCSFileSystem () . open ( storage_partition . qualified_path , \"w\" ) as file : json . dump ( data , file )","title":"Module arti.io.json_gcsfile_python"},{"location":"reference/arti/io/json_localfile_python/","text":"Module arti.io.json_localfile_python None None View Source from __future__ import annotations import json from collections.abc import Sequence from itertools import chain from pathlib import Path from typing import Any from arti.formats.json import JSON from arti.io import register_reader , register_writer from arti.storage.local import LocalFilePartition from arti.types import Type , is_partitioned from arti.views.python import PythonBuiltin # TODO: Do I need to inject the partition keys into the returned data? Likely useful... # Maybe a View option? def _read_json_file ( path : str ) -> Any : with open ( path ) as file : return json . load ( file ) @register_reader def _read_json_localfile_python ( type_ : Type , format : JSON , storage_partitions : Sequence [ LocalFilePartition ], view : PythonBuiltin , ) -> Any : if is_partitioned ( type_ ): return list ( chain . from_iterable ( _read_json_file ( storage_partition . path ) for storage_partition in storage_partitions ) ) assert len ( storage_partitions ) == 1 # Better error handled in base read return _read_json_file ( storage_partitions [ 0 ] . path ) @register_writer def _write_json_localfile_python ( data : Any , type_ : Type , format : JSON , storage_partition : LocalFilePartition , view : PythonBuiltin ) -> None : path = Path ( storage_partition . path ) path . parent . mkdir ( exist_ok = True , parents = True ) with path . open ( \"w\" ) as file : json . dump ( data , file )","title":"Json Localfile Python"},{"location":"reference/arti/io/json_localfile_python/#module-artiiojson_localfile_python","text":"None None View Source from __future__ import annotations import json from collections.abc import Sequence from itertools import chain from pathlib import Path from typing import Any from arti.formats.json import JSON from arti.io import register_reader , register_writer from arti.storage.local import LocalFilePartition from arti.types import Type , is_partitioned from arti.views.python import PythonBuiltin # TODO: Do I need to inject the partition keys into the returned data? Likely useful... # Maybe a View option? def _read_json_file ( path : str ) -> Any : with open ( path ) as file : return json . load ( file ) @register_reader def _read_json_localfile_python ( type_ : Type , format : JSON , storage_partitions : Sequence [ LocalFilePartition ], view : PythonBuiltin , ) -> Any : if is_partitioned ( type_ ): return list ( chain . from_iterable ( _read_json_file ( storage_partition . path ) for storage_partition in storage_partitions ) ) assert len ( storage_partitions ) == 1 # Better error handled in base read return _read_json_file ( storage_partitions [ 0 ] . path ) @register_writer def _write_json_localfile_python ( data : Any , type_ : Type , format : JSON , storage_partition : LocalFilePartition , view : PythonBuiltin ) -> None : path = Path ( storage_partition . path ) path . parent . mkdir ( exist_ok = True , parents = True ) with path . open ( \"w\" ) as file : json . dump ( data , file )","title":"Module arti.io.json_localfile_python"},{"location":"reference/arti/io/json_stringliteral_python/","text":"Module arti.io.json_stringliteral_python None None View Source from __future__ import annotations import json from collections.abc import Sequence from itertools import chain from typing import Any from arti.formats.json import JSON from arti.io import register_reader , register_writer from arti.storage.literal import StringLiteralPartition , _not_written_err from arti.types import Type , is_partitioned from arti.views.python import PythonBuiltin def _read_json_literal ( partition : StringLiteralPartition ) -> Any : if partition . value is None : raise _not_written_err return json . loads ( partition . value ) @register_reader def _read_json_stringliteral_python ( type_ : Type , format : JSON , storage_partitions : Sequence [ StringLiteralPartition ], view : PythonBuiltin , ) -> Any : if is_partitioned ( type_ ): return list ( chain . from_iterable ( _read_json_literal ( storage_partition ) for storage_partition in storage_partitions ) ) assert len ( storage_partitions ) == 1 # Better error handled in base read return _read_json_literal ( storage_partitions [ 0 ]) @register_writer def _write_json_stringliteral_python ( data : Any , type_ : Type , format : JSON , storage_partition : StringLiteralPartition , view : PythonBuiltin , ) -> StringLiteralPartition : if storage_partition . value is not None : # We can't overwrite the original value stored in LiteralStorage - on subsequent # `.discover_partitions`, a partition with the original value will still be used. raise ValueError ( \"Literals with a value already set cannot be written\" ) return storage_partition . copy ( update = { \"value\" : json . dumps ( data )})","title":"Json Stringliteral Python"},{"location":"reference/arti/io/json_stringliteral_python/#module-artiiojson_stringliteral_python","text":"None None View Source from __future__ import annotations import json from collections.abc import Sequence from itertools import chain from typing import Any from arti.formats.json import JSON from arti.io import register_reader , register_writer from arti.storage.literal import StringLiteralPartition , _not_written_err from arti.types import Type , is_partitioned from arti.views.python import PythonBuiltin def _read_json_literal ( partition : StringLiteralPartition ) -> Any : if partition . value is None : raise _not_written_err return json . loads ( partition . value ) @register_reader def _read_json_stringliteral_python ( type_ : Type , format : JSON , storage_partitions : Sequence [ StringLiteralPartition ], view : PythonBuiltin , ) -> Any : if is_partitioned ( type_ ): return list ( chain . from_iterable ( _read_json_literal ( storage_partition ) for storage_partition in storage_partitions ) ) assert len ( storage_partitions ) == 1 # Better error handled in base read return _read_json_literal ( storage_partitions [ 0 ]) @register_writer def _write_json_stringliteral_python ( data : Any , type_ : Type , format : JSON , storage_partition : StringLiteralPartition , view : PythonBuiltin , ) -> StringLiteralPartition : if storage_partition . value is not None : # We can't overwrite the original value stored in LiteralStorage - on subsequent # `.discover_partitions`, a partition with the original value will still be used. raise ValueError ( \"Literals with a value already set cannot be written\" ) return storage_partition . copy ( update = { \"value\" : json . dumps ( data )})","title":"Module arti.io.json_stringliteral_python"},{"location":"reference/arti/io/pickle_gcsfile_python/","text":"Module arti.io.pickle_gcsfile_python None None View Source from __future__ import annotations import pickle from collections.abc import Sequence from itertools import chain from typing import Any from gcsfs import GCSFileSystem from arti.formats.pickle import Pickle from arti.io import register_reader , register_writer from arti.storage.google.cloud.storage import GCSFilePartition from arti.types import Type , is_partitioned from arti.views.python import PythonBuiltin # TODO: Do I need to inject the partition keys into the returned data? Likely useful... # Maybe a View option? def _read_pickle_file ( path : str ) -> Any : # TODO: GCSFileSystem needs to be injected somehow with GCSFileSystem () . open ( path , \"rb\" ) as file : return pickle . load ( file ) # nosec # User opted into pickle, ignore bandit check @register_reader def _read_pickle_gcsfile_python ( type_ : Type , format : Pickle , storage_partitions : Sequence [ GCSFilePartition ], view : PythonBuiltin , ) -> Any : if is_partitioned ( type_ ): return list ( chain . from_iterable ( _read_pickle_file ( storage_partition . qualified_path ) for storage_partition in storage_partitions ) ) assert len ( storage_partitions ) == 1 # Better error handled in base read return _read_pickle_file ( storage_partitions [ 0 ] . qualified_path ) @register_writer def _write_pickle_gcsfile_python ( data : Any , type_ : Type , format : Pickle , storage_partition : GCSFilePartition , view : PythonBuiltin ) -> None : with GCSFileSystem () . open ( storage_partition . qualified_path , \"wb\" ) as file : pickle . dump ( data , file )","title":"Pickle Gcsfile Python"},{"location":"reference/arti/io/pickle_gcsfile_python/#module-artiiopickle_gcsfile_python","text":"None None View Source from __future__ import annotations import pickle from collections.abc import Sequence from itertools import chain from typing import Any from gcsfs import GCSFileSystem from arti.formats.pickle import Pickle from arti.io import register_reader , register_writer from arti.storage.google.cloud.storage import GCSFilePartition from arti.types import Type , is_partitioned from arti.views.python import PythonBuiltin # TODO: Do I need to inject the partition keys into the returned data? Likely useful... # Maybe a View option? def _read_pickle_file ( path : str ) -> Any : # TODO: GCSFileSystem needs to be injected somehow with GCSFileSystem () . open ( path , \"rb\" ) as file : return pickle . load ( file ) # nosec # User opted into pickle, ignore bandit check @register_reader def _read_pickle_gcsfile_python ( type_ : Type , format : Pickle , storage_partitions : Sequence [ GCSFilePartition ], view : PythonBuiltin , ) -> Any : if is_partitioned ( type_ ): return list ( chain . from_iterable ( _read_pickle_file ( storage_partition . qualified_path ) for storage_partition in storage_partitions ) ) assert len ( storage_partitions ) == 1 # Better error handled in base read return _read_pickle_file ( storage_partitions [ 0 ] . qualified_path ) @register_writer def _write_pickle_gcsfile_python ( data : Any , type_ : Type , format : Pickle , storage_partition : GCSFilePartition , view : PythonBuiltin ) -> None : with GCSFileSystem () . open ( storage_partition . qualified_path , \"wb\" ) as file : pickle . dump ( data , file )","title":"Module arti.io.pickle_gcsfile_python"},{"location":"reference/arti/io/pickle_localfile_python/","text":"Module arti.io.pickle_localfile_python None None View Source from __future__ import annotations import pickle from collections.abc import Sequence from itertools import chain from pathlib import Path from typing import Any from arti.formats.pickle import Pickle from arti.io import register_reader , register_writer from arti.storage.local import LocalFilePartition from arti.types import Type , is_partitioned from arti.views.python import PythonBuiltin def _read_pickle_file ( path : str ) -> Any : with open ( path , \"rb\" ) as file : return pickle . load ( file ) # nosec # User opted into pickle, ignore bandit check @register_reader def _read_pickle_localfile_python ( type_ : Type , format : Pickle , storage_partitions : Sequence [ LocalFilePartition ], view : PythonBuiltin , ) -> Any : if is_partitioned ( type_ ): return list ( chain . from_iterable ( _read_pickle_file ( storage_partition . path ) for storage_partition in storage_partitions ) ) assert len ( storage_partitions ) == 1 # Better error handled in base read return _read_pickle_file ( storage_partitions [ 0 ] . path ) @register_writer def _write_pickle_localfile_python ( data : Any , type_ : Type , format : Pickle , storage_partition : LocalFilePartition , view : PythonBuiltin , ) -> None : path = Path ( storage_partition . path ) path . parent . mkdir ( exist_ok = True , parents = True ) with path . open ( \"wb\" ) as file : pickle . dump ( data , file )","title":"Pickle Localfile Python"},{"location":"reference/arti/io/pickle_localfile_python/#module-artiiopickle_localfile_python","text":"None None View Source from __future__ import annotations import pickle from collections.abc import Sequence from itertools import chain from pathlib import Path from typing import Any from arti.formats.pickle import Pickle from arti.io import register_reader , register_writer from arti.storage.local import LocalFilePartition from arti.types import Type , is_partitioned from arti.views.python import PythonBuiltin def _read_pickle_file ( path : str ) -> Any : with open ( path , \"rb\" ) as file : return pickle . load ( file ) # nosec # User opted into pickle, ignore bandit check @register_reader def _read_pickle_localfile_python ( type_ : Type , format : Pickle , storage_partitions : Sequence [ LocalFilePartition ], view : PythonBuiltin , ) -> Any : if is_partitioned ( type_ ): return list ( chain . from_iterable ( _read_pickle_file ( storage_partition . path ) for storage_partition in storage_partitions ) ) assert len ( storage_partitions ) == 1 # Better error handled in base read return _read_pickle_file ( storage_partitions [ 0 ] . path ) @register_writer def _write_pickle_localfile_python ( data : Any , type_ : Type , format : Pickle , storage_partition : LocalFilePartition , view : PythonBuiltin , ) -> None : path = Path ( storage_partition . path ) path . parent . mkdir ( exist_ok = True , parents = True ) with path . open ( \"wb\" ) as file : pickle . dump ( data , file )","title":"Module arti.io.pickle_localfile_python"},{"location":"reference/arti/storage/","text":"Module arti.storage None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import abc import os from typing import TYPE_CHECKING , Any , ClassVar , Generic , Optional , TypeVar from pydantic import PrivateAttr from arti.fingerprints import Fingerprint from arti.formats import Format from arti.internal.models import Model from arti.internal.type_hints import Self , get_class_type_vars , lenient_issubclass from arti.internal.utils import frozendict from arti.partitions import CompositeKey , CompositeKeyTypes , InputFingerprints , PartitionKey from arti.storage._internal import partial_format , strip_partition_indexes from arti.types import Type if TYPE_CHECKING : from arti.graphs import Graph class StoragePartition ( Model ): keys : CompositeKey = CompositeKey () input_fingerprint : Fingerprint = Fingerprint . empty () content_fingerprint : Fingerprint = Fingerprint . empty () def with_content_fingerprint ( self , keep_existing : bool = True ) -> Self : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()}) @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" ) StoragePartitionVar = TypeVar ( \"StoragePartitionVar\" , bound = StoragePartition ) StoragePartitionVar_co = TypeVar ( \"StoragePartitionVar_co\" , bound = StoragePartition , covariant = True ) StoragePartitions = tuple [ StoragePartition , ... ] class Storage ( Model , Generic [ StoragePartitionVar_co ]): \"\"\"Storage is a data reference identifying 1 or more partitions of data. Storage fields should have defaults set with placeholders for tags and partition keys. This allows automatic injection of the tags and partition keys for simple cases. \"\"\" _abstract_ = True storage_partition_type : ClassVar [ type [ StoragePartitionVar_co ]] # type: ignore[misc] # These separators are used in the default resolve_* helpers to format metadata into # the storage fields. # # The defaults are tailored for \"path\"-like fields. key_value_sep : ClassVar [ str ] = \"=\" partition_name_component_sep : ClassVar [ str ] = \"_\" segment_sep : ClassVar [ str ] = os . sep _key_types : Optional [ CompositeKeyTypes ] = PrivateAttr ( None ) @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return cls . storage_partition_type = get_class_type_vars ( cls )[ 0 ] expected_field_types = { name : info . outer_type_ for name , info in cls . storage_partition_type . __fields__ . items () if name not in StoragePartition . __fields__ } fields = { name : info . outer_type_ for name , info in cls . __fields__ . items () if name not in Storage . __fields__ } if fields != expected_field_types : raise TypeError ( f \" { cls . __name__ } fields must match { cls . storage_partition_type . __name__ } ( { expected_field_types } ), got: { fields } \" ) @classmethod def get_default ( cls ) -> Storage [ StoragePartition ]: from arti.storage.literal import StringLiteral return StringLiteral () # TODO: Support some sort of configurable defaults. def _visit_type ( self , type_ : Type ) -> Self : # TODO: Check support for the types and partitioning on the specified field(s). copy = self . copy () copy . _key_types = PartitionKey . types_from ( type_ ) assert copy . key_types is not None key_component_specs = { f \" { name }{ self . partition_name_component_sep }{ component_name } \" : f \" {{ { name } . { component_spec } }} \" for name , pk in copy . key_types . items () for component_name , component_spec in pk . default_key_components . items () } return copy . resolve ( partition_key_spec = self . segment_sep . join ( f \" { name }{ self . key_value_sep }{ spec } \" for name , spec in key_component_specs . items () ) ) def _visit_format ( self , format_ : Format ) -> Self : return self . resolve ( extension = format_ . extension ) def _visit_graph ( self , graph : Graph ) -> Self : return self . resolve ( graph_name = graph . name , path_tags = self . segment_sep . join ( f \" { tag }{ self . key_value_sep }{ value } \" for tag , value in graph . path_tags . items () ), ) def _visit_input_fingerprint ( self , input_fingerprint : Fingerprint ) -> Self : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" return self . resolve ( input_fingerprint = input_fingerprint_key ) def _visit_names ( self , names : tuple [ str , ... ]) -> Self : return self . resolve ( name = names [ - 1 ] if names else \"\" , names = self . segment_sep . join ( names )) @property def includes_input_fingerprint_template ( self ) -> bool : return any ( \" {input_fingerprint} \" in val for val in self . _format_fields . values ()) @property def key_types ( self ) -> CompositeKeyTypes : if self . _key_types is None : raise ValueError ( \"`key_types` have not been set yet.\" ) return self . _key_types @property def _format_fields ( self ) -> frozendict [ str , str ]: return frozendict ( { name : value for name in self . __fields__ if lenient_issubclass ( type ( value := getattr ( self , name )), str ) } ) @classmethod def _check_keys ( cls , key_types : CompositeKeyTypes , keys : CompositeKey ) -> None : # TODO: Confirm the key names and types align if key_types and not keys : raise ValueError ( f \"Expected partition keys { tuple ( key_types ) } but none were passed\" ) if keys and not key_types : raise ValueError ( f \"Expected no partition keys but got: { keys } \" ) @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartitionVar_co , ... ]: raise NotImplementedError () def generate_partition ( self , keys : CompositeKey = CompositeKey (), input_fingerprint : Fingerprint = Fingerprint . empty (), with_content_fingerprint : bool = True , ) -> StoragePartitionVar_co : self . _check_keys ( self . key_types , keys ) format_kwargs = dict [ Any , Any ]( keys ) if input_fingerprint . is_empty : if self . includes_input_fingerprint_template : raise ValueError ( f \" { self } requires an input_fingerprint, but none was provided\" ) else : if not self . includes_input_fingerprint_template : raise ValueError ( f \" { self } does not specify a {{ input_fingerprint }} template\" ) format_kwargs [ \"input_fingerprint\" ] = str ( input_fingerprint . key ) field_values = { name : ( strip_partition_indexes ( original ) . format ( ** format_kwargs ) if lenient_issubclass ( type ( original := getattr ( self , name )), str ) else original ) for name in self . __fields__ if name in self . storage_partition_type . __fields__ } partition = self . storage_partition_type ( input_fingerprint = input_fingerprint , keys = keys , ** field_values ) if with_content_fingerprint : partition = partition . with_content_fingerprint () return partition def _resolve_field ( self , name : str , spec : str , placeholder_values : dict [ str , str ]) -> str : for placeholder , value in placeholder_values . items (): if not value : # Strip placeholder *and* any trailing self.segment_sep. trim = \"{\" + placeholder + \"}\" if f \" { trim }{ self . segment_sep } \" in spec : trim = f \" { trim }{ self . segment_sep } \" # Also strip any trailing separators, eg: if the placeholder was at the end. spec = spec . replace ( trim , \"\" ) . rstrip ( self . segment_sep ) if not spec : raise ValueError ( f \" { self } . { name } was empty after removing unused templates\" ) return partial_format ( spec , ** placeholder_values ) def resolve ( self , ** values : str ) -> Self : return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity (which # only shows \"set\" fields by default). if ( new := self . _resolve_field ( name , original , values )) != original } ) Sub-modules arti.storage.literal arti.storage.local Variables StoragePartitionVar StoragePartitionVar_co StoragePartitions TYPE_CHECKING Classes Storage class Storage ( __pydantic_self__ , ** data : Any ) View Source class Storage ( Model , Generic [ StoragePartitionVar_co ] ) : \"\"\"Storage is a data reference identifying 1 or more partitions of data. Storage fields should have defaults set with placeholders for tags and partition keys. This allows automatic injection of the tags and partition keys for simple cases. \"\"\" _abstract_ = True storage_partition_type : ClassVar [ type[StoragePartitionVar_co ] ] # type : ignore [ misc ] # These separators are used in the default resolve_ * helpers to format metadata into # the storage fields . # # The defaults are tailored for \"path\" - like fields . key_value_sep : ClassVar [ str ] = \"=\" partition_name_component_sep : ClassVar [ str ] = \"_\" segment_sep : ClassVar [ str ] = os . sep _key_types : Optional [ CompositeKeyTypes ] = PrivateAttr ( None ) @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return cls . storage_partition_type = get_class_type_vars ( cls ) [ 0 ] expected_field_types = { name : info . outer_type_ for name , info in cls . storage_partition_type . __fields__ . items () if name not in StoragePartition . __fields__ } fields = { name : info . outer_type_ for name , info in cls . __fields__ . items () if name not in Storage . __fields__ } if fields != expected_field_types : raise TypeError ( f \"{cls.__name__} fields must match {cls.storage_partition_type.__name__} ({expected_field_types}), got: {fields}\" ) @classmethod def get_default ( cls ) -> Storage [ StoragePartition ] : from arti . storage . literal import StringLiteral return StringLiteral () # TODO : Support some sort of configurable defaults . def _visit_type ( self , type_ : Type ) -> Self : # TODO : Check support for the types and partitioning on the specified field ( s ). copy = self . copy () copy . _key_types = PartitionKey . types_from ( type_ ) assert copy . key_types is not None key_component_specs = { f \"{name}{self.partition_name_component_sep}{component_name}\" : f \"{{{name}.{component_spec}}}\" for name , pk in copy . key_types . items () for component_name , component_spec in pk . default_key_components . items () } return copy . resolve ( partition_key_spec = self . segment_sep . join ( f \"{name}{self.key_value_sep}{spec}\" for name , spec in key_component_specs . items () ) ) def _visit_format ( self , format_ : Format ) -> Self : return self . resolve ( extension = format_ . extension ) def _visit_graph ( self , graph : Graph ) -> Self : return self . resolve ( graph_name = graph . name , path_tags = self . segment_sep . join ( f \"{tag}{self.key_value_sep}{value}\" for tag , value in graph . path_tags . items () ), ) def _visit_input_fingerprint ( self , input_fingerprint : Fingerprint ) -> Self : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" return self . resolve ( input_fingerprint = input_fingerprint_key ) def _visit_names ( self , names : tuple [ str, ... ] ) -> Self : return self . resolve ( name = names [ -1 ] if names else \"\" , names = self . segment_sep . join ( names )) @property def includes_input_fingerprint_template ( self ) -> bool : return any ( \"{input_fingerprint}\" in val for val in self . _format_fields . values ()) @property def key_types ( self ) -> CompositeKeyTypes : if self . _key_types is None : raise ValueError ( \"`key_types` have not been set yet.\" ) return self . _key_types @property def _format_fields ( self ) -> frozendict [ str, str ] : return frozendict ( { name : value for name in self . __fields__ if lenient_issubclass ( type ( value : = getattr ( self , name )), str ) } ) @classmethod def _check_keys ( cls , key_types : CompositeKeyTypes , keys : CompositeKey ) -> None : # TODO : Confirm the key names and types align if key_types and not keys : raise ValueError ( f \"Expected partition keys {tuple(key_types)} but none were passed\" ) if keys and not key_types : raise ValueError ( f \"Expected no partition keys but got: {keys}\" ) @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartitionVar_co, ... ] : raise NotImplementedError () def generate_partition ( self , keys : CompositeKey = CompositeKey (), input_fingerprint : Fingerprint = Fingerprint . empty (), with_content_fingerprint : bool = True , ) -> StoragePartitionVar_co : self . _check_keys ( self . key_types , keys ) format_kwargs = dict [ Any, Any ] ( keys ) if input_fingerprint . is_empty : if self . includes_input_fingerprint_template : raise ValueError ( f \"{self} requires an input_fingerprint, but none was provided\" ) else : if not self . includes_input_fingerprint_template : raise ValueError ( f \"{self} does not specify a {{input_fingerprint}} template\" ) format_kwargs [ \"input_fingerprint\" ] = str ( input_fingerprint . key ) field_values = { name : ( strip_partition_indexes ( original ). format ( ** format_kwargs ) if lenient_issubclass ( type ( original : = getattr ( self , name )), str ) else original ) for name in self . __fields__ if name in self . storage_partition_type . __fields__ } partition = self . storage_partition_type ( input_fingerprint = input_fingerprint , keys = keys , ** field_values ) if with_content_fingerprint : partition = partition . with_content_fingerprint () return partition def _resolve_field ( self , name : str , spec : str , placeholder_values : dict [ str, str ] ) -> str : for placeholder , value in placeholder_values . items () : if not value : # Strip placeholder * and * any trailing self . segment_sep . trim = \"{\" + placeholder + \"}\" if f \"{trim}{self.segment_sep}\" in spec : trim = f \"{trim}{self.segment_sep}\" # Also strip any trailing separators , eg : if the placeholder was at the end . spec = spec . replace ( trim , \"\" ). rstrip ( self . segment_sep ) if not spec : raise ValueError ( f \"{self}.{name} was empty after removing unused templates\" ) return partial_format ( spec , ** placeholder_values ) def resolve ( self , ** values : str ) -> Self : return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ). if ( new : = self . _resolve_field ( name , original , values )) != original } ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic Descendants arti.storage.google.cloud.storage.GCSFile arti.storage.local.LocalFile arti.storage.literal.StringLiteral Class variables Config key_value_sep partition_name_component_sep segment_sep Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_default def get_default ( ) -> 'Storage[StoragePartition]' View Source @classmethod def get_default ( cls ) -> Storage [ StoragePartition ]: from arti.storage.literal import StringLiteral return StringLiteral () # TODO: Support some sort of configurable defaults. parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint includes_input_fingerprint_template key_types Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. discover_partitions def discover_partitions ( self , input_fingerprints : 'InputFingerprints' = {} ) -> 'tuple[StoragePartitionVar_co, ...]' View Source @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartitionVar_co, ... ] : raise NotImplementedError () generate_partition def generate_partition ( self , keys : 'CompositeKey' = {}, input_fingerprint : 'Fingerprint' = Fingerprint ( key = None ), with_content_fingerprint : 'bool' = True ) -> 'StoragePartitionVar_co' View Source def generate_partition( self, keys: CompositeKey = CompositeKey(), input_fingerprint: Fingerprint = Fingerprint.empty(), with_content_fingerprint: bool = True, ) -> StoragePartitionVar_co: self._check_keys(self.key_types, keys) format_kwargs = dict[Any, Any](keys) if input_fingerprint.is_empty: if self.includes_input_fingerprint_template: raise ValueError(f\"{self} requires an input_fingerprint, but none was provided\") else: if not self.includes_input_fingerprint_template: raise ValueError(f\"{self} does not specify a {{ input_fingerprint }} template\") format_kwargs[\"input_fingerprint\"] = str(input_fingerprint.key) field_values = { name: ( strip_partition_indexes(original).format(**format_kwargs) if lenient_issubclass(type(original := getattr(self, name)), str) else original ) for name in self.__fields__ if name in self.storage_partition_type.__fields__ } partition = self.storage_partition_type( input_fingerprint=input_fingerprint, keys=keys, **field_values ) if with_content_fingerprint: partition = partition.with_content_fingerprint() return partition json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . resolve def resolve ( self , ** values : 'str' ) -> 'Self' View Source def resolve ( self , ** values : str ) -> Self : return self . copy ( update = { name : new for name , or iginal in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ) . if ( new := self . _resolve_field ( name , or iginal , values )) != or iginal } ) StoragePartition class StoragePartition ( __pydantic_self__ , ** data : Any ) View Source class StoragePartition ( Model ) : keys : CompositeKey = CompositeKey () input_fingerprint : Fingerprint = Fingerprint . empty () content_fingerprint : Fingerprint = Fingerprint . empty () def with_content_fingerprint ( self , keep_existing : bool = True ) -> Self : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint () } ) @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.storage.google.cloud.storage.GCSFilePartition arti.storage.local.LocalFilePartition arti.storage.literal.StringLiteralPartition Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods compute_content_fingerprint def compute_content_fingerprint ( self ) -> 'Fingerprint' View Source @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . with_content_fingerprint def with_content_fingerprint ( self , keep_existing : 'bool' = True ) -> 'Self' View Source def with_content_fingerprint ( self , keep_existing : bool = True ) -> Self : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()})","title":"Index"},{"location":"reference/arti/storage/#module-artistorage","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import abc import os from typing import TYPE_CHECKING , Any , ClassVar , Generic , Optional , TypeVar from pydantic import PrivateAttr from arti.fingerprints import Fingerprint from arti.formats import Format from arti.internal.models import Model from arti.internal.type_hints import Self , get_class_type_vars , lenient_issubclass from arti.internal.utils import frozendict from arti.partitions import CompositeKey , CompositeKeyTypes , InputFingerprints , PartitionKey from arti.storage._internal import partial_format , strip_partition_indexes from arti.types import Type if TYPE_CHECKING : from arti.graphs import Graph class StoragePartition ( Model ): keys : CompositeKey = CompositeKey () input_fingerprint : Fingerprint = Fingerprint . empty () content_fingerprint : Fingerprint = Fingerprint . empty () def with_content_fingerprint ( self , keep_existing : bool = True ) -> Self : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()}) @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" ) StoragePartitionVar = TypeVar ( \"StoragePartitionVar\" , bound = StoragePartition ) StoragePartitionVar_co = TypeVar ( \"StoragePartitionVar_co\" , bound = StoragePartition , covariant = True ) StoragePartitions = tuple [ StoragePartition , ... ] class Storage ( Model , Generic [ StoragePartitionVar_co ]): \"\"\"Storage is a data reference identifying 1 or more partitions of data. Storage fields should have defaults set with placeholders for tags and partition keys. This allows automatic injection of the tags and partition keys for simple cases. \"\"\" _abstract_ = True storage_partition_type : ClassVar [ type [ StoragePartitionVar_co ]] # type: ignore[misc] # These separators are used in the default resolve_* helpers to format metadata into # the storage fields. # # The defaults are tailored for \"path\"-like fields. key_value_sep : ClassVar [ str ] = \"=\" partition_name_component_sep : ClassVar [ str ] = \"_\" segment_sep : ClassVar [ str ] = os . sep _key_types : Optional [ CompositeKeyTypes ] = PrivateAttr ( None ) @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return cls . storage_partition_type = get_class_type_vars ( cls )[ 0 ] expected_field_types = { name : info . outer_type_ for name , info in cls . storage_partition_type . __fields__ . items () if name not in StoragePartition . __fields__ } fields = { name : info . outer_type_ for name , info in cls . __fields__ . items () if name not in Storage . __fields__ } if fields != expected_field_types : raise TypeError ( f \" { cls . __name__ } fields must match { cls . storage_partition_type . __name__ } ( { expected_field_types } ), got: { fields } \" ) @classmethod def get_default ( cls ) -> Storage [ StoragePartition ]: from arti.storage.literal import StringLiteral return StringLiteral () # TODO: Support some sort of configurable defaults. def _visit_type ( self , type_ : Type ) -> Self : # TODO: Check support for the types and partitioning on the specified field(s). copy = self . copy () copy . _key_types = PartitionKey . types_from ( type_ ) assert copy . key_types is not None key_component_specs = { f \" { name }{ self . partition_name_component_sep }{ component_name } \" : f \" {{ { name } . { component_spec } }} \" for name , pk in copy . key_types . items () for component_name , component_spec in pk . default_key_components . items () } return copy . resolve ( partition_key_spec = self . segment_sep . join ( f \" { name }{ self . key_value_sep }{ spec } \" for name , spec in key_component_specs . items () ) ) def _visit_format ( self , format_ : Format ) -> Self : return self . resolve ( extension = format_ . extension ) def _visit_graph ( self , graph : Graph ) -> Self : return self . resolve ( graph_name = graph . name , path_tags = self . segment_sep . join ( f \" { tag }{ self . key_value_sep }{ value } \" for tag , value in graph . path_tags . items () ), ) def _visit_input_fingerprint ( self , input_fingerprint : Fingerprint ) -> Self : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" return self . resolve ( input_fingerprint = input_fingerprint_key ) def _visit_names ( self , names : tuple [ str , ... ]) -> Self : return self . resolve ( name = names [ - 1 ] if names else \"\" , names = self . segment_sep . join ( names )) @property def includes_input_fingerprint_template ( self ) -> bool : return any ( \" {input_fingerprint} \" in val for val in self . _format_fields . values ()) @property def key_types ( self ) -> CompositeKeyTypes : if self . _key_types is None : raise ValueError ( \"`key_types` have not been set yet.\" ) return self . _key_types @property def _format_fields ( self ) -> frozendict [ str , str ]: return frozendict ( { name : value for name in self . __fields__ if lenient_issubclass ( type ( value := getattr ( self , name )), str ) } ) @classmethod def _check_keys ( cls , key_types : CompositeKeyTypes , keys : CompositeKey ) -> None : # TODO: Confirm the key names and types align if key_types and not keys : raise ValueError ( f \"Expected partition keys { tuple ( key_types ) } but none were passed\" ) if keys and not key_types : raise ValueError ( f \"Expected no partition keys but got: { keys } \" ) @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartitionVar_co , ... ]: raise NotImplementedError () def generate_partition ( self , keys : CompositeKey = CompositeKey (), input_fingerprint : Fingerprint = Fingerprint . empty (), with_content_fingerprint : bool = True , ) -> StoragePartitionVar_co : self . _check_keys ( self . key_types , keys ) format_kwargs = dict [ Any , Any ]( keys ) if input_fingerprint . is_empty : if self . includes_input_fingerprint_template : raise ValueError ( f \" { self } requires an input_fingerprint, but none was provided\" ) else : if not self . includes_input_fingerprint_template : raise ValueError ( f \" { self } does not specify a {{ input_fingerprint }} template\" ) format_kwargs [ \"input_fingerprint\" ] = str ( input_fingerprint . key ) field_values = { name : ( strip_partition_indexes ( original ) . format ( ** format_kwargs ) if lenient_issubclass ( type ( original := getattr ( self , name )), str ) else original ) for name in self . __fields__ if name in self . storage_partition_type . __fields__ } partition = self . storage_partition_type ( input_fingerprint = input_fingerprint , keys = keys , ** field_values ) if with_content_fingerprint : partition = partition . with_content_fingerprint () return partition def _resolve_field ( self , name : str , spec : str , placeholder_values : dict [ str , str ]) -> str : for placeholder , value in placeholder_values . items (): if not value : # Strip placeholder *and* any trailing self.segment_sep. trim = \"{\" + placeholder + \"}\" if f \" { trim }{ self . segment_sep } \" in spec : trim = f \" { trim }{ self . segment_sep } \" # Also strip any trailing separators, eg: if the placeholder was at the end. spec = spec . replace ( trim , \"\" ) . rstrip ( self . segment_sep ) if not spec : raise ValueError ( f \" { self } . { name } was empty after removing unused templates\" ) return partial_format ( spec , ** placeholder_values ) def resolve ( self , ** values : str ) -> Self : return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity (which # only shows \"set\" fields by default). if ( new := self . _resolve_field ( name , original , values )) != original } )","title":"Module arti.storage"},{"location":"reference/arti/storage/#sub-modules","text":"arti.storage.literal arti.storage.local","title":"Sub-modules"},{"location":"reference/arti/storage/#variables","text":"StoragePartitionVar StoragePartitionVar_co StoragePartitions TYPE_CHECKING","title":"Variables"},{"location":"reference/arti/storage/#classes","text":"","title":"Classes"},{"location":"reference/arti/storage/#storage","text":"class Storage ( __pydantic_self__ , ** data : Any ) View Source class Storage ( Model , Generic [ StoragePartitionVar_co ] ) : \"\"\"Storage is a data reference identifying 1 or more partitions of data. Storage fields should have defaults set with placeholders for tags and partition keys. This allows automatic injection of the tags and partition keys for simple cases. \"\"\" _abstract_ = True storage_partition_type : ClassVar [ type[StoragePartitionVar_co ] ] # type : ignore [ misc ] # These separators are used in the default resolve_ * helpers to format metadata into # the storage fields . # # The defaults are tailored for \"path\" - like fields . key_value_sep : ClassVar [ str ] = \"=\" partition_name_component_sep : ClassVar [ str ] = \"_\" segment_sep : ClassVar [ str ] = os . sep _key_types : Optional [ CompositeKeyTypes ] = PrivateAttr ( None ) @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return cls . storage_partition_type = get_class_type_vars ( cls ) [ 0 ] expected_field_types = { name : info . outer_type_ for name , info in cls . storage_partition_type . __fields__ . items () if name not in StoragePartition . __fields__ } fields = { name : info . outer_type_ for name , info in cls . __fields__ . items () if name not in Storage . __fields__ } if fields != expected_field_types : raise TypeError ( f \"{cls.__name__} fields must match {cls.storage_partition_type.__name__} ({expected_field_types}), got: {fields}\" ) @classmethod def get_default ( cls ) -> Storage [ StoragePartition ] : from arti . storage . literal import StringLiteral return StringLiteral () # TODO : Support some sort of configurable defaults . def _visit_type ( self , type_ : Type ) -> Self : # TODO : Check support for the types and partitioning on the specified field ( s ). copy = self . copy () copy . _key_types = PartitionKey . types_from ( type_ ) assert copy . key_types is not None key_component_specs = { f \"{name}{self.partition_name_component_sep}{component_name}\" : f \"{{{name}.{component_spec}}}\" for name , pk in copy . key_types . items () for component_name , component_spec in pk . default_key_components . items () } return copy . resolve ( partition_key_spec = self . segment_sep . join ( f \"{name}{self.key_value_sep}{spec}\" for name , spec in key_component_specs . items () ) ) def _visit_format ( self , format_ : Format ) -> Self : return self . resolve ( extension = format_ . extension ) def _visit_graph ( self , graph : Graph ) -> Self : return self . resolve ( graph_name = graph . name , path_tags = self . segment_sep . join ( f \"{tag}{self.key_value_sep}{value}\" for tag , value in graph . path_tags . items () ), ) def _visit_input_fingerprint ( self , input_fingerprint : Fingerprint ) -> Self : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" return self . resolve ( input_fingerprint = input_fingerprint_key ) def _visit_names ( self , names : tuple [ str, ... ] ) -> Self : return self . resolve ( name = names [ -1 ] if names else \"\" , names = self . segment_sep . join ( names )) @property def includes_input_fingerprint_template ( self ) -> bool : return any ( \"{input_fingerprint}\" in val for val in self . _format_fields . values ()) @property def key_types ( self ) -> CompositeKeyTypes : if self . _key_types is None : raise ValueError ( \"`key_types` have not been set yet.\" ) return self . _key_types @property def _format_fields ( self ) -> frozendict [ str, str ] : return frozendict ( { name : value for name in self . __fields__ if lenient_issubclass ( type ( value : = getattr ( self , name )), str ) } ) @classmethod def _check_keys ( cls , key_types : CompositeKeyTypes , keys : CompositeKey ) -> None : # TODO : Confirm the key names and types align if key_types and not keys : raise ValueError ( f \"Expected partition keys {tuple(key_types)} but none were passed\" ) if keys and not key_types : raise ValueError ( f \"Expected no partition keys but got: {keys}\" ) @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartitionVar_co, ... ] : raise NotImplementedError () def generate_partition ( self , keys : CompositeKey = CompositeKey (), input_fingerprint : Fingerprint = Fingerprint . empty (), with_content_fingerprint : bool = True , ) -> StoragePartitionVar_co : self . _check_keys ( self . key_types , keys ) format_kwargs = dict [ Any, Any ] ( keys ) if input_fingerprint . is_empty : if self . includes_input_fingerprint_template : raise ValueError ( f \"{self} requires an input_fingerprint, but none was provided\" ) else : if not self . includes_input_fingerprint_template : raise ValueError ( f \"{self} does not specify a {{input_fingerprint}} template\" ) format_kwargs [ \"input_fingerprint\" ] = str ( input_fingerprint . key ) field_values = { name : ( strip_partition_indexes ( original ). format ( ** format_kwargs ) if lenient_issubclass ( type ( original : = getattr ( self , name )), str ) else original ) for name in self . __fields__ if name in self . storage_partition_type . __fields__ } partition = self . storage_partition_type ( input_fingerprint = input_fingerprint , keys = keys , ** field_values ) if with_content_fingerprint : partition = partition . with_content_fingerprint () return partition def _resolve_field ( self , name : str , spec : str , placeholder_values : dict [ str, str ] ) -> str : for placeholder , value in placeholder_values . items () : if not value : # Strip placeholder * and * any trailing self . segment_sep . trim = \"{\" + placeholder + \"}\" if f \"{trim}{self.segment_sep}\" in spec : trim = f \"{trim}{self.segment_sep}\" # Also strip any trailing separators , eg : if the placeholder was at the end . spec = spec . replace ( trim , \"\" ). rstrip ( self . segment_sep ) if not spec : raise ValueError ( f \"{self}.{name} was empty after removing unused templates\" ) return partial_format ( spec , ** placeholder_values ) def resolve ( self , ** values : str ) -> Self : return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ). if ( new : = self . _resolve_field ( name , original , values )) != original } )","title":"Storage"},{"location":"reference/arti/storage/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/arti/storage/#descendants","text":"arti.storage.google.cloud.storage.GCSFile arti.storage.local.LocalFile arti.storage.literal.StringLiteral","title":"Descendants"},{"location":"reference/arti/storage/#class-variables","text":"Config key_value_sep partition_name_component_sep segment_sep","title":"Class variables"},{"location":"reference/arti/storage/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/storage/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/storage/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/storage/#get_default","text":"def get_default ( ) -> 'Storage[StoragePartition]' View Source @classmethod def get_default ( cls ) -> Storage [ StoragePartition ]: from arti.storage.literal import StringLiteral return StringLiteral () # TODO: Support some sort of configurable defaults.","title":"get_default"},{"location":"reference/arti/storage/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/storage/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/storage/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/storage/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/storage/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/storage/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/storage/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/storage/#instance-variables","text":"fingerprint includes_input_fingerprint_template key_types","title":"Instance variables"},{"location":"reference/arti/storage/#methods","text":"","title":"Methods"},{"location":"reference/arti/storage/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/storage/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/storage/#discover_partitions","text":"def discover_partitions ( self , input_fingerprints : 'InputFingerprints' = {} ) -> 'tuple[StoragePartitionVar_co, ...]' View Source @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartitionVar_co, ... ] : raise NotImplementedError ()","title":"discover_partitions"},{"location":"reference/arti/storage/#generate_partition","text":"def generate_partition ( self , keys : 'CompositeKey' = {}, input_fingerprint : 'Fingerprint' = Fingerprint ( key = None ), with_content_fingerprint : 'bool' = True ) -> 'StoragePartitionVar_co' View Source def generate_partition( self, keys: CompositeKey = CompositeKey(), input_fingerprint: Fingerprint = Fingerprint.empty(), with_content_fingerprint: bool = True, ) -> StoragePartitionVar_co: self._check_keys(self.key_types, keys) format_kwargs = dict[Any, Any](keys) if input_fingerprint.is_empty: if self.includes_input_fingerprint_template: raise ValueError(f\"{self} requires an input_fingerprint, but none was provided\") else: if not self.includes_input_fingerprint_template: raise ValueError(f\"{self} does not specify a {{ input_fingerprint }} template\") format_kwargs[\"input_fingerprint\"] = str(input_fingerprint.key) field_values = { name: ( strip_partition_indexes(original).format(**format_kwargs) if lenient_issubclass(type(original := getattr(self, name)), str) else original ) for name in self.__fields__ if name in self.storage_partition_type.__fields__ } partition = self.storage_partition_type( input_fingerprint=input_fingerprint, keys=keys, **field_values ) if with_content_fingerprint: partition = partition.with_content_fingerprint() return partition","title":"generate_partition"},{"location":"reference/arti/storage/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/storage/#resolve","text":"def resolve ( self , ** values : 'str' ) -> 'Self' View Source def resolve ( self , ** values : str ) -> Self : return self . copy ( update = { name : new for name , or iginal in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ) . if ( new := self . _resolve_field ( name , or iginal , values )) != or iginal } )","title":"resolve"},{"location":"reference/arti/storage/#storagepartition","text":"class StoragePartition ( __pydantic_self__ , ** data : Any ) View Source class StoragePartition ( Model ) : keys : CompositeKey = CompositeKey () input_fingerprint : Fingerprint = Fingerprint . empty () content_fingerprint : Fingerprint = Fingerprint . empty () def with_content_fingerprint ( self , keep_existing : bool = True ) -> Self : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint () } ) @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" )","title":"StoragePartition"},{"location":"reference/arti/storage/#ancestors-in-mro_1","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/storage/#descendants_1","text":"arti.storage.google.cloud.storage.GCSFilePartition arti.storage.local.LocalFilePartition arti.storage.literal.StringLiteralPartition","title":"Descendants"},{"location":"reference/arti/storage/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/arti/storage/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/storage/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/storage/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/storage/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/storage/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/storage/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/storage/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/storage/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/storage/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/storage/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/storage/#instance-variables_1","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/storage/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/storage/#compute_content_fingerprint","text":"def compute_content_fingerprint ( self ) -> 'Fingerprint' View Source @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" )","title":"compute_content_fingerprint"},{"location":"reference/arti/storage/#copy_1","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/storage/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/storage/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/storage/#with_content_fingerprint","text":"def with_content_fingerprint ( self , keep_existing : 'bool' = True ) -> 'Self' View Source def with_content_fingerprint ( self , keep_existing : bool = True ) -> Self : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()})","title":"with_content_fingerprint"},{"location":"reference/arti/storage/literal/","text":"Module arti.storage.literal None None View Source from __future__ import annotations from typing import Optional from arti.fingerprints import Fingerprint from arti.partitions import CompositeKey , InputFingerprints from arti.storage import Storage , StoragePartition _not_written_err = FileNotFoundError ( \"Literal has not been written yet\" ) class StringLiteralPartition ( StoragePartition ): id : str value : Optional [ str ] def compute_content_fingerprint ( self ) -> Fingerprint : if self . value is None : raise _not_written_err return Fingerprint . from_string ( self . value ) class StringLiteral ( Storage [ StringLiteralPartition ]): \"\"\"StringLiteral stores a literal String value directly in the Backend.\"\"\" id : str = \" {graph_name} / {path_tags} / {names} / {partition_key_spec} / {input_fingerprint} / {name}{extension} \" value : Optional [ str ] def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StringLiteralPartition , ... ]: if input_fingerprints and self . value is not None : raise ValueError ( f \"Literal storage cannot have a `value` preset ( { self . value } ) for a Producer output\" ) if self . key_types and not input_fingerprints : # We won't know what partitions to lookup. raise ValueError ( \"Literal storage can only be partitioned if generated by a Producer.\" ) # Existing StringLiteralPartitions may be stored in the Graph's backend, however we don't # have access here to lookup. if self . value is None : return () return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for keys , input_fingerprint in ( input_fingerprints or { CompositeKey (): Fingerprint . empty ()} ) . items () ) Classes StringLiteral class StringLiteral ( __pydantic_self__ , ** data : Any ) View Source class StringLiteral ( Storage [ StringLiteralPartition ] ) : \"\"\"StringLiteral stores a literal String value directly in the Backend.\"\"\" id : str = \"{graph_name}/{path_tags}/{names}/{partition_key_spec}/{input_fingerprint}/{name}{extension}\" value : Optional [ str ] def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StringLiteralPartition, ... ] : if input_fingerprints and self . value is not None : raise ValueError ( f \"Literal storage cannot have a `value` preset ({self.value}) for a Producer output\" ) if self . key_types and not input_fingerprints : # We won 't know what partitions to lookup. raise ValueError(\"Literal storage can only be partitioned if generated by a Producer.\") # Existing StringLiteralPartitions may be stored in the Graph' s backend , however we don ' t # have access here to lookup . if self . value is None : return () return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for keys , input_fingerprint in ( input_fingerprints or { CompositeKey () : Fingerprint . empty () } ). items () ) Ancestors (in MRO) arti.storage.Storage arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic Class variables Config key_value_sep partition_name_component_sep segment_sep storage_partition_type Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_default def get_default ( ) -> 'Storage[StoragePartition]' View Source @classmethod def get_default ( cls ) -> Storage [ StoragePartition ]: from arti.storage.literal import StringLiteral return StringLiteral () # TODO: Support some sort of configurable defaults. parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint includes_input_fingerprint_template key_types Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. discover_partitions def discover_partitions ( self , input_fingerprints : 'InputFingerprints' = {} ) -> 'tuple[StringLiteralPartition, ...]' View Source def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StringLiteralPartition , ... ] : if input_fingerprints and self . value is not None : raise ValueError ( f \"Literal storage cannot have a `value` preset ({self.value}) for a Producer output\" ) if self . key_types and not input_fingerprints : # We won't know what partitions to lookup. raise ValueError ( \"Literal storage can only be partitioned if generated by a Producer.\" ) # Existing StringLiteralPartitions may be stored in the Graph's backend, however we don't # have access here to lookup. if self . value is None : return () return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for keys , input_fingerprint in ( input_fingerprints or { CompositeKey () : Fingerprint . empty () } ). items () ) generate_partition def generate_partition ( self , keys : 'CompositeKey' = {}, input_fingerprint : 'Fingerprint' = Fingerprint ( key = None ), with_content_fingerprint : 'bool' = True ) -> 'StoragePartitionVar_co' View Source def generate_partition( self, keys: CompositeKey = CompositeKey(), input_fingerprint: Fingerprint = Fingerprint.empty(), with_content_fingerprint: bool = True, ) -> StoragePartitionVar_co: self._check_keys(self.key_types, keys) format_kwargs = dict[Any, Any](keys) if input_fingerprint.is_empty: if self.includes_input_fingerprint_template: raise ValueError(f\"{self} requires an input_fingerprint, but none was provided\") else: if not self.includes_input_fingerprint_template: raise ValueError(f\"{self} does not specify a {{ input_fingerprint }} template\") format_kwargs[\"input_fingerprint\"] = str(input_fingerprint.key) field_values = { name: ( strip_partition_indexes(original).format(**format_kwargs) if lenient_issubclass(type(original := getattr(self, name)), str) else original ) for name in self.__fields__ if name in self.storage_partition_type.__fields__ } partition = self.storage_partition_type( input_fingerprint=input_fingerprint, keys=keys, **field_values ) if with_content_fingerprint: partition = partition.with_content_fingerprint() return partition json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . resolve def resolve ( self , ** values : 'str' ) -> 'Self' View Source def resolve ( self , ** values : str ) -> Self : return self . copy ( update = { name : new for name , or iginal in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ) . if ( new := self . _resolve_field ( name , or iginal , values )) != or iginal } ) StringLiteralPartition class StringLiteralPartition ( __pydantic_self__ , ** data : Any ) View Source class StringLiteralPartition ( StoragePartition ) : id : str value : Optional [ str ] def compute_content_fingerprint ( self ) -> Fingerprint : if self . value is None : raise _not_written_err return Fingerprint . from_string ( self . value ) Ancestors (in MRO) arti.storage.StoragePartition arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods compute_content_fingerprint def compute_content_fingerprint ( self ) -> 'Fingerprint' View Source def compute_content_fingerprint ( self ) -> Fingerprint : if self . value is None : raise _not_written_err return Fingerprint . from_string ( self . value ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . with_content_fingerprint def with_content_fingerprint ( self , keep_existing : 'bool' = True ) -> 'Self' View Source def with_content_fingerprint ( self , keep_existing : bool = True ) -> Self : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()})","title":"Literal"},{"location":"reference/arti/storage/literal/#module-artistorageliteral","text":"None None View Source from __future__ import annotations from typing import Optional from arti.fingerprints import Fingerprint from arti.partitions import CompositeKey , InputFingerprints from arti.storage import Storage , StoragePartition _not_written_err = FileNotFoundError ( \"Literal has not been written yet\" ) class StringLiteralPartition ( StoragePartition ): id : str value : Optional [ str ] def compute_content_fingerprint ( self ) -> Fingerprint : if self . value is None : raise _not_written_err return Fingerprint . from_string ( self . value ) class StringLiteral ( Storage [ StringLiteralPartition ]): \"\"\"StringLiteral stores a literal String value directly in the Backend.\"\"\" id : str = \" {graph_name} / {path_tags} / {names} / {partition_key_spec} / {input_fingerprint} / {name}{extension} \" value : Optional [ str ] def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StringLiteralPartition , ... ]: if input_fingerprints and self . value is not None : raise ValueError ( f \"Literal storage cannot have a `value` preset ( { self . value } ) for a Producer output\" ) if self . key_types and not input_fingerprints : # We won't know what partitions to lookup. raise ValueError ( \"Literal storage can only be partitioned if generated by a Producer.\" ) # Existing StringLiteralPartitions may be stored in the Graph's backend, however we don't # have access here to lookup. if self . value is None : return () return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for keys , input_fingerprint in ( input_fingerprints or { CompositeKey (): Fingerprint . empty ()} ) . items () )","title":"Module arti.storage.literal"},{"location":"reference/arti/storage/literal/#classes","text":"","title":"Classes"},{"location":"reference/arti/storage/literal/#stringliteral","text":"class StringLiteral ( __pydantic_self__ , ** data : Any ) View Source class StringLiteral ( Storage [ StringLiteralPartition ] ) : \"\"\"StringLiteral stores a literal String value directly in the Backend.\"\"\" id : str = \"{graph_name}/{path_tags}/{names}/{partition_key_spec}/{input_fingerprint}/{name}{extension}\" value : Optional [ str ] def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StringLiteralPartition, ... ] : if input_fingerprints and self . value is not None : raise ValueError ( f \"Literal storage cannot have a `value` preset ({self.value}) for a Producer output\" ) if self . key_types and not input_fingerprints : # We won 't know what partitions to lookup. raise ValueError(\"Literal storage can only be partitioned if generated by a Producer.\") # Existing StringLiteralPartitions may be stored in the Graph' s backend , however we don ' t # have access here to lookup . if self . value is None : return () return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for keys , input_fingerprint in ( input_fingerprints or { CompositeKey () : Fingerprint . empty () } ). items () )","title":"StringLiteral"},{"location":"reference/arti/storage/literal/#ancestors-in-mro","text":"arti.storage.Storage arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/arti/storage/literal/#class-variables","text":"Config key_value_sep partition_name_component_sep segment_sep storage_partition_type","title":"Class variables"},{"location":"reference/arti/storage/literal/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/storage/literal/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/storage/literal/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/storage/literal/#get_default","text":"def get_default ( ) -> 'Storage[StoragePartition]' View Source @classmethod def get_default ( cls ) -> Storage [ StoragePartition ]: from arti.storage.literal import StringLiteral return StringLiteral () # TODO: Support some sort of configurable defaults.","title":"get_default"},{"location":"reference/arti/storage/literal/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/storage/literal/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/storage/literal/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/storage/literal/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/storage/literal/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/storage/literal/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/storage/literal/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/storage/literal/#instance-variables","text":"fingerprint includes_input_fingerprint_template key_types","title":"Instance variables"},{"location":"reference/arti/storage/literal/#methods","text":"","title":"Methods"},{"location":"reference/arti/storage/literal/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/storage/literal/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/storage/literal/#discover_partitions","text":"def discover_partitions ( self , input_fingerprints : 'InputFingerprints' = {} ) -> 'tuple[StringLiteralPartition, ...]' View Source def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StringLiteralPartition , ... ] : if input_fingerprints and self . value is not None : raise ValueError ( f \"Literal storage cannot have a `value` preset ({self.value}) for a Producer output\" ) if self . key_types and not input_fingerprints : # We won't know what partitions to lookup. raise ValueError ( \"Literal storage can only be partitioned if generated by a Producer.\" ) # Existing StringLiteralPartitions may be stored in the Graph's backend, however we don't # have access here to lookup. if self . value is None : return () return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for keys , input_fingerprint in ( input_fingerprints or { CompositeKey () : Fingerprint . empty () } ). items () )","title":"discover_partitions"},{"location":"reference/arti/storage/literal/#generate_partition","text":"def generate_partition ( self , keys : 'CompositeKey' = {}, input_fingerprint : 'Fingerprint' = Fingerprint ( key = None ), with_content_fingerprint : 'bool' = True ) -> 'StoragePartitionVar_co' View Source def generate_partition( self, keys: CompositeKey = CompositeKey(), input_fingerprint: Fingerprint = Fingerprint.empty(), with_content_fingerprint: bool = True, ) -> StoragePartitionVar_co: self._check_keys(self.key_types, keys) format_kwargs = dict[Any, Any](keys) if input_fingerprint.is_empty: if self.includes_input_fingerprint_template: raise ValueError(f\"{self} requires an input_fingerprint, but none was provided\") else: if not self.includes_input_fingerprint_template: raise ValueError(f\"{self} does not specify a {{ input_fingerprint }} template\") format_kwargs[\"input_fingerprint\"] = str(input_fingerprint.key) field_values = { name: ( strip_partition_indexes(original).format(**format_kwargs) if lenient_issubclass(type(original := getattr(self, name)), str) else original ) for name in self.__fields__ if name in self.storage_partition_type.__fields__ } partition = self.storage_partition_type( input_fingerprint=input_fingerprint, keys=keys, **field_values ) if with_content_fingerprint: partition = partition.with_content_fingerprint() return partition","title":"generate_partition"},{"location":"reference/arti/storage/literal/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/storage/literal/#resolve","text":"def resolve ( self , ** values : 'str' ) -> 'Self' View Source def resolve ( self , ** values : str ) -> Self : return self . copy ( update = { name : new for name , or iginal in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ) . if ( new := self . _resolve_field ( name , or iginal , values )) != or iginal } )","title":"resolve"},{"location":"reference/arti/storage/literal/#stringliteralpartition","text":"class StringLiteralPartition ( __pydantic_self__ , ** data : Any ) View Source class StringLiteralPartition ( StoragePartition ) : id : str value : Optional [ str ] def compute_content_fingerprint ( self ) -> Fingerprint : if self . value is None : raise _not_written_err return Fingerprint . from_string ( self . value )","title":"StringLiteralPartition"},{"location":"reference/arti/storage/literal/#ancestors-in-mro_1","text":"arti.storage.StoragePartition arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/storage/literal/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/arti/storage/literal/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/storage/literal/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/storage/literal/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/storage/literal/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/storage/literal/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/storage/literal/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/storage/literal/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/storage/literal/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/storage/literal/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/storage/literal/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/storage/literal/#instance-variables_1","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/storage/literal/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/storage/literal/#compute_content_fingerprint","text":"def compute_content_fingerprint ( self ) -> 'Fingerprint' View Source def compute_content_fingerprint ( self ) -> Fingerprint : if self . value is None : raise _not_written_err return Fingerprint . from_string ( self . value )","title":"compute_content_fingerprint"},{"location":"reference/arti/storage/literal/#copy_1","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/storage/literal/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/storage/literal/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/storage/literal/#with_content_fingerprint","text":"def with_content_fingerprint ( self , keep_existing : 'bool' = True ) -> 'Self' View Source def with_content_fingerprint ( self , keep_existing : bool = True ) -> Self : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()})","title":"with_content_fingerprint"},{"location":"reference/arti/storage/local/","text":"Module arti.storage.local None None View Source from __future__ import annotations import hashlib import tempfile from glob import glob from pathlib import Path from typing import Optional , Union from arti.fingerprints import Fingerprint from arti.partitions import InputFingerprints from arti.storage import Storage , StoragePartition from arti.storage._internal import parse_spec , spec_to_wildcard class LocalFilePartition ( StoragePartition ): path : str def compute_content_fingerprint ( self , buffer_size : int = 1024 * 1024 ) -> Fingerprint : with open ( self . path , mode = \"rb\" ) as f : sha = hashlib . sha256 () while data := f . read ( buffer_size ): sha . update ( data ) return Fingerprint . from_string ( sha . hexdigest ()) class LocalFile ( Storage [ LocalFilePartition ]): # `_DEFAULT_PATH_TEMPLATE` and `rooted_at` ease testing, where we often want to just override # the tempdir, but keep the rest of the template. Eventually, we should introduce Resources and # implement a MockFS (to be used in `io.*`). _DEFAULT_PATH_TEMPLATE = str ( Path ( \" {graph_name} \" ) / \" {path_tags} \" / \" {names} \" / \" {partition_key_spec} \" / \" {input_fingerprint} \" / \" {name}{extension} \" ) path : str = str ( Path ( tempfile . gettempdir ()) / _DEFAULT_PATH_TEMPLATE ) def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ LocalFilePartition , ... ]: wildcard = spec_to_wildcard ( self . path , self . key_types ) paths = set ( glob ( wildcard )) path_metadata = parse_spec ( paths , spec = self . path , key_types = self . key_types , input_fingerprints = input_fingerprints ) return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for path , ( input_fingerprint , keys ) in path_metadata . items () ) @classmethod def rooted_at ( cls , root : Union [ str , Path ], path : Optional [ str ] = None ) -> LocalFile : path = path if path is not None else cls . _DEFAULT_PATH_TEMPLATE return cls ( path = str ( Path ( root ) / path )) Classes LocalFile class LocalFile ( __pydantic_self__ , ** data : Any ) View Source class LocalFile ( Storage [ LocalFilePartition ] ) : # ` _DEFAULT_PATH_TEMPLATE ` and ` rooted_at ` ease testing , where we often want to just override # the tempdir , but keep the rest of the template . Eventually , we should introduce Resources and # implement a MockFS ( to be used in ` io . * ` ). _DEFAULT_PATH_TEMPLATE = str ( Path ( \"{graph_name}\" ) / \"{path_tags}\" / \"{names}\" / \"{partition_key_spec}\" / \"{input_fingerprint}\" / \"{name}{extension}\" ) path : str = str ( Path ( tempfile . gettempdir ()) / _DEFAULT_PATH_TEMPLATE ) def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ LocalFilePartition, ... ] : wildcard = spec_to_wildcard ( self . path , self . key_types ) paths = set ( glob ( wildcard )) path_metadata = parse_spec ( paths , spec = self . path , key_types = self . key_types , input_fingerprints = input_fingerprints ) return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for path , ( input_fingerprint , keys ) in path_metadata . items () ) @classmethod def rooted_at ( cls , root : Union [ str, Path ] , path : Optional [ str ] = None ) -> LocalFile : path = path if path is not None else cls . _DEFAULT_PATH_TEMPLATE return cls ( path = str ( Path ( root ) / path )) Ancestors (in MRO) arti.storage.Storage arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic Class variables Config key_value_sep partition_name_component_sep segment_sep storage_partition_type Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_default def get_default ( ) -> 'Storage[StoragePartition]' View Source @classmethod def get_default ( cls ) -> Storage [ StoragePartition ]: from arti.storage.literal import StringLiteral return StringLiteral () # TODO: Support some sort of configurable defaults. parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' rooted_at def rooted_at ( root : 'Union[str, Path]' , path : 'Optional[str]' = None ) -> 'LocalFile' View Source @classmethod def rooted_at ( cls , root : Union [ str, Path ] , path : Optional [ str ] = None ) -> LocalFile : path = path if path is not None else cls . _DEFAULT_PATH_TEMPLATE return cls ( path = str ( Path ( root ) / path )) schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint includes_input_fingerprint_template key_types Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. discover_partitions def discover_partitions ( self , input_fingerprints : 'InputFingerprints' = {} ) -> 'tuple[LocalFilePartition, ...]' View Source def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ LocalFilePartition , ...] : wildcard = spec_to_wildcard ( self . path , self . key_types ) paths = set ( glob ( wildcard )) path_metadata = parse_spec ( paths , spec = self . path , key_types = self . key_types , input_fingerprints = input_fingerprints ) return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for path , ( input_fingerprint , keys ) in path_metadata . items () ) generate_partition def generate_partition ( self , keys : 'CompositeKey' = {}, input_fingerprint : 'Fingerprint' = Fingerprint ( key = None ), with_content_fingerprint : 'bool' = True ) -> 'StoragePartitionVar_co' View Source def generate_partition( self, keys: CompositeKey = CompositeKey(), input_fingerprint: Fingerprint = Fingerprint.empty(), with_content_fingerprint: bool = True, ) -> StoragePartitionVar_co: self._check_keys(self.key_types, keys) format_kwargs = dict[Any, Any](keys) if input_fingerprint.is_empty: if self.includes_input_fingerprint_template: raise ValueError(f\"{self} requires an input_fingerprint, but none was provided\") else: if not self.includes_input_fingerprint_template: raise ValueError(f\"{self} does not specify a {{ input_fingerprint }} template\") format_kwargs[\"input_fingerprint\"] = str(input_fingerprint.key) field_values = { name: ( strip_partition_indexes(original).format(**format_kwargs) if lenient_issubclass(type(original := getattr(self, name)), str) else original ) for name in self.__fields__ if name in self.storage_partition_type.__fields__ } partition = self.storage_partition_type( input_fingerprint=input_fingerprint, keys=keys, **field_values ) if with_content_fingerprint: partition = partition.with_content_fingerprint() return partition json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . resolve def resolve ( self , ** values : 'str' ) -> 'Self' View Source def resolve ( self , ** values : str ) -> Self : return self . copy ( update = { name : new for name , or iginal in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ) . if ( new := self . _resolve_field ( name , or iginal , values )) != or iginal } ) LocalFilePartition class LocalFilePartition ( __pydantic_self__ , ** data : Any ) View Source class LocalFilePartition ( StoragePartition ): path: str def compute_content_fingerprint ( self , buffer_size: int = 1024 * 1024 ) -> Fingerprint: with open ( self . path , mode = \"rb\" ) as f: sha = hashlib . sha256 () while data := f . read ( buffer_size ): sha . update ( data ) return Fingerprint . from_string ( sha . hexdigest ()) Ancestors (in MRO) arti.storage.StoragePartition arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods compute_content_fingerprint def compute_content_fingerprint ( self , buffer_size : 'int' = 1048576 ) -> 'Fingerprint' View Source def compute_content_fingerprint ( self , buffer_size : int = 1024 * 1024 ) -> Fingerprint : with open ( self . path , mode = \"rb\" ) as f : sha = hashlib . sha256 () while data := f . read ( buffer_size ) : sha . update ( data ) return Fingerprint . from _string ( sha . hexdigest ()) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . with_content_fingerprint def with_content_fingerprint ( self , keep_existing : 'bool' = True ) -> 'Self' View Source def with_content_fingerprint ( self , keep_existing : bool = True ) -> Self : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()})","title":"Local"},{"location":"reference/arti/storage/local/#module-artistoragelocal","text":"None None View Source from __future__ import annotations import hashlib import tempfile from glob import glob from pathlib import Path from typing import Optional , Union from arti.fingerprints import Fingerprint from arti.partitions import InputFingerprints from arti.storage import Storage , StoragePartition from arti.storage._internal import parse_spec , spec_to_wildcard class LocalFilePartition ( StoragePartition ): path : str def compute_content_fingerprint ( self , buffer_size : int = 1024 * 1024 ) -> Fingerprint : with open ( self . path , mode = \"rb\" ) as f : sha = hashlib . sha256 () while data := f . read ( buffer_size ): sha . update ( data ) return Fingerprint . from_string ( sha . hexdigest ()) class LocalFile ( Storage [ LocalFilePartition ]): # `_DEFAULT_PATH_TEMPLATE` and `rooted_at` ease testing, where we often want to just override # the tempdir, but keep the rest of the template. Eventually, we should introduce Resources and # implement a MockFS (to be used in `io.*`). _DEFAULT_PATH_TEMPLATE = str ( Path ( \" {graph_name} \" ) / \" {path_tags} \" / \" {names} \" / \" {partition_key_spec} \" / \" {input_fingerprint} \" / \" {name}{extension} \" ) path : str = str ( Path ( tempfile . gettempdir ()) / _DEFAULT_PATH_TEMPLATE ) def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ LocalFilePartition , ... ]: wildcard = spec_to_wildcard ( self . path , self . key_types ) paths = set ( glob ( wildcard )) path_metadata = parse_spec ( paths , spec = self . path , key_types = self . key_types , input_fingerprints = input_fingerprints ) return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for path , ( input_fingerprint , keys ) in path_metadata . items () ) @classmethod def rooted_at ( cls , root : Union [ str , Path ], path : Optional [ str ] = None ) -> LocalFile : path = path if path is not None else cls . _DEFAULT_PATH_TEMPLATE return cls ( path = str ( Path ( root ) / path ))","title":"Module arti.storage.local"},{"location":"reference/arti/storage/local/#classes","text":"","title":"Classes"},{"location":"reference/arti/storage/local/#localfile","text":"class LocalFile ( __pydantic_self__ , ** data : Any ) View Source class LocalFile ( Storage [ LocalFilePartition ] ) : # ` _DEFAULT_PATH_TEMPLATE ` and ` rooted_at ` ease testing , where we often want to just override # the tempdir , but keep the rest of the template . Eventually , we should introduce Resources and # implement a MockFS ( to be used in ` io . * ` ). _DEFAULT_PATH_TEMPLATE = str ( Path ( \"{graph_name}\" ) / \"{path_tags}\" / \"{names}\" / \"{partition_key_spec}\" / \"{input_fingerprint}\" / \"{name}{extension}\" ) path : str = str ( Path ( tempfile . gettempdir ()) / _DEFAULT_PATH_TEMPLATE ) def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ LocalFilePartition, ... ] : wildcard = spec_to_wildcard ( self . path , self . key_types ) paths = set ( glob ( wildcard )) path_metadata = parse_spec ( paths , spec = self . path , key_types = self . key_types , input_fingerprints = input_fingerprints ) return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for path , ( input_fingerprint , keys ) in path_metadata . items () ) @classmethod def rooted_at ( cls , root : Union [ str, Path ] , path : Optional [ str ] = None ) -> LocalFile : path = path if path is not None else cls . _DEFAULT_PATH_TEMPLATE return cls ( path = str ( Path ( root ) / path ))","title":"LocalFile"},{"location":"reference/arti/storage/local/#ancestors-in-mro","text":"arti.storage.Storage arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/arti/storage/local/#class-variables","text":"Config key_value_sep partition_name_component_sep segment_sep storage_partition_type","title":"Class variables"},{"location":"reference/arti/storage/local/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/storage/local/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/storage/local/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/storage/local/#get_default","text":"def get_default ( ) -> 'Storage[StoragePartition]' View Source @classmethod def get_default ( cls ) -> Storage [ StoragePartition ]: from arti.storage.literal import StringLiteral return StringLiteral () # TODO: Support some sort of configurable defaults.","title":"get_default"},{"location":"reference/arti/storage/local/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/storage/local/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/storage/local/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/storage/local/#rooted_at","text":"def rooted_at ( root : 'Union[str, Path]' , path : 'Optional[str]' = None ) -> 'LocalFile' View Source @classmethod def rooted_at ( cls , root : Union [ str, Path ] , path : Optional [ str ] = None ) -> LocalFile : path = path if path is not None else cls . _DEFAULT_PATH_TEMPLATE return cls ( path = str ( Path ( root ) / path ))","title":"rooted_at"},{"location":"reference/arti/storage/local/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/storage/local/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/storage/local/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/storage/local/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/storage/local/#instance-variables","text":"fingerprint includes_input_fingerprint_template key_types","title":"Instance variables"},{"location":"reference/arti/storage/local/#methods","text":"","title":"Methods"},{"location":"reference/arti/storage/local/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/storage/local/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/storage/local/#discover_partitions","text":"def discover_partitions ( self , input_fingerprints : 'InputFingerprints' = {} ) -> 'tuple[LocalFilePartition, ...]' View Source def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ LocalFilePartition , ...] : wildcard = spec_to_wildcard ( self . path , self . key_types ) paths = set ( glob ( wildcard )) path_metadata = parse_spec ( paths , spec = self . path , key_types = self . key_types , input_fingerprints = input_fingerprints ) return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for path , ( input_fingerprint , keys ) in path_metadata . items () )","title":"discover_partitions"},{"location":"reference/arti/storage/local/#generate_partition","text":"def generate_partition ( self , keys : 'CompositeKey' = {}, input_fingerprint : 'Fingerprint' = Fingerprint ( key = None ), with_content_fingerprint : 'bool' = True ) -> 'StoragePartitionVar_co' View Source def generate_partition( self, keys: CompositeKey = CompositeKey(), input_fingerprint: Fingerprint = Fingerprint.empty(), with_content_fingerprint: bool = True, ) -> StoragePartitionVar_co: self._check_keys(self.key_types, keys) format_kwargs = dict[Any, Any](keys) if input_fingerprint.is_empty: if self.includes_input_fingerprint_template: raise ValueError(f\"{self} requires an input_fingerprint, but none was provided\") else: if not self.includes_input_fingerprint_template: raise ValueError(f\"{self} does not specify a {{ input_fingerprint }} template\") format_kwargs[\"input_fingerprint\"] = str(input_fingerprint.key) field_values = { name: ( strip_partition_indexes(original).format(**format_kwargs) if lenient_issubclass(type(original := getattr(self, name)), str) else original ) for name in self.__fields__ if name in self.storage_partition_type.__fields__ } partition = self.storage_partition_type( input_fingerprint=input_fingerprint, keys=keys, **field_values ) if with_content_fingerprint: partition = partition.with_content_fingerprint() return partition","title":"generate_partition"},{"location":"reference/arti/storage/local/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/storage/local/#resolve","text":"def resolve ( self , ** values : 'str' ) -> 'Self' View Source def resolve ( self , ** values : str ) -> Self : return self . copy ( update = { name : new for name , or iginal in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ) . if ( new := self . _resolve_field ( name , or iginal , values )) != or iginal } )","title":"resolve"},{"location":"reference/arti/storage/local/#localfilepartition","text":"class LocalFilePartition ( __pydantic_self__ , ** data : Any ) View Source class LocalFilePartition ( StoragePartition ): path: str def compute_content_fingerprint ( self , buffer_size: int = 1024 * 1024 ) -> Fingerprint: with open ( self . path , mode = \"rb\" ) as f: sha = hashlib . sha256 () while data := f . read ( buffer_size ): sha . update ( data ) return Fingerprint . from_string ( sha . hexdigest ())","title":"LocalFilePartition"},{"location":"reference/arti/storage/local/#ancestors-in-mro_1","text":"arti.storage.StoragePartition arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/storage/local/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/arti/storage/local/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/storage/local/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/storage/local/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/storage/local/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/storage/local/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/storage/local/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/storage/local/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/storage/local/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/storage/local/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/storage/local/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/storage/local/#instance-variables_1","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/storage/local/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/storage/local/#compute_content_fingerprint","text":"def compute_content_fingerprint ( self , buffer_size : 'int' = 1048576 ) -> 'Fingerprint' View Source def compute_content_fingerprint ( self , buffer_size : int = 1024 * 1024 ) -> Fingerprint : with open ( self . path , mode = \"rb\" ) as f : sha = hashlib . sha256 () while data := f . read ( buffer_size ) : sha . update ( data ) return Fingerprint . from _string ( sha . hexdigest ())","title":"compute_content_fingerprint"},{"location":"reference/arti/storage/local/#copy_1","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/storage/local/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/storage/local/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/storage/local/#with_content_fingerprint","text":"def with_content_fingerprint ( self , keep_existing : 'bool' = True ) -> 'Self' View Source def with_content_fingerprint ( self , keep_existing : bool = True ) -> Self : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()})","title":"with_content_fingerprint"},{"location":"reference/arti/types/","text":"Module arti.types None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from abc import abstractmethod from collections.abc import Iterable , Mapping from operator import attrgetter from typing import Any , ClassVar , Literal , Optional from pydantic import PrivateAttr , validator from pydantic import __version__ as pydantic_version from arti.internal.models import Model from arti.internal.type_hints import lenient_issubclass from arti.internal.utils import NoCopyDict , class_name , frozendict , register DEFAULT_ANONYMOUS_NAME = \"anon\" class Type ( Model ): \"\"\"Type represents a data type.\"\"\" _abstract_ = True # NOTE: Exclude the description to minimize fingerprint changes (and thus rebuilds). _fingerprint_excludes_ = frozenset ([ \"description\" ]) description : Optional [ str ] nullable : bool = False @property def friendly_key ( self ) -> str : \"\"\"A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. \"\"\" return self . _class_key_ def is_partitioned ( type_ : Type ) -> bool : \"\"\"Helper function to determine whether the type is partitioned.\"\"\" return isinstance ( type_ , Collection ) and bool ( type_ . partition_fields ) class _ContainerMixin ( Model ): element : Type class _NamedMixin ( Model ): name : str = DEFAULT_ANONYMOUS_NAME @classmethod def _pydantic_type_system_post_field_conversion_hook_ ( cls , type_ : Type , * , name : str , required : bool ) -> Type : type_ = super () . _pydantic_type_system_post_field_conversion_hook_ ( type_ , name = name , required = required ) if \"name\" not in type_ . __fields_set__ : type_ = type_ . copy ( update = { \"name\" : name }) return type_ @property @abstractmethod def _default_friendly_key ( self ) -> str : raise NotImplementedError () @property def friendly_key ( self ) -> str : return self . _default_friendly_key if self . name == DEFAULT_ANONYMOUS_NAME else self . name class _TimeMixin ( Model ): precision : Literal [ \"second\" , \"millisecond\" , \"microsecond\" , \"nanosecond\" ] ######################## # Core Artigraph Types # ######################## class _Numeric ( Type ): pass class _Float ( _Numeric ): pass class _Int ( _Numeric ): pass class Binary ( Type ): byte_size : Optional [ int ] class Boolean ( Type ): pass class Date ( Type ): pass class DateTime ( _TimeMixin , Type ): \"\"\"A Date and Time as shown on a calendar and clock, independent of timezone.\"\"\" class Enum ( _NamedMixin , Type ): type : Type items : frozenset [ Any ] @validator ( \"items\" , pre = True ) @classmethod def _cast_values ( cls , items : Any ) -> Any : if isinstance ( items , Iterable ) and not isinstance ( items , Mapping ): return frozenset ( items ) return items @validator ( \"items\" ) @classmethod def _validate_values ( cls , items : frozenset [ Any ], values : dict [ str , Any ]) -> frozenset [ Any ]: from arti.types.python import python_type_system if len ( items ) == 0 : raise ValueError ( \"cannot be empty.\" ) # `type` will be missing if it doesn't pass validation. if ( arti_type := values . get ( \"type\" )) is None : return items py_type = python_type_system . to_system ( arti_type , hints = {}) mismatched_items = [ item for item in items if not lenient_issubclass ( type ( item ), py_type )] if mismatched_items : raise ValueError ( f \"incompatible { arti_type } ( { py_type } ) item(s): { mismatched_items } \" ) return items @property def _default_friendly_key ( self ) -> str : return f \" { self . type . friendly_key }{ self . _class_key_ } \" class Float16 ( _Float ): pass class Float32 ( _Float ): pass class Float64 ( _Float ): pass class Geography ( Type ): format : Optional [ str ] # \"WKB\", \"WKT\", etc srid : Optional [ str ] class Int8 ( _Int ): pass class Int16 ( _Int ): pass class Int32 ( _Int ): pass class Int64 ( _Int ): pass class List ( _ContainerMixin , Type ): @property def friendly_key ( self ) -> str : return f \" { self . element . friendly_key }{ self . _class_key_ } \" class Collection ( _NamedMixin , List ): \"\"\"A collection of elements with partition and cluster metadata. Collections should not be nested in other types. \"\"\" partition_by : tuple [ str , ... ] = () cluster_by : tuple [ str , ... ] = () @validator ( \"partition_by\" , \"cluster_by\" ) @classmethod def _validate_field_ref ( cls , references : tuple [ str , ... ], values : dict [ str , Any ] ) -> tuple [ str , ... ]: if ( element := values . get ( \"element\" )) is None : return references if references and not isinstance ( element , Struct ): raise ValueError ( \"requires element to be a Struct\" ) known , requested = set ( element . fields ), set ( references ) if unknown := requested - known : raise ValueError ( f \"unknown field(s): { unknown } \" ) return references @validator ( \"cluster_by\" ) @classmethod def _validate_cluster_by ( cls , cluster_by : tuple [ str , ... ], values : dict [ str , Any ] ) -> tuple [ str , ... ]: if ( partition_by := values . get ( \"partition_by\" )) is None : return cluster_by if overlapping := set ( cluster_by ) & set ( partition_by ): raise ValueError ( f \"clustering fields overlap with partition fields: { overlapping } \" ) return cluster_by @property def _default_friendly_key ( self ) -> str : return f \" { self . element . friendly_key }{ self . _class_key_ } \" @property def fields ( self ) -> frozendict [ str , Type ]: \"\"\"Shorthand accessor to access Struct element fields. If the element is not a Struct, an AttributeError will be raised. \"\"\" return self . element . fields # type: ignore[attr-defined,no-any-return] # We want the standard AttributeError @property def partition_fields ( self ) -> frozendict [ str , Type ]: if not isinstance ( self . element , Struct ): return frozendict () return frozendict ({ name : self . element . fields [ name ] for name in self . partition_by }) class Map ( Type ): key : Type value : Type @property def friendly_key ( self ) -> str : return f \" { self . key . friendly_key } To { self . value . friendly_key } \" class Null ( Type ): pass class Set ( _ContainerMixin , Type ): @property def friendly_key ( self ) -> str : return f \" { self . element . friendly_key }{ self . _class_key_ } \" class String ( Type ): pass class Struct ( _NamedMixin , Type ): fields : frozendict [ str , Type ] @property def _default_friendly_key ( self ) -> str : return f \"Custom { self . _class_key_ } \" # :shrug: class Time ( _TimeMixin , Type ): pass class Timestamp ( _TimeMixin , Type ): \"\"\"UTC timestamp with configurable precision.\"\"\" @property def friendly_key ( self ) -> str : return f \" { self . precision . title () }{ self . _class_key_ } \" class UInt8 ( _Int ): pass class UInt16 ( _Int ): pass class UInt32 ( _Int ): pass class UInt64 ( _Int ): pass ############################## # Type conversion interfaces # ############################## class TypeAdapter : \"\"\"TypeAdapter maps between Artigraph types and a foreign type system.\"\"\" key : ClassVar [ str ] = class_name () artigraph : ClassVar [ type [ Type ]] # The internal Artigraph Type system : ClassVar [ Any ] # The external system's type priority : ClassVar [ int ] = 0 # Set the priority of this mapping. Higher is better. @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , cls . artigraph ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : raise NotImplementedError () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : raise NotImplementedError () @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : raise NotImplementedError () # _ScalarClassTypeAdapter can be used for scalars defined as python types (eg: int or str for the # python TypeSystem). class _ScalarClassTypeAdapter ( TypeAdapter ): @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return cls . artigraph () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return lenient_issubclass ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : return cls . system @classmethod def generate ( cls , * , artigraph : type [ Type ], system : Any , priority : int = 0 , type_system : TypeSystem , name : Optional [ str ] = None , ) -> type [ TypeAdapter ]: \"\"\"Generate a _ScalarClassTypeAdapter subclass for the scalar system type.\"\"\" name = name or f \" { type_system . key }{ artigraph . __name__ } \" return type_system . register_adapter ( type ( name , ( cls ,), { \"artigraph\" : artigraph , \"system\" : system , \"priority\" : priority }, ) ) class TypeSystem ( Model ): key : str extends : tuple [ TypeSystem , ... ] = () # NOTE: Use a NoCopyDict to avoid copies of the registry. Otherwise, TypeSystems that extend # this TypeSystem will only see the adapters registered *as of initialization* (as pydantic # would deepcopy the TypeSystems in the `extends` argument). _adapter_by_key : NoCopyDict [ str , type [ TypeAdapter ]] = PrivateAttr ( default_factory = NoCopyDict ) def register_adapter ( self , adapter : type [ TypeAdapter ]) -> type [ TypeAdapter ]: return register ( self . _adapter_by_key , adapter . key , adapter ) @property def _priority_sorted_adapters ( self ) -> list [ type [ TypeAdapter ]]: return sorted ( self . _adapter_by_key . values (), key = attrgetter ( \"priority\" ), reverse = True ) def to_artigraph ( self , type_ : Any , * , hints : dict [ str , Any ], root_type_system : Optional [ TypeSystem ] = None ) -> Type : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ): return adapter . to_artigraph ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_artigraph ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No { root_type_system } adapter for system type: { type_ } .\" ) def to_system ( self , type_ : Type , * , hints : dict [ str , Any ], root_type_system : Optional [ TypeSystem ] = None ) -> Any : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ): return adapter . to_system ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_system ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No { root_type_system } adapter for Artigraph type: { type_ } .\" ) if tuple ( int ( i ) for i in pydantic_version . split ( \".\" )) < ( 1 , 10 ): # pragma: no cover # Fix ForwardRefs in outer_type_ before https://github.com/samuelcolvin/pydantic/pull/4249 TypeSystem . __fields__ [ \"extends\" ] . outer_type_ = tuple [ TypeSystem , ... ] Sub-modules arti.types.bigquery arti.types.numpy arti.types.pandas arti.types.pyarrow arti.types.pydantic arti.types.python Variables DEFAULT_ANONYMOUS_NAME pydantic_version Functions is_partitioned def is_partitioned ( type_ : 'Type' ) -> 'bool' Helper function to determine whether the type is partitioned. View Source def is_partitioned ( type_ : Type ) -> bool : \"\"\"Helper function to determine whether the type is partitioned.\"\"\" return isinstance ( type_ , Collection ) and bool ( type_ . partition_fields ) Classes Binary class Binary ( __pydantic_self__ , ** data : Any ) View Source class Binary ( Type ) : byte_size : Optional [ int ] Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Boolean class Boolean ( __pydantic_self__ , ** data : Any ) View Source class Boolean ( Type ): pass Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Collection class Collection ( __pydantic_self__ , ** data : Any ) View Source class Collection ( _NamedMixin , List ) : \"\"\"A collection of elements with partition and cluster metadata. Collections should not be nested in other types. \"\"\" partition_by : tuple [ str, ... ] = () cluster_by : tuple [ str, ... ] = () @validator ( \"partition_by\" , \"cluster_by\" ) @classmethod def _validate_field_ref ( cls , references : tuple [ str, ... ] , values : dict [ str, Any ] ) -> tuple [ str, ... ] : if ( element : = values . get ( \"element\" )) is None : return references if references and not isinstance ( element , Struct ) : raise ValueError ( \"requires element to be a Struct\" ) known , requested = set ( element . fields ), set ( references ) if unknown : = requested - known : raise ValueError ( f \"unknown field(s): {unknown}\" ) return references @validator ( \"cluster_by\" ) @classmethod def _validate_cluster_by ( cls , cluster_by : tuple [ str, ... ] , values : dict [ str, Any ] ) -> tuple [ str, ... ] : if ( partition_by : = values . get ( \"partition_by\" )) is None : return cluster_by if overlapping : = set ( cluster_by ) & set ( partition_by ) : raise ValueError ( f \"clustering fields overlap with partition fields: {overlapping}\" ) return cluster_by @property def _default_friendly_key ( self ) -> str : return f \"{self.element.friendly_key}{self._class_key_}\" @property def fields ( self ) -> frozendict [ str, Type ] : \"\"\"Shorthand accessor to access Struct element fields. If the element is not a Struct, an AttributeError will be raised. \"\"\" return self . element . fields # type : ignore [ attr-defined,no-any-return ] # We want the standard AttributeError @property def partition_fields ( self ) -> frozendict [ str, Type ] : if not isinstance ( self . element , Struct ) : return frozendict () return frozendict ( { name : self . element . fields [ name ] for name in self . partition_by } ) Ancestors (in MRO) arti.types._NamedMixin arti.types.List arti.types._ContainerMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fields Shorthand accessor to access Struct element fields. If the element is not a Struct, an AttributeError will be raised. fingerprint friendly_key partition_fields Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Date class Date ( __pydantic_self__ , ** data : Any ) View Source class Date ( Type ): pass Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . DateTime class DateTime ( __pydantic_self__ , ** data : Any ) View Source class DateTime ( _TimeMixin , Type ): \"\"\"A Date and Time as shown on a calendar and clock, independent of timezone.\"\"\" Ancestors (in MRO) arti.types._TimeMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Enum class Enum ( __pydantic_self__ , ** data : Any ) View Source class Enum ( _NamedMixin , Type ): type : Type items : frozenset [ Any ] @validator ( \"items\" , pre = True ) @classmethod def _cast_values ( cls , items : Any ) -> Any : if isinstance ( items , Iterable ) and not isinstance ( items , Mapping ): return frozenset ( items ) return items @validator ( \"items\" ) @classmethod def _validate_values ( cls , items : frozenset [ Any ], values : dict [ str , Any ]) -> frozenset [ Any ]: from arti.types.python import python_type_system if len ( items ) == 0 : raise ValueError ( \"cannot be empty.\" ) # `type` will be missing if it doesn't pass validation. if ( arti_type := values . get ( \"type\" )) is None : return items py_type = python_type_system . to_system ( arti_type , hints = {}) mismatched_items = [ item for item in items if not lenient_issubclass ( type ( item ), py_type )] if mismatched_items : raise ValueError ( f \"incompatible { arti_type } ( { py_type } ) item(s): { mismatched_items } \" ) return items @property def _default_friendly_key ( self ) -> str : return f \" { self . type . friendly_key }{ self . _class_key_ } \" Ancestors (in MRO) arti.types._NamedMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Float16 class Float16 ( __pydantic_self__ , ** data : Any ) View Source class Float16 ( _Float ): pass Ancestors (in MRO) arti.types._Float arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Float32 class Float32 ( __pydantic_self__ , ** data : Any ) View Source class Float32 ( _Float ): pass Ancestors (in MRO) arti.types._Float arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Float64 class Float64 ( __pydantic_self__ , ** data : Any ) View Source class Float64 ( _Float ): pass Ancestors (in MRO) arti.types._Float arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Geography class Geography ( __pydantic_self__ , ** data : Any ) View Source class Geography ( Type ) : format : Optional [ str ] # \"WKB\" , \"WKT\" , etc srid : Optional [ str ] Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int16 class Int16 ( __pydantic_self__ , ** data : Any ) View Source class Int16 ( _Int ): pass Ancestors (in MRO) arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int32 class Int32 ( __pydantic_self__ , ** data : Any ) View Source class Int32 ( _Int ): pass Ancestors (in MRO) arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int64 class Int64 ( __pydantic_self__ , ** data : Any ) View Source class Int64 ( _Int ): pass Ancestors (in MRO) arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int8 class Int8 ( __pydantic_self__ , ** data : Any ) View Source class Int8 ( _Int ): pass Ancestors (in MRO) arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . List class List ( __pydantic_self__ , ** data : Any ) View Source class List ( _ContainerMixin , Type ) : @property def friendly_key ( self ) -> str : return f \"{self.element.friendly_key}{self._class_key_}\" Ancestors (in MRO) arti.types._ContainerMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.types.Collection Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Map class Map ( __pydantic_self__ , ** data : Any ) View Source class Map ( Type ) : key : Type value : Type @property def friendly_key ( self ) -> str : return f \"{self.key.friendly_key}To{self.value.friendly_key}\" Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Null class Null ( __pydantic_self__ , ** data : Any ) View Source class Null ( Type ): pass Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Set class Set ( __pydantic_self__ , ** data : Any ) View Source class Set ( _ContainerMixin , Type ) : @property def friendly_key ( self ) -> str : return f \"{self.element.friendly_key}{self._class_key_}\" Ancestors (in MRO) arti.types._ContainerMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . String class String ( __pydantic_self__ , ** data : Any ) View Source class String ( Type ): pass Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Struct class Struct ( __pydantic_self__ , ** data : Any ) View Source class Struct ( _NamedMixin , Type ) : fields : frozendict [ str, Type ] @property def _default_friendly_key ( self ) -> str : return f \"Custom{self._class_key_}\" # : shrug : Ancestors (in MRO) arti.types._NamedMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Time class Time ( __pydantic_self__ , ** data : Any ) View Source class Time ( _TimeMixin , Type ): pass Ancestors (in MRO) arti.types._TimeMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Timestamp class Timestamp ( __pydantic_self__ , ** data : Any ) View Source class Timestamp ( _TimeMixin , Type ) : \"\"\"UTC timestamp with configurable precision.\"\"\" @property def friendly_key ( self ) -> str : return f \"{self.precision.title()}{self._class_key_}\" Ancestors (in MRO) arti.types._TimeMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Type class Type ( __pydantic_self__ , ** data : Any ) View Source class Type ( Model ) : \"\"\"Type represents a data type.\"\"\" _abstract_ = True # NOTE : Exclude the description to minimize fingerprint changes ( and thus rebuilds ). _fingerprint_excludes_ = frozenset ( [ \"description\" ] ) description : Optional [ str ] nullable : bool = False @property def friendly_key ( self ) -> str : \"\"\"A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. \"\"\" return self . _class_key_ Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.types._Numeric arti.types.Binary arti.types.Boolean arti.types.Date arti.types.DateTime arti.types.Enum arti.types.Geography arti.types.List arti.types.Map arti.types.Null arti.types.Set arti.types.String arti.types.Struct arti.types.Time arti.types.Timestamp Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . TypeAdapter class TypeAdapter ( / , * args , ** kwargs ) View Source class TypeAdapter : \"\"\"TypeAdapter maps between Artigraph types and a foreign type system.\"\"\" key : ClassVar [ str ] = class_name () artigraph : ClassVar [ type[Type ] ] # The internal Artigraph Type system : ClassVar [ Any ] # The external system ' s type priority : ClassVar [ int ] = 0 # Set the priority of this mapping . Higher is better . @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : raise NotImplementedError () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : raise NotImplementedError () @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : raise NotImplementedError () Descendants arti.types._ScalarClassTypeAdapter arti.types.python.PyValueContainer arti.types.python.PyLiteral arti.types.python.PyMap arti.types.python.PyOptional arti.types.python.PyStruct arti.types.bigquery._BigQueryTypeAdapter arti.types.bigquery.ListFieldTypeAdapter arti.types.bigquery.TableTypeAdapter arti.types.numpy.ArrayAdapter arti.types.pandas.SeriesAdapter arti.types.pandas.DataFrameAdapter arti.types.pyarrow._PyarrowTypeAdapter arti.types.pydantic.BaseModelAdapter Class variables key priority Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : raise NotImplementedError () to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : raise NotImplementedError () to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : raise NotImplementedError () TypeSystem class TypeSystem ( __pydantic_self__ , ** data : Any ) View Source class TypeSystem ( Model ) : key : str extends : tuple [ TypeSystem, ... ] = () # NOTE : Use a NoCopyDict to avoid copies of the registry . Otherwise , TypeSystems that extend # this TypeSystem will only see the adapters registered * as of initialization * ( as pydantic # would deepcopy the TypeSystems in the ` extends ` argument ). _adapter_by_key : NoCopyDict [ str, type[TypeAdapter ] ] = PrivateAttr ( default_factory = NoCopyDict ) def register_adapter ( self , adapter : type [ TypeAdapter ] ) -> type [ TypeAdapter ] : return register ( self . _adapter_by_key , adapter . key , adapter ) @property def _priority_sorted_adapters ( self ) -> list [ type[TypeAdapter ] ]: return sorted ( self . _adapter_by_key . values (), key = attrgetter ( \"priority\" ), reverse = True ) def to_artigraph ( self , type_ : Any , * , hints : dict [ str, Any ] , root_type_system : Optional [ TypeSystem ] = None ) -> Type : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ) : return adapter . to_artigraph ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_artigraph ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No {root_type_system} adapter for system type: {type_}.\" ) def to_system ( self , type_ : Type , * , hints : dict [ str, Any ] , root_type_system : Optional [ TypeSystem ] = None ) -> Any : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ) : return adapter . to_system ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_system ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No {root_type_system} adapter for Artigraph type: {type_}.\" ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . register_adapter def register_adapter ( self , adapter : 'type[TypeAdapter]' ) -> 'type[TypeAdapter]' View Source def register_adapter ( self , adapter : type [ TypeAdapter ] ) -> type [ TypeAdapter ] : return register ( self . _adapter_by_key , adapter . key , adapter ) to_artigraph def to_artigraph ( self , type_ : 'Any' , * , hints : 'dict[str, Any]' , root_type_system : 'Optional[TypeSystem]' = None ) -> 'Type' View Source def to_artigraph ( self , type_ : Any , * , hints : dict [ str, Any ] , root_type_system : Optional [ TypeSystem ] = None ) -> Type : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ) : return adapter . to_artigraph ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_artigraph ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No {root_type_system} adapter for system type: {type_}.\" ) to_system def to_system ( self , type_ : 'Type' , * , hints : 'dict[str, Any]' , root_type_system : 'Optional[TypeSystem]' = None ) -> 'Any' View Source def to_system ( self , type_ : Type , * , hints : dict [ str, Any ] , root_type_system : Optional [ TypeSystem ] = None ) -> Any : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ) : return adapter . to_system ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_system ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No {root_type_system} adapter for Artigraph type: {type_}.\" ) UInt16 class UInt16 ( __pydantic_self__ , ** data : Any ) View Source class UInt16 ( _Int ): pass Ancestors (in MRO) arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . UInt32 class UInt32 ( __pydantic_self__ , ** data : Any ) View Source class UInt32 ( _Int ): pass Ancestors (in MRO) arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . UInt64 class UInt64 ( __pydantic_self__ , ** data : Any ) View Source class UInt64 ( _Int ): pass Ancestors (in MRO) arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . UInt8 class UInt8 ( __pydantic_self__ , ** data : Any ) View Source class UInt8 ( _Int ): pass Ancestors (in MRO) arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Index"},{"location":"reference/arti/types/#module-artitypes","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from abc import abstractmethod from collections.abc import Iterable , Mapping from operator import attrgetter from typing import Any , ClassVar , Literal , Optional from pydantic import PrivateAttr , validator from pydantic import __version__ as pydantic_version from arti.internal.models import Model from arti.internal.type_hints import lenient_issubclass from arti.internal.utils import NoCopyDict , class_name , frozendict , register DEFAULT_ANONYMOUS_NAME = \"anon\" class Type ( Model ): \"\"\"Type represents a data type.\"\"\" _abstract_ = True # NOTE: Exclude the description to minimize fingerprint changes (and thus rebuilds). _fingerprint_excludes_ = frozenset ([ \"description\" ]) description : Optional [ str ] nullable : bool = False @property def friendly_key ( self ) -> str : \"\"\"A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. \"\"\" return self . _class_key_ def is_partitioned ( type_ : Type ) -> bool : \"\"\"Helper function to determine whether the type is partitioned.\"\"\" return isinstance ( type_ , Collection ) and bool ( type_ . partition_fields ) class _ContainerMixin ( Model ): element : Type class _NamedMixin ( Model ): name : str = DEFAULT_ANONYMOUS_NAME @classmethod def _pydantic_type_system_post_field_conversion_hook_ ( cls , type_ : Type , * , name : str , required : bool ) -> Type : type_ = super () . _pydantic_type_system_post_field_conversion_hook_ ( type_ , name = name , required = required ) if \"name\" not in type_ . __fields_set__ : type_ = type_ . copy ( update = { \"name\" : name }) return type_ @property @abstractmethod def _default_friendly_key ( self ) -> str : raise NotImplementedError () @property def friendly_key ( self ) -> str : return self . _default_friendly_key if self . name == DEFAULT_ANONYMOUS_NAME else self . name class _TimeMixin ( Model ): precision : Literal [ \"second\" , \"millisecond\" , \"microsecond\" , \"nanosecond\" ] ######################## # Core Artigraph Types # ######################## class _Numeric ( Type ): pass class _Float ( _Numeric ): pass class _Int ( _Numeric ): pass class Binary ( Type ): byte_size : Optional [ int ] class Boolean ( Type ): pass class Date ( Type ): pass class DateTime ( _TimeMixin , Type ): \"\"\"A Date and Time as shown on a calendar and clock, independent of timezone.\"\"\" class Enum ( _NamedMixin , Type ): type : Type items : frozenset [ Any ] @validator ( \"items\" , pre = True ) @classmethod def _cast_values ( cls , items : Any ) -> Any : if isinstance ( items , Iterable ) and not isinstance ( items , Mapping ): return frozenset ( items ) return items @validator ( \"items\" ) @classmethod def _validate_values ( cls , items : frozenset [ Any ], values : dict [ str , Any ]) -> frozenset [ Any ]: from arti.types.python import python_type_system if len ( items ) == 0 : raise ValueError ( \"cannot be empty.\" ) # `type` will be missing if it doesn't pass validation. if ( arti_type := values . get ( \"type\" )) is None : return items py_type = python_type_system . to_system ( arti_type , hints = {}) mismatched_items = [ item for item in items if not lenient_issubclass ( type ( item ), py_type )] if mismatched_items : raise ValueError ( f \"incompatible { arti_type } ( { py_type } ) item(s): { mismatched_items } \" ) return items @property def _default_friendly_key ( self ) -> str : return f \" { self . type . friendly_key }{ self . _class_key_ } \" class Float16 ( _Float ): pass class Float32 ( _Float ): pass class Float64 ( _Float ): pass class Geography ( Type ): format : Optional [ str ] # \"WKB\", \"WKT\", etc srid : Optional [ str ] class Int8 ( _Int ): pass class Int16 ( _Int ): pass class Int32 ( _Int ): pass class Int64 ( _Int ): pass class List ( _ContainerMixin , Type ): @property def friendly_key ( self ) -> str : return f \" { self . element . friendly_key }{ self . _class_key_ } \" class Collection ( _NamedMixin , List ): \"\"\"A collection of elements with partition and cluster metadata. Collections should not be nested in other types. \"\"\" partition_by : tuple [ str , ... ] = () cluster_by : tuple [ str , ... ] = () @validator ( \"partition_by\" , \"cluster_by\" ) @classmethod def _validate_field_ref ( cls , references : tuple [ str , ... ], values : dict [ str , Any ] ) -> tuple [ str , ... ]: if ( element := values . get ( \"element\" )) is None : return references if references and not isinstance ( element , Struct ): raise ValueError ( \"requires element to be a Struct\" ) known , requested = set ( element . fields ), set ( references ) if unknown := requested - known : raise ValueError ( f \"unknown field(s): { unknown } \" ) return references @validator ( \"cluster_by\" ) @classmethod def _validate_cluster_by ( cls , cluster_by : tuple [ str , ... ], values : dict [ str , Any ] ) -> tuple [ str , ... ]: if ( partition_by := values . get ( \"partition_by\" )) is None : return cluster_by if overlapping := set ( cluster_by ) & set ( partition_by ): raise ValueError ( f \"clustering fields overlap with partition fields: { overlapping } \" ) return cluster_by @property def _default_friendly_key ( self ) -> str : return f \" { self . element . friendly_key }{ self . _class_key_ } \" @property def fields ( self ) -> frozendict [ str , Type ]: \"\"\"Shorthand accessor to access Struct element fields. If the element is not a Struct, an AttributeError will be raised. \"\"\" return self . element . fields # type: ignore[attr-defined,no-any-return] # We want the standard AttributeError @property def partition_fields ( self ) -> frozendict [ str , Type ]: if not isinstance ( self . element , Struct ): return frozendict () return frozendict ({ name : self . element . fields [ name ] for name in self . partition_by }) class Map ( Type ): key : Type value : Type @property def friendly_key ( self ) -> str : return f \" { self . key . friendly_key } To { self . value . friendly_key } \" class Null ( Type ): pass class Set ( _ContainerMixin , Type ): @property def friendly_key ( self ) -> str : return f \" { self . element . friendly_key }{ self . _class_key_ } \" class String ( Type ): pass class Struct ( _NamedMixin , Type ): fields : frozendict [ str , Type ] @property def _default_friendly_key ( self ) -> str : return f \"Custom { self . _class_key_ } \" # :shrug: class Time ( _TimeMixin , Type ): pass class Timestamp ( _TimeMixin , Type ): \"\"\"UTC timestamp with configurable precision.\"\"\" @property def friendly_key ( self ) -> str : return f \" { self . precision . title () }{ self . _class_key_ } \" class UInt8 ( _Int ): pass class UInt16 ( _Int ): pass class UInt32 ( _Int ): pass class UInt64 ( _Int ): pass ############################## # Type conversion interfaces # ############################## class TypeAdapter : \"\"\"TypeAdapter maps between Artigraph types and a foreign type system.\"\"\" key : ClassVar [ str ] = class_name () artigraph : ClassVar [ type [ Type ]] # The internal Artigraph Type system : ClassVar [ Any ] # The external system's type priority : ClassVar [ int ] = 0 # Set the priority of this mapping. Higher is better. @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , cls . artigraph ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : raise NotImplementedError () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : raise NotImplementedError () @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : raise NotImplementedError () # _ScalarClassTypeAdapter can be used for scalars defined as python types (eg: int or str for the # python TypeSystem). class _ScalarClassTypeAdapter ( TypeAdapter ): @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return cls . artigraph () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return lenient_issubclass ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : return cls . system @classmethod def generate ( cls , * , artigraph : type [ Type ], system : Any , priority : int = 0 , type_system : TypeSystem , name : Optional [ str ] = None , ) -> type [ TypeAdapter ]: \"\"\"Generate a _ScalarClassTypeAdapter subclass for the scalar system type.\"\"\" name = name or f \" { type_system . key }{ artigraph . __name__ } \" return type_system . register_adapter ( type ( name , ( cls ,), { \"artigraph\" : artigraph , \"system\" : system , \"priority\" : priority }, ) ) class TypeSystem ( Model ): key : str extends : tuple [ TypeSystem , ... ] = () # NOTE: Use a NoCopyDict to avoid copies of the registry. Otherwise, TypeSystems that extend # this TypeSystem will only see the adapters registered *as of initialization* (as pydantic # would deepcopy the TypeSystems in the `extends` argument). _adapter_by_key : NoCopyDict [ str , type [ TypeAdapter ]] = PrivateAttr ( default_factory = NoCopyDict ) def register_adapter ( self , adapter : type [ TypeAdapter ]) -> type [ TypeAdapter ]: return register ( self . _adapter_by_key , adapter . key , adapter ) @property def _priority_sorted_adapters ( self ) -> list [ type [ TypeAdapter ]]: return sorted ( self . _adapter_by_key . values (), key = attrgetter ( \"priority\" ), reverse = True ) def to_artigraph ( self , type_ : Any , * , hints : dict [ str , Any ], root_type_system : Optional [ TypeSystem ] = None ) -> Type : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ): return adapter . to_artigraph ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_artigraph ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No { root_type_system } adapter for system type: { type_ } .\" ) def to_system ( self , type_ : Type , * , hints : dict [ str , Any ], root_type_system : Optional [ TypeSystem ] = None ) -> Any : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ): return adapter . to_system ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_system ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No { root_type_system } adapter for Artigraph type: { type_ } .\" ) if tuple ( int ( i ) for i in pydantic_version . split ( \".\" )) < ( 1 , 10 ): # pragma: no cover # Fix ForwardRefs in outer_type_ before https://github.com/samuelcolvin/pydantic/pull/4249 TypeSystem . __fields__ [ \"extends\" ] . outer_type_ = tuple [ TypeSystem , ... ]","title":"Module arti.types"},{"location":"reference/arti/types/#sub-modules","text":"arti.types.bigquery arti.types.numpy arti.types.pandas arti.types.pyarrow arti.types.pydantic arti.types.python","title":"Sub-modules"},{"location":"reference/arti/types/#variables","text":"DEFAULT_ANONYMOUS_NAME pydantic_version","title":"Variables"},{"location":"reference/arti/types/#functions","text":"","title":"Functions"},{"location":"reference/arti/types/#is_partitioned","text":"def is_partitioned ( type_ : 'Type' ) -> 'bool' Helper function to determine whether the type is partitioned. View Source def is_partitioned ( type_ : Type ) -> bool : \"\"\"Helper function to determine whether the type is partitioned.\"\"\" return isinstance ( type_ , Collection ) and bool ( type_ . partition_fields )","title":"is_partitioned"},{"location":"reference/arti/types/#classes","text":"","title":"Classes"},{"location":"reference/arti/types/#binary","text":"class Binary ( __pydantic_self__ , ** data : Any ) View Source class Binary ( Type ) : byte_size : Optional [ int ]","title":"Binary"},{"location":"reference/arti/types/#ancestors-in-mro","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods","text":"","title":"Methods"},{"location":"reference/arti/types/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#boolean","text":"class Boolean ( __pydantic_self__ , ** data : Any ) View Source class Boolean ( Type ): pass","title":"Boolean"},{"location":"reference/arti/types/#ancestors-in-mro_1","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_1","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_1","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#collection","text":"class Collection ( __pydantic_self__ , ** data : Any ) View Source class Collection ( _NamedMixin , List ) : \"\"\"A collection of elements with partition and cluster metadata. Collections should not be nested in other types. \"\"\" partition_by : tuple [ str, ... ] = () cluster_by : tuple [ str, ... ] = () @validator ( \"partition_by\" , \"cluster_by\" ) @classmethod def _validate_field_ref ( cls , references : tuple [ str, ... ] , values : dict [ str, Any ] ) -> tuple [ str, ... ] : if ( element : = values . get ( \"element\" )) is None : return references if references and not isinstance ( element , Struct ) : raise ValueError ( \"requires element to be a Struct\" ) known , requested = set ( element . fields ), set ( references ) if unknown : = requested - known : raise ValueError ( f \"unknown field(s): {unknown}\" ) return references @validator ( \"cluster_by\" ) @classmethod def _validate_cluster_by ( cls , cluster_by : tuple [ str, ... ] , values : dict [ str, Any ] ) -> tuple [ str, ... ] : if ( partition_by : = values . get ( \"partition_by\" )) is None : return cluster_by if overlapping : = set ( cluster_by ) & set ( partition_by ) : raise ValueError ( f \"clustering fields overlap with partition fields: {overlapping}\" ) return cluster_by @property def _default_friendly_key ( self ) -> str : return f \"{self.element.friendly_key}{self._class_key_}\" @property def fields ( self ) -> frozendict [ str, Type ] : \"\"\"Shorthand accessor to access Struct element fields. If the element is not a Struct, an AttributeError will be raised. \"\"\" return self . element . fields # type : ignore [ attr-defined,no-any-return ] # We want the standard AttributeError @property def partition_fields ( self ) -> frozendict [ str, Type ] : if not isinstance ( self . element , Struct ) : return frozendict () return frozendict ( { name : self . element . fields [ name ] for name in self . partition_by } )","title":"Collection"},{"location":"reference/arti/types/#ancestors-in-mro_2","text":"arti.types._NamedMixin arti.types.List arti.types._ContainerMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_2","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_2","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_2","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_2","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_2","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_2","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_2","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_2","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_2","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_2","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_2","text":"fields Shorthand accessor to access Struct element fields. If the element is not a Struct, an AttributeError will be raised. fingerprint friendly_key partition_fields","title":"Instance variables"},{"location":"reference/arti/types/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_2","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_2","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_2","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#date","text":"class Date ( __pydantic_self__ , ** data : Any ) View Source class Date ( Type ): pass","title":"Date"},{"location":"reference/arti/types/#ancestors-in-mro_3","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_3","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_3","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_3","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_3","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_3","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_3","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_3","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_3","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_3","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_3","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_3","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_3","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_3","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_3","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_3","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#datetime","text":"class DateTime ( __pydantic_self__ , ** data : Any ) View Source class DateTime ( _TimeMixin , Type ): \"\"\"A Date and Time as shown on a calendar and clock, independent of timezone.\"\"\"","title":"DateTime"},{"location":"reference/arti/types/#ancestors-in-mro_4","text":"arti.types._TimeMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_4","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_4","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_4","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_4","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_4","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_4","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_4","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_4","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_4","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_4","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_4","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_4","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_4","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_4","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_4","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#enum","text":"class Enum ( __pydantic_self__ , ** data : Any ) View Source class Enum ( _NamedMixin , Type ): type : Type items : frozenset [ Any ] @validator ( \"items\" , pre = True ) @classmethod def _cast_values ( cls , items : Any ) -> Any : if isinstance ( items , Iterable ) and not isinstance ( items , Mapping ): return frozenset ( items ) return items @validator ( \"items\" ) @classmethod def _validate_values ( cls , items : frozenset [ Any ], values : dict [ str , Any ]) -> frozenset [ Any ]: from arti.types.python import python_type_system if len ( items ) == 0 : raise ValueError ( \"cannot be empty.\" ) # `type` will be missing if it doesn't pass validation. if ( arti_type := values . get ( \"type\" )) is None : return items py_type = python_type_system . to_system ( arti_type , hints = {}) mismatched_items = [ item for item in items if not lenient_issubclass ( type ( item ), py_type )] if mismatched_items : raise ValueError ( f \"incompatible { arti_type } ( { py_type } ) item(s): { mismatched_items } \" ) return items @property def _default_friendly_key ( self ) -> str : return f \" { self . type . friendly_key }{ self . _class_key_ } \"","title":"Enum"},{"location":"reference/arti/types/#ancestors-in-mro_5","text":"arti.types._NamedMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_5","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_5","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_5","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_5","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_5","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_5","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_5","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_5","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_5","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_5","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_5","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_5","text":"fingerprint friendly_key","title":"Instance variables"},{"location":"reference/arti/types/#methods_5","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_5","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_5","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_5","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#float16","text":"class Float16 ( __pydantic_self__ , ** data : Any ) View Source class Float16 ( _Float ): pass","title":"Float16"},{"location":"reference/arti/types/#ancestors-in-mro_6","text":"arti.types._Float arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_6","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_6","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_6","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_6","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_6","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_6","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_6","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_6","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_6","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_6","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_6","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_6","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_6","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_6","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_6","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_6","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#float32","text":"class Float32 ( __pydantic_self__ , ** data : Any ) View Source class Float32 ( _Float ): pass","title":"Float32"},{"location":"reference/arti/types/#ancestors-in-mro_7","text":"arti.types._Float arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_7","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_7","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_7","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_7","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_7","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_7","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_7","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_7","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_7","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_7","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_7","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_7","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_7","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_7","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_7","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_7","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#float64","text":"class Float64 ( __pydantic_self__ , ** data : Any ) View Source class Float64 ( _Float ): pass","title":"Float64"},{"location":"reference/arti/types/#ancestors-in-mro_8","text":"arti.types._Float arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_8","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_8","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_8","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_8","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_8","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_8","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_8","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_8","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_8","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_8","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_8","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_8","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_8","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_8","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_8","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_8","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#geography","text":"class Geography ( __pydantic_self__ , ** data : Any ) View Source class Geography ( Type ) : format : Optional [ str ] # \"WKB\" , \"WKT\" , etc srid : Optional [ str ]","title":"Geography"},{"location":"reference/arti/types/#ancestors-in-mro_9","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_9","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_9","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_9","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_9","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_9","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_9","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_9","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_9","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_9","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_9","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_9","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_9","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_9","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_9","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_9","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_9","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#int16","text":"class Int16 ( __pydantic_self__ , ** data : Any ) View Source class Int16 ( _Int ): pass","title":"Int16"},{"location":"reference/arti/types/#ancestors-in-mro_10","text":"arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_10","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_10","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_10","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_10","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_10","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_10","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_10","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_10","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_10","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_10","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_10","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_10","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_10","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_10","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_10","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_10","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#int32","text":"class Int32 ( __pydantic_self__ , ** data : Any ) View Source class Int32 ( _Int ): pass","title":"Int32"},{"location":"reference/arti/types/#ancestors-in-mro_11","text":"arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_11","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_11","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_11","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_11","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_11","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_11","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_11","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_11","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_11","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_11","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_11","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_11","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_11","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_11","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_11","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_11","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#int64","text":"class Int64 ( __pydantic_self__ , ** data : Any ) View Source class Int64 ( _Int ): pass","title":"Int64"},{"location":"reference/arti/types/#ancestors-in-mro_12","text":"arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_12","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_12","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_12","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_12","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_12","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_12","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_12","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_12","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_12","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_12","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_12","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_12","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_12","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_12","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_12","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_12","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#int8","text":"class Int8 ( __pydantic_self__ , ** data : Any ) View Source class Int8 ( _Int ): pass","title":"Int8"},{"location":"reference/arti/types/#ancestors-in-mro_13","text":"arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_13","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_13","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_13","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_13","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_13","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_13","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_13","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_13","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_13","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_13","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_13","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_13","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_13","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_13","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_13","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_13","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#list","text":"class List ( __pydantic_self__ , ** data : Any ) View Source class List ( _ContainerMixin , Type ) : @property def friendly_key ( self ) -> str : return f \"{self.element.friendly_key}{self._class_key_}\"","title":"List"},{"location":"reference/arti/types/#ancestors-in-mro_14","text":"arti.types._ContainerMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#descendants","text":"arti.types.Collection","title":"Descendants"},{"location":"reference/arti/types/#class-variables_14","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_14","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_14","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_14","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_14","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_14","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_14","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_14","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_14","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_14","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_14","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_14","text":"fingerprint friendly_key","title":"Instance variables"},{"location":"reference/arti/types/#methods_14","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_14","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_14","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_14","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#map","text":"class Map ( __pydantic_self__ , ** data : Any ) View Source class Map ( Type ) : key : Type value : Type @property def friendly_key ( self ) -> str : return f \"{self.key.friendly_key}To{self.value.friendly_key}\"","title":"Map"},{"location":"reference/arti/types/#ancestors-in-mro_15","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_15","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_15","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_15","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_15","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_15","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_15","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_15","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_15","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_15","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_15","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_15","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_15","text":"fingerprint friendly_key","title":"Instance variables"},{"location":"reference/arti/types/#methods_15","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_15","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_15","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_15","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#null","text":"class Null ( __pydantic_self__ , ** data : Any ) View Source class Null ( Type ): pass","title":"Null"},{"location":"reference/arti/types/#ancestors-in-mro_16","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_16","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_16","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_16","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_16","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_16","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_16","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_16","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_16","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_16","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_16","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_16","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_16","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_16","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_16","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_16","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_16","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#set","text":"class Set ( __pydantic_self__ , ** data : Any ) View Source class Set ( _ContainerMixin , Type ) : @property def friendly_key ( self ) -> str : return f \"{self.element.friendly_key}{self._class_key_}\"","title":"Set"},{"location":"reference/arti/types/#ancestors-in-mro_17","text":"arti.types._ContainerMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_17","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_17","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_17","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_17","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_17","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_17","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_17","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_17","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_17","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_17","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_17","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_17","text":"fingerprint friendly_key","title":"Instance variables"},{"location":"reference/arti/types/#methods_17","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_17","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_17","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_17","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#string","text":"class String ( __pydantic_self__ , ** data : Any ) View Source class String ( Type ): pass","title":"String"},{"location":"reference/arti/types/#ancestors-in-mro_18","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_18","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_18","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_18","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_18","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_18","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_18","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_18","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_18","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_18","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_18","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_18","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_18","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_18","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_18","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_18","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_18","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#struct","text":"class Struct ( __pydantic_self__ , ** data : Any ) View Source class Struct ( _NamedMixin , Type ) : fields : frozendict [ str, Type ] @property def _default_friendly_key ( self ) -> str : return f \"Custom{self._class_key_}\" # : shrug :","title":"Struct"},{"location":"reference/arti/types/#ancestors-in-mro_19","text":"arti.types._NamedMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_19","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_19","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_19","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_19","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_19","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_19","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_19","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_19","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_19","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_19","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_19","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_19","text":"fingerprint friendly_key","title":"Instance variables"},{"location":"reference/arti/types/#methods_19","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_19","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_19","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_19","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#time","text":"class Time ( __pydantic_self__ , ** data : Any ) View Source class Time ( _TimeMixin , Type ): pass","title":"Time"},{"location":"reference/arti/types/#ancestors-in-mro_20","text":"arti.types._TimeMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_20","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_20","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_20","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_20","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_20","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_20","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_20","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_20","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_20","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_20","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_20","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_20","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_20","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_20","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_20","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_20","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#timestamp","text":"class Timestamp ( __pydantic_self__ , ** data : Any ) View Source class Timestamp ( _TimeMixin , Type ) : \"\"\"UTC timestamp with configurable precision.\"\"\" @property def friendly_key ( self ) -> str : return f \"{self.precision.title()}{self._class_key_}\"","title":"Timestamp"},{"location":"reference/arti/types/#ancestors-in-mro_21","text":"arti.types._TimeMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_21","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_21","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_21","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_21","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_21","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_21","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_21","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_21","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_21","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_21","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_21","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_21","text":"fingerprint friendly_key","title":"Instance variables"},{"location":"reference/arti/types/#methods_21","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_21","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_21","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_21","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#type","text":"class Type ( __pydantic_self__ , ** data : Any ) View Source class Type ( Model ) : \"\"\"Type represents a data type.\"\"\" _abstract_ = True # NOTE : Exclude the description to minimize fingerprint changes ( and thus rebuilds ). _fingerprint_excludes_ = frozenset ( [ \"description\" ] ) description : Optional [ str ] nullable : bool = False @property def friendly_key ( self ) -> str : \"\"\"A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. \"\"\" return self . _class_key_","title":"Type"},{"location":"reference/arti/types/#ancestors-in-mro_22","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#descendants_1","text":"arti.types._Numeric arti.types.Binary arti.types.Boolean arti.types.Date arti.types.DateTime arti.types.Enum arti.types.Geography arti.types.List arti.types.Map arti.types.Null arti.types.Set arti.types.String arti.types.Struct arti.types.Time arti.types.Timestamp","title":"Descendants"},{"location":"reference/arti/types/#class-variables_22","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_22","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_22","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_22","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_22","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_22","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_22","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_22","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_22","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_22","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_22","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_22","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_22","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_22","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_22","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_22","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#typeadapter","text":"class TypeAdapter ( / , * args , ** kwargs ) View Source class TypeAdapter : \"\"\"TypeAdapter maps between Artigraph types and a foreign type system.\"\"\" key : ClassVar [ str ] = class_name () artigraph : ClassVar [ type[Type ] ] # The internal Artigraph Type system : ClassVar [ Any ] # The external system ' s type priority : ClassVar [ int ] = 0 # Set the priority of this mapping . Higher is better . @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : raise NotImplementedError () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : raise NotImplementedError () @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : raise NotImplementedError ()","title":"TypeAdapter"},{"location":"reference/arti/types/#descendants_2","text":"arti.types._ScalarClassTypeAdapter arti.types.python.PyValueContainer arti.types.python.PyLiteral arti.types.python.PyMap arti.types.python.PyOptional arti.types.python.PyStruct arti.types.bigquery._BigQueryTypeAdapter arti.types.bigquery.ListFieldTypeAdapter arti.types.bigquery.TableTypeAdapter arti.types.numpy.ArrayAdapter arti.types.pandas.SeriesAdapter arti.types.pandas.DataFrameAdapter arti.types.pyarrow._PyarrowTypeAdapter arti.types.pydantic.BaseModelAdapter","title":"Descendants"},{"location":"reference/arti/types/#class-variables_23","text":"key priority","title":"Class variables"},{"location":"reference/arti/types/#static-methods_23","text":"","title":"Static methods"},{"location":"reference/arti/types/#matches_artigraph","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/#matches_system","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : raise NotImplementedError ()","title":"matches_system"},{"location":"reference/arti/types/#to_artigraph","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : raise NotImplementedError ()","title":"to_artigraph"},{"location":"reference/arti/types/#to_system","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : raise NotImplementedError ()","title":"to_system"},{"location":"reference/arti/types/#typesystem","text":"class TypeSystem ( __pydantic_self__ , ** data : Any ) View Source class TypeSystem ( Model ) : key : str extends : tuple [ TypeSystem, ... ] = () # NOTE : Use a NoCopyDict to avoid copies of the registry . Otherwise , TypeSystems that extend # this TypeSystem will only see the adapters registered * as of initialization * ( as pydantic # would deepcopy the TypeSystems in the ` extends ` argument ). _adapter_by_key : NoCopyDict [ str, type[TypeAdapter ] ] = PrivateAttr ( default_factory = NoCopyDict ) def register_adapter ( self , adapter : type [ TypeAdapter ] ) -> type [ TypeAdapter ] : return register ( self . _adapter_by_key , adapter . key , adapter ) @property def _priority_sorted_adapters ( self ) -> list [ type[TypeAdapter ] ]: return sorted ( self . _adapter_by_key . values (), key = attrgetter ( \"priority\" ), reverse = True ) def to_artigraph ( self , type_ : Any , * , hints : dict [ str, Any ] , root_type_system : Optional [ TypeSystem ] = None ) -> Type : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ) : return adapter . to_artigraph ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_artigraph ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No {root_type_system} adapter for system type: {type_}.\" ) def to_system ( self , type_ : Type , * , hints : dict [ str, Any ] , root_type_system : Optional [ TypeSystem ] = None ) -> Any : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ) : return adapter . to_system ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_system ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No {root_type_system} adapter for Artigraph type: {type_}.\" )","title":"TypeSystem"},{"location":"reference/arti/types/#ancestors-in-mro_23","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_24","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_24","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_23","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_23","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_23","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_23","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_23","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_23","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_23","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_23","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_23","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_23","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/types/#methods_23","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_23","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_23","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_23","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#register_adapter","text":"def register_adapter ( self , adapter : 'type[TypeAdapter]' ) -> 'type[TypeAdapter]' View Source def register_adapter ( self , adapter : type [ TypeAdapter ] ) -> type [ TypeAdapter ] : return register ( self . _adapter_by_key , adapter . key , adapter )","title":"register_adapter"},{"location":"reference/arti/types/#to_artigraph_1","text":"def to_artigraph ( self , type_ : 'Any' , * , hints : 'dict[str, Any]' , root_type_system : 'Optional[TypeSystem]' = None ) -> 'Type' View Source def to_artigraph ( self , type_ : Any , * , hints : dict [ str, Any ] , root_type_system : Optional [ TypeSystem ] = None ) -> Type : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ) : return adapter . to_artigraph ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_artigraph ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No {root_type_system} adapter for system type: {type_}.\" )","title":"to_artigraph"},{"location":"reference/arti/types/#to_system_1","text":"def to_system ( self , type_ : 'Type' , * , hints : 'dict[str, Any]' , root_type_system : 'Optional[TypeSystem]' = None ) -> 'Any' View Source def to_system ( self , type_ : Type , * , hints : dict [ str, Any ] , root_type_system : Optional [ TypeSystem ] = None ) -> Any : root_type_system = root_type_system or self for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ) : return adapter . to_system ( type_ , hints = hints , type_system = root_type_system ) for type_system in self . extends : try : return type_system . to_system ( type_ , hints = hints , root_type_system = root_type_system ) except NotImplementedError : pass raise NotImplementedError ( f \"No {root_type_system} adapter for Artigraph type: {type_}.\" )","title":"to_system"},{"location":"reference/arti/types/#uint16","text":"class UInt16 ( __pydantic_self__ , ** data : Any ) View Source class UInt16 ( _Int ): pass","title":"UInt16"},{"location":"reference/arti/types/#ancestors-in-mro_24","text":"arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_25","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_25","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_24","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_24","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_24","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_24","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_24","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_24","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_24","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_24","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_24","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_24","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_24","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_24","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_24","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_24","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#uint32","text":"class UInt32 ( __pydantic_self__ , ** data : Any ) View Source class UInt32 ( _Int ): pass","title":"UInt32"},{"location":"reference/arti/types/#ancestors-in-mro_25","text":"arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_26","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_26","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_25","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_25","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_25","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_25","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_25","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_25","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_25","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_25","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_25","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_25","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_25","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_25","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_25","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_25","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#uint64","text":"class UInt64 ( __pydantic_self__ , ** data : Any ) View Source class UInt64 ( _Int ): pass","title":"UInt64"},{"location":"reference/arti/types/#ancestors-in-mro_26","text":"arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_27","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_27","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_26","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_26","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_26","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_26","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_26","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_26","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_26","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_26","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_26","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_26","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_26","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_26","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_26","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_26","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#uint8","text":"class UInt8 ( __pydantic_self__ , ** data : Any ) View Source class UInt8 ( _Int ): pass","title":"UInt8"},{"location":"reference/arti/types/#ancestors-in-mro_27","text":"arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_28","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_28","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_27","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_27","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_27","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_27","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_27","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_27","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_27","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_27","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_27","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_27","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_27","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_27","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_27","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_27","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/bigquery/","text":"Module arti.types.bigquery None None View Source from __future__ import annotations import warnings from copy import deepcopy from typing import Any from google.cloud import bigquery from google.cloud.bigquery.enums import SqlTypeNames from arti import Type , TypeAdapter , TypeSystem , types # The BigQuery types are enumerated in [1], but a few are not (yet) implemented: # - BIGNUMERIC # - INTERVAL # - JSON # - NUMERIC # # 1: https://github.com/googleapis/python-bigquery/blob/76d88fbb1316317a61fa1a63c101bc6f42f23af8/google/cloud/bigquery/enums.py#L252-L274 bigquery_type_system = TypeSystem ( key = \"bigquery\" ) class BIGQUERY_MODE : REQUIRED = \"REQUIRED\" NULLABLE = \"NULLABLE\" REPEATED = \"REPEATED\" # BigQuery Structs contain list[SchemaField], each with an embedded name. Artigraph Structs contain # dict[name, Type]. Therefore, converting a Type to a SchemaField requires the field name to be # passed in from higher up, which is handled via this key in the `hints`. BIGQUERY_HINT_FIELD_NAME = f \" { bigquery_type_system . key } .field_name\" def _create_schema_field ( field_type : str , type_ : Type , hints : dict [ str , Any ], ** kwargs : Any ) -> bigquery . SchemaField : # TODO: Support default values (which would need support in arti.Type) if type_ . description is not None : kwargs . setdefault ( \"description\" , type_ . description ) return bigquery . SchemaField ( name = hints . get ( BIGQUERY_HINT_FIELD_NAME , types . DEFAULT_ANONYMOUS_NAME ), field_type = field_type , mode = BIGQUERY_MODE . NULLABLE if type_ . nullable else BIGQUERY_MODE . REQUIRED , ** kwargs , ) class _BigQueryTypeAdapter ( TypeAdapter ): @classmethod def to_artigraph ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return cls . artigraph ( description = type_ . description , nullable = type_ . is_nullable ) @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , bigquery . SchemaField ) and type_ . field_type . upper () == cls . system # type: ignore[no-any-return] @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : return _create_schema_field ( cls . system , type_ , hints ) def _gen_adapter ( * , artigraph : type [ Type ], system : Any , priority : int = 0 ) -> type [ TypeAdapter ]: return bigquery_type_system . register_adapter ( type ( f \"BigQuery { system }{ artigraph } \" , ( _BigQueryTypeAdapter ,), { \"artigraph\" : artigraph , \"system\" : system , \"priority\" : priority }, ) ) _gen_adapter ( artigraph = types . Binary , system = SqlTypeNames . BYTES ) _gen_adapter ( artigraph = types . Boolean , system = SqlTypeNames . BOOL ) _gen_adapter ( artigraph = types . Date , system = SqlTypeNames . DATE ) _gen_adapter ( artigraph = types . Geography , system = SqlTypeNames . GEOGRAPHY ) _gen_adapter ( artigraph = types . String , system = SqlTypeNames . STRING ) # BQ only supports 64-bit ints and floats (aside from numerics), so round tripping results in eg: # arti Float16 -> bq FLOAT64 -> arti Float64 for _precision in ( 16 , 32 , 64 ): _gen_adapter ( artigraph = getattr ( types , f \"Float { _precision } \" ), system = SqlTypeNames . FLOAT64 , priority = _precision , ) for _precision in ( 8 , 16 , 32 , 64 ): _gen_adapter ( artigraph = getattr ( types , f \"Int { _precision } \" ), system = SqlTypeNames . INT64 , priority = _precision ) class _BaseTimeTypeAdapter ( _BigQueryTypeAdapter ): # BQ time precision is microsecond (https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#timestamp_type) precision = \"microsecond\" @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : assert issubclass ( cls . artigraph , types . _TimeMixin ) return cls . artigraph ( description = type_ . description , nullable = type_ . is_nullable , precision = cls . precision ) @bigquery_type_system . register_adapter class DateTimeTypeAdapter ( _BaseTimeTypeAdapter ): artigraph = types . DateTime system = SqlTypeNames . DATETIME @bigquery_type_system . register_adapter class TimeTypeAdapter ( _BaseTimeTypeAdapter ): artigraph = types . Time system = SqlTypeNames . TIME @bigquery_type_system . register_adapter class TimestampTypeAdapter ( _BaseTimeTypeAdapter ): artigraph = types . Timestamp system = SqlTypeNames . TIMESTAMP @bigquery_type_system . register_adapter class StructTypeAdapter ( _BigQueryTypeAdapter ): # See https://cloud.google.com/bigquery/docs/nested-repeated artigraph = types . Struct system = SqlTypeNames . STRUCT @classmethod def to_artigraph ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return cls . artigraph ( description = type_ . description , fields = { field . name : type_system . to_artigraph ( field , hints = hints ) for field in type_ . fields }, nullable = type_ . is_nullable , ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return _create_schema_field ( cls . system , type_ , hints , fields = [ type_system . to_system ( subtype , hints = hints | { BIGQUERY_HINT_FIELD_NAME : name }) for name , subtype in type_ . fields . items () ], ) @bigquery_type_system . register_adapter class ListFieldTypeAdapter ( TypeAdapter ): # See https://cloud.google.com/bigquery/docs/nested-repeated artigraph = types . List system = bigquery . SchemaField priority = int ( 1e9 ) # Bump the priority so we can catch all with `mode == \"REPEATED\"` @classmethod def to_artigraph ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : # Convert the REPEATED field to REQUIRED (BigQuery only supports non-nullable array # elements) for subsequent conversion by other TypeAdapters. element_type = deepcopy ( type_ ) element_type . _properties [ \"mode\" ] = BIGQUERY_MODE . REQUIRED return types . List ( description = type_ . description , element = type_system . to_artigraph ( element_type , hints = hints ), nullable = False , # Cannot be nullable ) @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , bigquery . SchemaField ) and type_ . mode == BIGQUERY_MODE . REPEATED # type: ignore[no-any-return] @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : # Collection is a subclass of List - but handled by a separate TypeAdapter return super () . matches_artigraph ( type_ , hints = hints ) and not isinstance ( type_ , types . Collection ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) if type_ . nullable : warnings . warn ( \"BigQuery doesn't support nullable arrays\" , stacklevel = 2 ) if type_ . element . nullable : warnings . warn ( \"BigQuery doesn't support nullable array elements\" , stacklevel = 2 ) type_ = type_ . copy ( update = { \"element\" : type_ . element . copy ( update = { \"nullable\" : False })}) if isinstance ( type_ . element , types . List ): raise ValueError ( \"BigQuery doesn't support nested arrays\" ) field = type_system . to_system ( type_ . element , hints = hints ) assert field . mode == BIGQUERY_MODE . REQUIRED field . _properties [ \"mode\" ] = BIGQUERY_MODE . REPEATED return field @bigquery_type_system . register_adapter class TableTypeAdapter ( TypeAdapter ): artigraph = types . Collection system = bigquery . Table priority = ListFieldTypeAdapter . priority + 1 @classmethod def to_artigraph ( cls , type_ : bigquery . Table , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : kwargs : dict [ str , Any ] = {} if type_ . time_partitioning : if type_ . time_partitioning . type_ != bigquery . TimePartitioningType . DAY : raise NotImplementedError ( f \"BigQuery time partitioning other than 'DAY' is not implemented (got ' { type_ . time_partitioning . type_ } ')\" ) kwargs [ \"partition_by\" ] = ( type_ . time_partitioning . field ,) if type_ . range_partitioning : raise NotImplementedError ( \"BigQuery integer range partitioning is not implemented\" ) if type_ . clustering_fields : kwargs [ \"cluster_by\" ] = tuple ( type_ . clustering_fields ) return cls . artigraph ( name = f \" { type_ . project } . { type_ . dataset_id } . { type_ . table_id } \" , element = type_system . to_artigraph ( bigquery . SchemaField ( types . DEFAULT_ANONYMOUS_NAME , SqlTypeNames . STRUCT , fields = type_ . schema , mode = \"REQUIRED\" , ), hints = hints , ), nullable = False , ** kwargs , ) @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , types . Struct ) name = type_ . name # Override invalid default (must be project/dataset qualified) if name == types . DEFAULT_ANONYMOUS_NAME : name = \"project.dataset.table\" table = cls . system ( name , schema = type_system . to_system ( type_ . element , hints = hints ) . fields ) partition , cluster = type_ . partition_by , type_ . cluster_by if partition : # BigQuery only supports a single partitioning field. We'll move the rest to the # beginning of the cluster_by. This shouldn't matter much anyway since, depending on the # Storage, we'll have separate tables for each unique composite key. head , * tail = partition if tail : cluster = ( * tail , * cluster ) if isinstance ( type_ . element . fields [ head ], ( types . Date , types . DateTime , types . Timestamp ) ): # TODO: Support other granularities than DAY table . time_partitioning = bigquery . TimePartitioning ( field = head , type_ = bigquery . TimePartitioningType . DAY ) table . require_partition_filter = True elif isinstance ( type_ . element . fields [ head ], types . _Int ): raise NotImplementedError ( \"BigQuery integer range partitioning is not implemented\" ) else : raise ValueError ( \"BigQuery only supports integer range or time partitioning\" ) if cluster : table . clustering_fields = cluster return table Variables BIGQUERY_HINT_FIELD_NAME bigquery_type_system Classes BIGQUERY_MODE class BIGQUERY_MODE ( / , * args , ** kwargs ) View Source class BIGQUERY_MODE: REQUIRED = \"REQUIRED\" NULLABLE = \"NULLABLE\" REPEATED = \"REPEATED\" Class variables NULLABLE REPEATED REQUIRED DateTimeTypeAdapter class DateTimeTypeAdapter ( / , * args , ** kwargs ) View Source @bigquery_type_system . register_adapter class DateTimeTypeAdapter ( _BaseTimeTypeAdapter ) : artigraph = types . DateTime system = SqlTypeNames . DATETIME Ancestors (in MRO) arti.types.bigquery._BaseTimeTypeAdapter arti.types.bigquery._BigQueryTypeAdapter arti.types.TypeAdapter Class variables artigraph key precision priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'bigquery.SchemaField' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , bigquery . SchemaField ) and type_ . field_type . upper () == cls . system # type : ignore [ no-any-return ] to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : assert issubclass ( cls . artigraph , types . _TimeMixin ) return cls . artigraph ( description = type_ . description , nullable = type_ . is_nullable , precision = cls . precision ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return _create_schema_field ( cls . system , type_ , hints ) ListFieldTypeAdapter class ListFieldTypeAdapter ( / , * args , ** kwargs ) View Source @bigquery_type_system . register_adapter class ListFieldTypeAdapter ( TypeAdapter ) : # See https : // cloud . google . com / bigquery / docs / nested - repeated artigraph = types . List system = bigquery . SchemaField priority = int ( 1e9 ) # Bump the priority so we can catch all with ` mode == \"REPEATED\" ` @classmethod def to_artigraph ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : # Convert the REPEATED field to REQUIRED ( BigQuery only supports non - nullable array # elements ) for subsequent conversion by other TypeAdapters . element_type = deepcopy ( type_ ) element_type . _properties [ \"mode\" ] = BIGQUERY_MODE . REQUIRED return types . List ( description = type_ . description , element = type_system . to_artigraph ( element_type , hints = hints ), nullable = False , # Cannot be nullable ) @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , bigquery . SchemaField ) and type_ . mode == BIGQUERY_MODE . REPEATED # type : ignore [ no-any-return ] @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : # Collection is a subclass of List - but handled by a separate TypeAdapter return super (). matches_artigraph ( type_ , hints = hints ) and not isinstance ( type_ , types . Collection ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) if type_ . nullable : warnings . warn ( \"BigQuery doesn't support nullable arrays\" , stacklevel = 2 ) if type_ . element . nullable : warnings . warn ( \"BigQuery doesn't support nullable array elements\" , stacklevel = 2 ) type_ = type_ . copy ( update = { \"element\" : type_ . element . copy ( update = { \"nullable\" : False } ) } ) if isinstance ( type_ . element , types . List ) : raise ValueError ( \"BigQuery doesn't support nested arrays\" ) field = type_system . to_system ( type_ . element , hints = hints ) assert field . mode == BIGQUERY_MODE . REQUIRED field . _properties [ \"mode\" ] = BIGQUERY_MODE . REPEATED return field Ancestors (in MRO) arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : # Collection is a subclass of List - but handled by a separate TypeAdapter return super (). matches_artigraph ( type_ , hints = hints ) and not isinstance ( type_ , types . Collection ) matches_system def matches_system ( type_ : 'bigquery.SchemaField' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , bigquery . SchemaField ) and type_ . mode == BIGQUERY_MODE . REPEATED # type : ignore [ no-any-return ] to_artigraph def to_artigraph ( type_ : 'bigquery.SchemaField' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : # Convert the REPEATED field to REQUIRED ( BigQuery only supports non - nullable array # elements ) for subsequent conversion by other TypeAdapters . element_type = deepcopy ( type_ ) element_type . _properties [ \"mode\" ] = BIGQUERY_MODE . REQUIRED return types . List ( description = type_ . description , element = type_system . to_artigraph ( element_type , hints = hints ), nullable = False , # Cannot be nullable ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) if type_ . nullable : warnings . warn ( \"BigQuery doesn't support nullable arrays\" , stacklevel = 2 ) if type_ . element . nullable : warnings . warn ( \"BigQuery doesn't support nullable array elements\" , stacklevel = 2 ) type_ = type_ . copy ( update = { \"element\" : type_ . element . copy ( update = { \"nullable\" : False } ) } ) if isinstance ( type_ . element , types . List ) : raise ValueError ( \"BigQuery doesn't support nested arrays\" ) field = type_system . to_system ( type_ . element , hints = hints ) assert field . mode == BIGQUERY_MODE . REQUIRED field . _properties [ \"mode\" ] = BIGQUERY_MODE . REPEATED return field StructTypeAdapter class StructTypeAdapter ( / , * args , ** kwargs ) View Source @bigquery_type_system . register_adapter class StructTypeAdapter ( _BigQueryTypeAdapter ) : # See https : // cloud . google . com / bigquery / docs / nested - repeated artigraph = types . Struct system = SqlTypeNames . STRUCT @classmethod def to_artigraph ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( description = type_ . description , fields = { field . name : type_system . to_artigraph ( field , hints = hints ) for field in type_ . fields } , nullable = type_ . is_nullable , ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return _create_schema_field ( cls . system , type_ , hints , fields =[ type_system.to_system(subtype, hints=hints | {BIGQUERY_HINT_FIELD_NAME: name}) for name, subtype in type_.fields.items() ] , ) Ancestors (in MRO) arti.types.bigquery._BigQueryTypeAdapter arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'bigquery.SchemaField' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , bigquery . SchemaField ) and type_ . field_type . upper () == cls . system # type : ignore [ no-any-return ] to_artigraph def to_artigraph ( type_ : 'bigquery.SchemaField' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( description = type_ . description , fields = { field . name : type_system . to_artigraph ( field , hints = hints ) for field in type_ . fields } , nullable = type_ . is_nullable , ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return _create_schema_field ( cls . system , type_ , hints , fields =[ type_system.to_system(subtype, hints=hints | {BIGQUERY_HINT_FIELD_NAME: name}) for name, subtype in type_.fields.items() ] , ) TableTypeAdapter class TableTypeAdapter ( / , * args , ** kwargs ) View Source @bigquery_type_system . register_adapter class TableTypeAdapter ( TypeAdapter ) : artigraph = types . Collection system = bigquery . Table priority = ListFieldTypeAdapter . priority + 1 @classmethod def to_artigraph ( cls , type_ : bigquery . Table , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : kwargs : dict [ str, Any ] = {} if type_ . time_partitioning : if type_ . time_partitioning . type_ != bigquery . TimePartitioningType . DAY : raise NotImplementedError ( f \"BigQuery time partitioning other than 'DAY' is not implemented (got '{type_.time_partitioning.type_}')\" ) kwargs [ \"partition_by\" ] = ( type_ . time_partitioning . field ,) if type_ . range_partitioning : raise NotImplementedError ( \"BigQuery integer range partitioning is not implemented\" ) if type_ . clustering_fields : kwargs [ \"cluster_by\" ] = tuple ( type_ . clustering_fields ) return cls . artigraph ( name = f \"{type_.project}.{type_.dataset_id}.{type_.table_id}\" , element = type_system . to_artigraph ( bigquery . SchemaField ( types . DEFAULT_ANONYMOUS_NAME , SqlTypeNames . STRUCT , fields = type_ . schema , mode = \"REQUIRED\" , ), hints = hints , ), nullable = False , ** kwargs , ) @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , types . Struct ) name = type_ . name # Override invalid default ( must be project / dataset qualified ) if name == types . DEFAULT_ANONYMOUS_NAME : name = \"project.dataset.table\" table = cls . system ( name , schema = type_system . to_system ( type_ . element , hints = hints ). fields ) partition , cluster = type_ . partition_by , type_ . cluster_by if partition : # BigQuery only supports a single partitioning field . We 'll move the rest to the # beginning of the cluster_by. This shouldn' t matter much anyway since , depending on the # Storage , we ' ll have separate tables for each unique composite key . head , * tail = partition if tail : cluster = ( * tail , * cluster ) if isinstance ( type_ . element . fields [ head ] , ( types . Date , types . DateTime , types . Timestamp ) ) : # TODO : Support other granularities than DAY table . time_partitioning = bigquery . TimePartitioning ( field = head , type_ = bigquery . TimePartitioningType . DAY ) table . require_partition_filter = True elif isinstance ( type_ . element . fields [ head ] , types . _Int ) : raise NotImplementedError ( \"BigQuery integer range partitioning is not implemented\" ) else : raise ValueError ( \"BigQuery only supports integer range or time partitioning\" ) if cluster : table . clustering_fields = cluster return table Ancestors (in MRO) arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'bigquery.SchemaField' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . system ) to_artigraph def to_artigraph ( type_ : 'bigquery.Table' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : bigquery . Table , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : kwargs : dict [ str, Any ] = {} if type_ . time_partitioning : if type_ . time_partitioning . type_ != bigquery . TimePartitioningType . DAY : raise NotImplementedError ( f \"BigQuery time partitioning other than 'DAY' is not implemented (got '{type_.time_partitioning.type_}')\" ) kwargs [ \"partition_by\" ] = ( type_ . time_partitioning . field ,) if type_ . range_partitioning : raise NotImplementedError ( \"BigQuery integer range partitioning is not implemented\" ) if type_ . clustering_fields : kwargs [ \"cluster_by\" ] = tuple ( type_ . clustering_fields ) return cls . artigraph ( name = f \"{type_.project}.{type_.dataset_id}.{type_.table_id}\" , element = type_system . to_artigraph ( bigquery . SchemaField ( types . DEFAULT_ANONYMOUS_NAME , SqlTypeNames . STRUCT , fields = type_ . schema , mode = \"REQUIRED\" , ), hints = hints , ), nullable = False , ** kwargs , ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , types . Struct ) name = type_ . name # Override invalid default ( must be project / dataset qualified ) if name == types . DEFAULT_ANONYMOUS_NAME : name = \"project.dataset.table\" table = cls . system ( name , schema = type_system . to_system ( type_ . element , hints = hints ). fields ) partition , cluster = type_ . partition_by , type_ . cluster_by if partition : # BigQuery only supports a single partitioning field . We 'll move the rest to the # beginning of the cluster_by. This shouldn' t matter much anyway since , depending on the # Storage , we ' ll have separate tables for each unique composite key . head , * tail = partition if tail : cluster = ( * tail , * cluster ) if isinstance ( type_ . element . fields [ head ] , ( types . Date , types . DateTime , types . Timestamp ) ) : # TODO : Support other granularities than DAY table . time_partitioning = bigquery . TimePartitioning ( field = head , type_ = bigquery . TimePartitioningType . DAY ) table . require_partition_filter = True elif isinstance ( type_ . element . fields [ head ] , types . _Int ) : raise NotImplementedError ( \"BigQuery integer range partitioning is not implemented\" ) else : raise ValueError ( \"BigQuery only supports integer range or time partitioning\" ) if cluster : table . clustering_fields = cluster return table TimeTypeAdapter class TimeTypeAdapter ( / , * args , ** kwargs ) View Source @bigquery_type_system . register_adapter class TimeTypeAdapter ( _BaseTimeTypeAdapter ) : artigraph = types . Time system = SqlTypeNames . TIME Ancestors (in MRO) arti.types.bigquery._BaseTimeTypeAdapter arti.types.bigquery._BigQueryTypeAdapter arti.types.TypeAdapter Class variables artigraph key precision priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'bigquery.SchemaField' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , bigquery . SchemaField ) and type_ . field_type . upper () == cls . system # type : ignore [ no-any-return ] to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : assert issubclass ( cls . artigraph , types . _TimeMixin ) return cls . artigraph ( description = type_ . description , nullable = type_ . is_nullable , precision = cls . precision ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return _create_schema_field ( cls . system , type_ , hints ) TimestampTypeAdapter class TimestampTypeAdapter ( / , * args , ** kwargs ) View Source @bigquery_type_system . register_adapter class TimestampTypeAdapter ( _BaseTimeTypeAdapter ) : artigraph = types . Timestamp system = SqlTypeNames . TIMESTAMP Ancestors (in MRO) arti.types.bigquery._BaseTimeTypeAdapter arti.types.bigquery._BigQueryTypeAdapter arti.types.TypeAdapter Class variables artigraph key precision priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'bigquery.SchemaField' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , bigquery . SchemaField ) and type_ . field_type . upper () == cls . system # type : ignore [ no-any-return ] to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : assert issubclass ( cls . artigraph , types . _TimeMixin ) return cls . artigraph ( description = type_ . description , nullable = type_ . is_nullable , precision = cls . precision ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return _create_schema_field ( cls . system , type_ , hints )","title":"Bigquery"},{"location":"reference/arti/types/bigquery/#module-artitypesbigquery","text":"None None View Source from __future__ import annotations import warnings from copy import deepcopy from typing import Any from google.cloud import bigquery from google.cloud.bigquery.enums import SqlTypeNames from arti import Type , TypeAdapter , TypeSystem , types # The BigQuery types are enumerated in [1], but a few are not (yet) implemented: # - BIGNUMERIC # - INTERVAL # - JSON # - NUMERIC # # 1: https://github.com/googleapis/python-bigquery/blob/76d88fbb1316317a61fa1a63c101bc6f42f23af8/google/cloud/bigquery/enums.py#L252-L274 bigquery_type_system = TypeSystem ( key = \"bigquery\" ) class BIGQUERY_MODE : REQUIRED = \"REQUIRED\" NULLABLE = \"NULLABLE\" REPEATED = \"REPEATED\" # BigQuery Structs contain list[SchemaField], each with an embedded name. Artigraph Structs contain # dict[name, Type]. Therefore, converting a Type to a SchemaField requires the field name to be # passed in from higher up, which is handled via this key in the `hints`. BIGQUERY_HINT_FIELD_NAME = f \" { bigquery_type_system . key } .field_name\" def _create_schema_field ( field_type : str , type_ : Type , hints : dict [ str , Any ], ** kwargs : Any ) -> bigquery . SchemaField : # TODO: Support default values (which would need support in arti.Type) if type_ . description is not None : kwargs . setdefault ( \"description\" , type_ . description ) return bigquery . SchemaField ( name = hints . get ( BIGQUERY_HINT_FIELD_NAME , types . DEFAULT_ANONYMOUS_NAME ), field_type = field_type , mode = BIGQUERY_MODE . NULLABLE if type_ . nullable else BIGQUERY_MODE . REQUIRED , ** kwargs , ) class _BigQueryTypeAdapter ( TypeAdapter ): @classmethod def to_artigraph ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return cls . artigraph ( description = type_ . description , nullable = type_ . is_nullable ) @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , bigquery . SchemaField ) and type_ . field_type . upper () == cls . system # type: ignore[no-any-return] @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : return _create_schema_field ( cls . system , type_ , hints ) def _gen_adapter ( * , artigraph : type [ Type ], system : Any , priority : int = 0 ) -> type [ TypeAdapter ]: return bigquery_type_system . register_adapter ( type ( f \"BigQuery { system }{ artigraph } \" , ( _BigQueryTypeAdapter ,), { \"artigraph\" : artigraph , \"system\" : system , \"priority\" : priority }, ) ) _gen_adapter ( artigraph = types . Binary , system = SqlTypeNames . BYTES ) _gen_adapter ( artigraph = types . Boolean , system = SqlTypeNames . BOOL ) _gen_adapter ( artigraph = types . Date , system = SqlTypeNames . DATE ) _gen_adapter ( artigraph = types . Geography , system = SqlTypeNames . GEOGRAPHY ) _gen_adapter ( artigraph = types . String , system = SqlTypeNames . STRING ) # BQ only supports 64-bit ints and floats (aside from numerics), so round tripping results in eg: # arti Float16 -> bq FLOAT64 -> arti Float64 for _precision in ( 16 , 32 , 64 ): _gen_adapter ( artigraph = getattr ( types , f \"Float { _precision } \" ), system = SqlTypeNames . FLOAT64 , priority = _precision , ) for _precision in ( 8 , 16 , 32 , 64 ): _gen_adapter ( artigraph = getattr ( types , f \"Int { _precision } \" ), system = SqlTypeNames . INT64 , priority = _precision ) class _BaseTimeTypeAdapter ( _BigQueryTypeAdapter ): # BQ time precision is microsecond (https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#timestamp_type) precision = \"microsecond\" @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : assert issubclass ( cls . artigraph , types . _TimeMixin ) return cls . artigraph ( description = type_ . description , nullable = type_ . is_nullable , precision = cls . precision ) @bigquery_type_system . register_adapter class DateTimeTypeAdapter ( _BaseTimeTypeAdapter ): artigraph = types . DateTime system = SqlTypeNames . DATETIME @bigquery_type_system . register_adapter class TimeTypeAdapter ( _BaseTimeTypeAdapter ): artigraph = types . Time system = SqlTypeNames . TIME @bigquery_type_system . register_adapter class TimestampTypeAdapter ( _BaseTimeTypeAdapter ): artigraph = types . Timestamp system = SqlTypeNames . TIMESTAMP @bigquery_type_system . register_adapter class StructTypeAdapter ( _BigQueryTypeAdapter ): # See https://cloud.google.com/bigquery/docs/nested-repeated artigraph = types . Struct system = SqlTypeNames . STRUCT @classmethod def to_artigraph ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return cls . artigraph ( description = type_ . description , fields = { field . name : type_system . to_artigraph ( field , hints = hints ) for field in type_ . fields }, nullable = type_ . is_nullable , ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return _create_schema_field ( cls . system , type_ , hints , fields = [ type_system . to_system ( subtype , hints = hints | { BIGQUERY_HINT_FIELD_NAME : name }) for name , subtype in type_ . fields . items () ], ) @bigquery_type_system . register_adapter class ListFieldTypeAdapter ( TypeAdapter ): # See https://cloud.google.com/bigquery/docs/nested-repeated artigraph = types . List system = bigquery . SchemaField priority = int ( 1e9 ) # Bump the priority so we can catch all with `mode == \"REPEATED\"` @classmethod def to_artigraph ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : # Convert the REPEATED field to REQUIRED (BigQuery only supports non-nullable array # elements) for subsequent conversion by other TypeAdapters. element_type = deepcopy ( type_ ) element_type . _properties [ \"mode\" ] = BIGQUERY_MODE . REQUIRED return types . List ( description = type_ . description , element = type_system . to_artigraph ( element_type , hints = hints ), nullable = False , # Cannot be nullable ) @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , bigquery . SchemaField ) and type_ . mode == BIGQUERY_MODE . REPEATED # type: ignore[no-any-return] @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : # Collection is a subclass of List - but handled by a separate TypeAdapter return super () . matches_artigraph ( type_ , hints = hints ) and not isinstance ( type_ , types . Collection ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) if type_ . nullable : warnings . warn ( \"BigQuery doesn't support nullable arrays\" , stacklevel = 2 ) if type_ . element . nullable : warnings . warn ( \"BigQuery doesn't support nullable array elements\" , stacklevel = 2 ) type_ = type_ . copy ( update = { \"element\" : type_ . element . copy ( update = { \"nullable\" : False })}) if isinstance ( type_ . element , types . List ): raise ValueError ( \"BigQuery doesn't support nested arrays\" ) field = type_system . to_system ( type_ . element , hints = hints ) assert field . mode == BIGQUERY_MODE . REQUIRED field . _properties [ \"mode\" ] = BIGQUERY_MODE . REPEATED return field @bigquery_type_system . register_adapter class TableTypeAdapter ( TypeAdapter ): artigraph = types . Collection system = bigquery . Table priority = ListFieldTypeAdapter . priority + 1 @classmethod def to_artigraph ( cls , type_ : bigquery . Table , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : kwargs : dict [ str , Any ] = {} if type_ . time_partitioning : if type_ . time_partitioning . type_ != bigquery . TimePartitioningType . DAY : raise NotImplementedError ( f \"BigQuery time partitioning other than 'DAY' is not implemented (got ' { type_ . time_partitioning . type_ } ')\" ) kwargs [ \"partition_by\" ] = ( type_ . time_partitioning . field ,) if type_ . range_partitioning : raise NotImplementedError ( \"BigQuery integer range partitioning is not implemented\" ) if type_ . clustering_fields : kwargs [ \"cluster_by\" ] = tuple ( type_ . clustering_fields ) return cls . artigraph ( name = f \" { type_ . project } . { type_ . dataset_id } . { type_ . table_id } \" , element = type_system . to_artigraph ( bigquery . SchemaField ( types . DEFAULT_ANONYMOUS_NAME , SqlTypeNames . STRUCT , fields = type_ . schema , mode = \"REQUIRED\" , ), hints = hints , ), nullable = False , ** kwargs , ) @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , types . Struct ) name = type_ . name # Override invalid default (must be project/dataset qualified) if name == types . DEFAULT_ANONYMOUS_NAME : name = \"project.dataset.table\" table = cls . system ( name , schema = type_system . to_system ( type_ . element , hints = hints ) . fields ) partition , cluster = type_ . partition_by , type_ . cluster_by if partition : # BigQuery only supports a single partitioning field. We'll move the rest to the # beginning of the cluster_by. This shouldn't matter much anyway since, depending on the # Storage, we'll have separate tables for each unique composite key. head , * tail = partition if tail : cluster = ( * tail , * cluster ) if isinstance ( type_ . element . fields [ head ], ( types . Date , types . DateTime , types . Timestamp ) ): # TODO: Support other granularities than DAY table . time_partitioning = bigquery . TimePartitioning ( field = head , type_ = bigquery . TimePartitioningType . DAY ) table . require_partition_filter = True elif isinstance ( type_ . element . fields [ head ], types . _Int ): raise NotImplementedError ( \"BigQuery integer range partitioning is not implemented\" ) else : raise ValueError ( \"BigQuery only supports integer range or time partitioning\" ) if cluster : table . clustering_fields = cluster return table","title":"Module arti.types.bigquery"},{"location":"reference/arti/types/bigquery/#variables","text":"BIGQUERY_HINT_FIELD_NAME bigquery_type_system","title":"Variables"},{"location":"reference/arti/types/bigquery/#classes","text":"","title":"Classes"},{"location":"reference/arti/types/bigquery/#bigquery_mode","text":"class BIGQUERY_MODE ( / , * args , ** kwargs ) View Source class BIGQUERY_MODE: REQUIRED = \"REQUIRED\" NULLABLE = \"NULLABLE\" REPEATED = \"REPEATED\"","title":"BIGQUERY_MODE"},{"location":"reference/arti/types/bigquery/#class-variables","text":"NULLABLE REPEATED REQUIRED","title":"Class variables"},{"location":"reference/arti/types/bigquery/#datetimetypeadapter","text":"class DateTimeTypeAdapter ( / , * args , ** kwargs ) View Source @bigquery_type_system . register_adapter class DateTimeTypeAdapter ( _BaseTimeTypeAdapter ) : artigraph = types . DateTime system = SqlTypeNames . DATETIME","title":"DateTimeTypeAdapter"},{"location":"reference/arti/types/bigquery/#ancestors-in-mro","text":"arti.types.bigquery._BaseTimeTypeAdapter arti.types.bigquery._BigQueryTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/bigquery/#class-variables_1","text":"artigraph key precision priority system","title":"Class variables"},{"location":"reference/arti/types/bigquery/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/types/bigquery/#matches_artigraph","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/bigquery/#matches_system","text":"def matches_system ( type_ : 'bigquery.SchemaField' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , bigquery . SchemaField ) and type_ . field_type . upper () == cls . system # type : ignore [ no-any-return ]","title":"matches_system"},{"location":"reference/arti/types/bigquery/#to_artigraph","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : assert issubclass ( cls . artigraph , types . _TimeMixin ) return cls . artigraph ( description = type_ . description , nullable = type_ . is_nullable , precision = cls . precision )","title":"to_artigraph"},{"location":"reference/arti/types/bigquery/#to_system","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return _create_schema_field ( cls . system , type_ , hints )","title":"to_system"},{"location":"reference/arti/types/bigquery/#listfieldtypeadapter","text":"class ListFieldTypeAdapter ( / , * args , ** kwargs ) View Source @bigquery_type_system . register_adapter class ListFieldTypeAdapter ( TypeAdapter ) : # See https : // cloud . google . com / bigquery / docs / nested - repeated artigraph = types . List system = bigquery . SchemaField priority = int ( 1e9 ) # Bump the priority so we can catch all with ` mode == \"REPEATED\" ` @classmethod def to_artigraph ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : # Convert the REPEATED field to REQUIRED ( BigQuery only supports non - nullable array # elements ) for subsequent conversion by other TypeAdapters . element_type = deepcopy ( type_ ) element_type . _properties [ \"mode\" ] = BIGQUERY_MODE . REQUIRED return types . List ( description = type_ . description , element = type_system . to_artigraph ( element_type , hints = hints ), nullable = False , # Cannot be nullable ) @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , bigquery . SchemaField ) and type_ . mode == BIGQUERY_MODE . REPEATED # type : ignore [ no-any-return ] @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : # Collection is a subclass of List - but handled by a separate TypeAdapter return super (). matches_artigraph ( type_ , hints = hints ) and not isinstance ( type_ , types . Collection ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) if type_ . nullable : warnings . warn ( \"BigQuery doesn't support nullable arrays\" , stacklevel = 2 ) if type_ . element . nullable : warnings . warn ( \"BigQuery doesn't support nullable array elements\" , stacklevel = 2 ) type_ = type_ . copy ( update = { \"element\" : type_ . element . copy ( update = { \"nullable\" : False } ) } ) if isinstance ( type_ . element , types . List ) : raise ValueError ( \"BigQuery doesn't support nested arrays\" ) field = type_system . to_system ( type_ . element , hints = hints ) assert field . mode == BIGQUERY_MODE . REQUIRED field . _properties [ \"mode\" ] = BIGQUERY_MODE . REPEATED return field","title":"ListFieldTypeAdapter"},{"location":"reference/arti/types/bigquery/#ancestors-in-mro_1","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/bigquery/#class-variables_2","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/bigquery/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/types/bigquery/#matches_artigraph_1","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : # Collection is a subclass of List - but handled by a separate TypeAdapter return super (). matches_artigraph ( type_ , hints = hints ) and not isinstance ( type_ , types . Collection )","title":"matches_artigraph"},{"location":"reference/arti/types/bigquery/#matches_system_1","text":"def matches_system ( type_ : 'bigquery.SchemaField' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , bigquery . SchemaField ) and type_ . mode == BIGQUERY_MODE . REPEATED # type : ignore [ no-any-return ]","title":"matches_system"},{"location":"reference/arti/types/bigquery/#to_artigraph_1","text":"def to_artigraph ( type_ : 'bigquery.SchemaField' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : # Convert the REPEATED field to REQUIRED ( BigQuery only supports non - nullable array # elements ) for subsequent conversion by other TypeAdapters . element_type = deepcopy ( type_ ) element_type . _properties [ \"mode\" ] = BIGQUERY_MODE . REQUIRED return types . List ( description = type_ . description , element = type_system . to_artigraph ( element_type , hints = hints ), nullable = False , # Cannot be nullable )","title":"to_artigraph"},{"location":"reference/arti/types/bigquery/#to_system_1","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) if type_ . nullable : warnings . warn ( \"BigQuery doesn't support nullable arrays\" , stacklevel = 2 ) if type_ . element . nullable : warnings . warn ( \"BigQuery doesn't support nullable array elements\" , stacklevel = 2 ) type_ = type_ . copy ( update = { \"element\" : type_ . element . copy ( update = { \"nullable\" : False } ) } ) if isinstance ( type_ . element , types . List ) : raise ValueError ( \"BigQuery doesn't support nested arrays\" ) field = type_system . to_system ( type_ . element , hints = hints ) assert field . mode == BIGQUERY_MODE . REQUIRED field . _properties [ \"mode\" ] = BIGQUERY_MODE . REPEATED return field","title":"to_system"},{"location":"reference/arti/types/bigquery/#structtypeadapter","text":"class StructTypeAdapter ( / , * args , ** kwargs ) View Source @bigquery_type_system . register_adapter class StructTypeAdapter ( _BigQueryTypeAdapter ) : # See https : // cloud . google . com / bigquery / docs / nested - repeated artigraph = types . Struct system = SqlTypeNames . STRUCT @classmethod def to_artigraph ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( description = type_ . description , fields = { field . name : type_system . to_artigraph ( field , hints = hints ) for field in type_ . fields } , nullable = type_ . is_nullable , ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return _create_schema_field ( cls . system , type_ , hints , fields =[ type_system.to_system(subtype, hints=hints | {BIGQUERY_HINT_FIELD_NAME: name}) for name, subtype in type_.fields.items() ] , )","title":"StructTypeAdapter"},{"location":"reference/arti/types/bigquery/#ancestors-in-mro_2","text":"arti.types.bigquery._BigQueryTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/bigquery/#class-variables_3","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/bigquery/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/arti/types/bigquery/#matches_artigraph_2","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/bigquery/#matches_system_2","text":"def matches_system ( type_ : 'bigquery.SchemaField' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , bigquery . SchemaField ) and type_ . field_type . upper () == cls . system # type : ignore [ no-any-return ]","title":"matches_system"},{"location":"reference/arti/types/bigquery/#to_artigraph_2","text":"def to_artigraph ( type_ : 'bigquery.SchemaField' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( description = type_ . description , fields = { field . name : type_system . to_artigraph ( field , hints = hints ) for field in type_ . fields } , nullable = type_ . is_nullable , )","title":"to_artigraph"},{"location":"reference/arti/types/bigquery/#to_system_2","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return _create_schema_field ( cls . system , type_ , hints , fields =[ type_system.to_system(subtype, hints=hints | {BIGQUERY_HINT_FIELD_NAME: name}) for name, subtype in type_.fields.items() ] , )","title":"to_system"},{"location":"reference/arti/types/bigquery/#tabletypeadapter","text":"class TableTypeAdapter ( / , * args , ** kwargs ) View Source @bigquery_type_system . register_adapter class TableTypeAdapter ( TypeAdapter ) : artigraph = types . Collection system = bigquery . Table priority = ListFieldTypeAdapter . priority + 1 @classmethod def to_artigraph ( cls , type_ : bigquery . Table , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : kwargs : dict [ str, Any ] = {} if type_ . time_partitioning : if type_ . time_partitioning . type_ != bigquery . TimePartitioningType . DAY : raise NotImplementedError ( f \"BigQuery time partitioning other than 'DAY' is not implemented (got '{type_.time_partitioning.type_}')\" ) kwargs [ \"partition_by\" ] = ( type_ . time_partitioning . field ,) if type_ . range_partitioning : raise NotImplementedError ( \"BigQuery integer range partitioning is not implemented\" ) if type_ . clustering_fields : kwargs [ \"cluster_by\" ] = tuple ( type_ . clustering_fields ) return cls . artigraph ( name = f \"{type_.project}.{type_.dataset_id}.{type_.table_id}\" , element = type_system . to_artigraph ( bigquery . SchemaField ( types . DEFAULT_ANONYMOUS_NAME , SqlTypeNames . STRUCT , fields = type_ . schema , mode = \"REQUIRED\" , ), hints = hints , ), nullable = False , ** kwargs , ) @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , types . Struct ) name = type_ . name # Override invalid default ( must be project / dataset qualified ) if name == types . DEFAULT_ANONYMOUS_NAME : name = \"project.dataset.table\" table = cls . system ( name , schema = type_system . to_system ( type_ . element , hints = hints ). fields ) partition , cluster = type_ . partition_by , type_ . cluster_by if partition : # BigQuery only supports a single partitioning field . We 'll move the rest to the # beginning of the cluster_by. This shouldn' t matter much anyway since , depending on the # Storage , we ' ll have separate tables for each unique composite key . head , * tail = partition if tail : cluster = ( * tail , * cluster ) if isinstance ( type_ . element . fields [ head ] , ( types . Date , types . DateTime , types . Timestamp ) ) : # TODO : Support other granularities than DAY table . time_partitioning = bigquery . TimePartitioning ( field = head , type_ = bigquery . TimePartitioningType . DAY ) table . require_partition_filter = True elif isinstance ( type_ . element . fields [ head ] , types . _Int ) : raise NotImplementedError ( \"BigQuery integer range partitioning is not implemented\" ) else : raise ValueError ( \"BigQuery only supports integer range or time partitioning\" ) if cluster : table . clustering_fields = cluster return table","title":"TableTypeAdapter"},{"location":"reference/arti/types/bigquery/#ancestors-in-mro_3","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/bigquery/#class-variables_4","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/bigquery/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/arti/types/bigquery/#matches_artigraph_3","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/bigquery/#matches_system_3","text":"def matches_system ( type_ : 'bigquery.SchemaField' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . system )","title":"matches_system"},{"location":"reference/arti/types/bigquery/#to_artigraph_3","text":"def to_artigraph ( type_ : 'bigquery.Table' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : bigquery . Table , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : kwargs : dict [ str, Any ] = {} if type_ . time_partitioning : if type_ . time_partitioning . type_ != bigquery . TimePartitioningType . DAY : raise NotImplementedError ( f \"BigQuery time partitioning other than 'DAY' is not implemented (got '{type_.time_partitioning.type_}')\" ) kwargs [ \"partition_by\" ] = ( type_ . time_partitioning . field ,) if type_ . range_partitioning : raise NotImplementedError ( \"BigQuery integer range partitioning is not implemented\" ) if type_ . clustering_fields : kwargs [ \"cluster_by\" ] = tuple ( type_ . clustering_fields ) return cls . artigraph ( name = f \"{type_.project}.{type_.dataset_id}.{type_.table_id}\" , element = type_system . to_artigraph ( bigquery . SchemaField ( types . DEFAULT_ANONYMOUS_NAME , SqlTypeNames . STRUCT , fields = type_ . schema , mode = \"REQUIRED\" , ), hints = hints , ), nullable = False , ** kwargs , )","title":"to_artigraph"},{"location":"reference/arti/types/bigquery/#to_system_3","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , types . Struct ) name = type_ . name # Override invalid default ( must be project / dataset qualified ) if name == types . DEFAULT_ANONYMOUS_NAME : name = \"project.dataset.table\" table = cls . system ( name , schema = type_system . to_system ( type_ . element , hints = hints ). fields ) partition , cluster = type_ . partition_by , type_ . cluster_by if partition : # BigQuery only supports a single partitioning field . We 'll move the rest to the # beginning of the cluster_by. This shouldn' t matter much anyway since , depending on the # Storage , we ' ll have separate tables for each unique composite key . head , * tail = partition if tail : cluster = ( * tail , * cluster ) if isinstance ( type_ . element . fields [ head ] , ( types . Date , types . DateTime , types . Timestamp ) ) : # TODO : Support other granularities than DAY table . time_partitioning = bigquery . TimePartitioning ( field = head , type_ = bigquery . TimePartitioningType . DAY ) table . require_partition_filter = True elif isinstance ( type_ . element . fields [ head ] , types . _Int ) : raise NotImplementedError ( \"BigQuery integer range partitioning is not implemented\" ) else : raise ValueError ( \"BigQuery only supports integer range or time partitioning\" ) if cluster : table . clustering_fields = cluster return table","title":"to_system"},{"location":"reference/arti/types/bigquery/#timetypeadapter","text":"class TimeTypeAdapter ( / , * args , ** kwargs ) View Source @bigquery_type_system . register_adapter class TimeTypeAdapter ( _BaseTimeTypeAdapter ) : artigraph = types . Time system = SqlTypeNames . TIME","title":"TimeTypeAdapter"},{"location":"reference/arti/types/bigquery/#ancestors-in-mro_4","text":"arti.types.bigquery._BaseTimeTypeAdapter arti.types.bigquery._BigQueryTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/bigquery/#class-variables_5","text":"artigraph key precision priority system","title":"Class variables"},{"location":"reference/arti/types/bigquery/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/arti/types/bigquery/#matches_artigraph_4","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/bigquery/#matches_system_4","text":"def matches_system ( type_ : 'bigquery.SchemaField' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , bigquery . SchemaField ) and type_ . field_type . upper () == cls . system # type : ignore [ no-any-return ]","title":"matches_system"},{"location":"reference/arti/types/bigquery/#to_artigraph_4","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : assert issubclass ( cls . artigraph , types . _TimeMixin ) return cls . artigraph ( description = type_ . description , nullable = type_ . is_nullable , precision = cls . precision )","title":"to_artigraph"},{"location":"reference/arti/types/bigquery/#to_system_4","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return _create_schema_field ( cls . system , type_ , hints )","title":"to_system"},{"location":"reference/arti/types/bigquery/#timestamptypeadapter","text":"class TimestampTypeAdapter ( / , * args , ** kwargs ) View Source @bigquery_type_system . register_adapter class TimestampTypeAdapter ( _BaseTimeTypeAdapter ) : artigraph = types . Timestamp system = SqlTypeNames . TIMESTAMP","title":"TimestampTypeAdapter"},{"location":"reference/arti/types/bigquery/#ancestors-in-mro_5","text":"arti.types.bigquery._BaseTimeTypeAdapter arti.types.bigquery._BigQueryTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/bigquery/#class-variables_6","text":"artigraph key precision priority system","title":"Class variables"},{"location":"reference/arti/types/bigquery/#static-methods_5","text":"","title":"Static methods"},{"location":"reference/arti/types/bigquery/#matches_artigraph_5","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/bigquery/#matches_system_5","text":"def matches_system ( type_ : 'bigquery.SchemaField' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : bigquery . SchemaField , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , bigquery . SchemaField ) and type_ . field_type . upper () == cls . system # type : ignore [ no-any-return ]","title":"matches_system"},{"location":"reference/arti/types/bigquery/#to_artigraph_5","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : assert issubclass ( cls . artigraph , types . _TimeMixin ) return cls . artigraph ( description = type_ . description , nullable = type_ . is_nullable , precision = cls . precision )","title":"to_artigraph"},{"location":"reference/arti/types/bigquery/#to_system_5","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return _create_schema_field ( cls . system , type_ , hints )","title":"to_system"},{"location":"reference/arti/types/numpy/","text":"Module arti.types.numpy None None View Source from __future__ import annotations from functools import partial from typing import Any , cast import numpy as np import arti.types from arti.types import ( Binary , Boolean , List , String , Type , TypeAdapter , TypeSystem , _ScalarClassTypeAdapter , ) # NOTE: TypeAdapters for some types may still be missing. Please open an issue or PR if you find # anything missing. # # TODO: Handle compound/structured dtypes and recarray # - This page will likely be helpful: https://numpy.org/doc/stable/reference/arrays.dtypes.html#arrays-dtypes numpy_type_system = TypeSystem ( key = \"numpy\" ) class _NumpyScalarTypeAdapter ( _ScalarClassTypeAdapter ): @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : if isinstance ( type_ , np . ndarray ): return False # NOTE: this works for both direct type and np.dtype comparison, eg: # - np.bool_ == np.bool_ # - np.dtype(\"bool\") == np.bool_ return cast ( bool , type_ == cls . system ) _generate = partial ( _NumpyScalarTypeAdapter . generate , type_system = numpy_type_system ) _generate ( artigraph = Binary , system = np . bytes_ ) _generate ( artigraph = Boolean , system = np . bool_ ) _generate ( artigraph = String , system = np . str_ ) for _precision in ( 16 , 32 , 64 ): _generate ( artigraph = getattr ( arti . types , f \"Float { _precision } \" ), system = getattr ( np , f \"float { _precision } \" ), priority = _precision , ) for _precision in ( 8 , 16 , 32 , 64 ): _generate ( artigraph = getattr ( arti . types , f \"Int { _precision } \" ), system = getattr ( np , f \"int { _precision } \" ), priority = _precision , ) _generate ( artigraph = getattr ( arti . types , f \"UInt { _precision } \" ), system = getattr ( np , f \"uint { _precision } \" ), priority = _precision , ) @numpy_type_system . register_adapter class ArrayAdapter ( TypeAdapter ): artigraph = List system = np . ndarray # NOTE: np.ndarray now supports TypeVars, eg: `np.ndarray[Any, np.dtype[np.float64]]` # # We may consider supporting that form in addition to an empty value, but it doesn't yet support # specifying the shape/ndim. @classmethod def to_artigraph ( cls , type_ : np . ndarray [ Any , Any ], * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : value = type_ [ 0 ] if isinstance ( type_ [ 0 ], np . ndarray ) else type ( type_ [ 0 ]) return cls . artigraph ( element = type_system . to_artigraph ( value , hints = hints )) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , np . ndarray ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> np . ndarray [ Any , Any ]: assert isinstance ( type_ , List ) element_type = type_system . to_system ( type_ . element , hints = hints ) # scalar numpy dtypes can be instantiated to return a zero value, like the python types value = element_type if isinstance ( element_type , np . ndarray ) else element_type () return np . array ([ value ]) Variables numpy_type_system Classes ArrayAdapter class ArrayAdapter ( / , * args , ** kwargs ) View Source @numpy_type_system . register_adapter class ArrayAdapter ( TypeAdapter ) : artigraph = List system = np . ndarray # NOTE : np . ndarray now supports TypeVars , eg : ` np . ndarray [ Any, np.dtype[np.float64 ] ]` # # We may consider supporting that form in addition to an empty value , but it doesn ' t yet support # specifying the shape / ndim . @classmethod def to_artigraph ( cls , type_ : np . ndarray [ Any, Any ] , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : value = type_ [ 0 ] if isinstance ( type_ [ 0 ] , np . ndarray ) else type ( type_ [ 0 ] ) return cls . artigraph ( element = type_system . to_artigraph ( value , hints = hints )) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , np . ndarray ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> np . ndarray [ Any, Any ] : assert isinstance ( type_ , List ) element_type = type_system . to_system ( type_ . element , hints = hints ) # scalar numpy dtypes can be instantiated to return a zero value , like the python types value = element_type if isinstance ( element_type , np . ndarray ) else element_type () return np . array ( [ value ] ) Ancestors (in MRO) arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , np . ndarray ) to_artigraph def to_artigraph ( type_ : 'np.ndarray[Any, Any]' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : np . ndarray [ Any, Any ] , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : value = type_ [ 0 ] if isinstance ( type_ [ 0 ] , np . ndarray ) else type ( type_ [ 0 ] ) return cls . artigraph ( element = type_system . to_artigraph ( value , hints = hints )) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'np.ndarray[Any, Any]' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> np . ndarray [ Any, Any ] : assert isinstance ( type_ , List ) element_type = type_system . to_system ( type_ . element , hints = hints ) # scalar numpy dtypes can be instantiated to return a zero value , like the python types value = element_type if isinstance ( element_type , np . ndarray ) else element_type () return np . array ( [ value ] )","title":"Numpy"},{"location":"reference/arti/types/numpy/#module-artitypesnumpy","text":"None None View Source from __future__ import annotations from functools import partial from typing import Any , cast import numpy as np import arti.types from arti.types import ( Binary , Boolean , List , String , Type , TypeAdapter , TypeSystem , _ScalarClassTypeAdapter , ) # NOTE: TypeAdapters for some types may still be missing. Please open an issue or PR if you find # anything missing. # # TODO: Handle compound/structured dtypes and recarray # - This page will likely be helpful: https://numpy.org/doc/stable/reference/arrays.dtypes.html#arrays-dtypes numpy_type_system = TypeSystem ( key = \"numpy\" ) class _NumpyScalarTypeAdapter ( _ScalarClassTypeAdapter ): @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : if isinstance ( type_ , np . ndarray ): return False # NOTE: this works for both direct type and np.dtype comparison, eg: # - np.bool_ == np.bool_ # - np.dtype(\"bool\") == np.bool_ return cast ( bool , type_ == cls . system ) _generate = partial ( _NumpyScalarTypeAdapter . generate , type_system = numpy_type_system ) _generate ( artigraph = Binary , system = np . bytes_ ) _generate ( artigraph = Boolean , system = np . bool_ ) _generate ( artigraph = String , system = np . str_ ) for _precision in ( 16 , 32 , 64 ): _generate ( artigraph = getattr ( arti . types , f \"Float { _precision } \" ), system = getattr ( np , f \"float { _precision } \" ), priority = _precision , ) for _precision in ( 8 , 16 , 32 , 64 ): _generate ( artigraph = getattr ( arti . types , f \"Int { _precision } \" ), system = getattr ( np , f \"int { _precision } \" ), priority = _precision , ) _generate ( artigraph = getattr ( arti . types , f \"UInt { _precision } \" ), system = getattr ( np , f \"uint { _precision } \" ), priority = _precision , ) @numpy_type_system . register_adapter class ArrayAdapter ( TypeAdapter ): artigraph = List system = np . ndarray # NOTE: np.ndarray now supports TypeVars, eg: `np.ndarray[Any, np.dtype[np.float64]]` # # We may consider supporting that form in addition to an empty value, but it doesn't yet support # specifying the shape/ndim. @classmethod def to_artigraph ( cls , type_ : np . ndarray [ Any , Any ], * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : value = type_ [ 0 ] if isinstance ( type_ [ 0 ], np . ndarray ) else type ( type_ [ 0 ]) return cls . artigraph ( element = type_system . to_artigraph ( value , hints = hints )) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , np . ndarray ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> np . ndarray [ Any , Any ]: assert isinstance ( type_ , List ) element_type = type_system . to_system ( type_ . element , hints = hints ) # scalar numpy dtypes can be instantiated to return a zero value, like the python types value = element_type if isinstance ( element_type , np . ndarray ) else element_type () return np . array ([ value ])","title":"Module arti.types.numpy"},{"location":"reference/arti/types/numpy/#variables","text":"numpy_type_system","title":"Variables"},{"location":"reference/arti/types/numpy/#classes","text":"","title":"Classes"},{"location":"reference/arti/types/numpy/#arrayadapter","text":"class ArrayAdapter ( / , * args , ** kwargs ) View Source @numpy_type_system . register_adapter class ArrayAdapter ( TypeAdapter ) : artigraph = List system = np . ndarray # NOTE : np . ndarray now supports TypeVars , eg : ` np . ndarray [ Any, np.dtype[np.float64 ] ]` # # We may consider supporting that form in addition to an empty value , but it doesn ' t yet support # specifying the shape / ndim . @classmethod def to_artigraph ( cls , type_ : np . ndarray [ Any, Any ] , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : value = type_ [ 0 ] if isinstance ( type_ [ 0 ] , np . ndarray ) else type ( type_ [ 0 ] ) return cls . artigraph ( element = type_system . to_artigraph ( value , hints = hints )) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , np . ndarray ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> np . ndarray [ Any, Any ] : assert isinstance ( type_ , List ) element_type = type_system . to_system ( type_ . element , hints = hints ) # scalar numpy dtypes can be instantiated to return a zero value , like the python types value = element_type if isinstance ( element_type , np . ndarray ) else element_type () return np . array ( [ value ] )","title":"ArrayAdapter"},{"location":"reference/arti/types/numpy/#ancestors-in-mro","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/numpy/#class-variables","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/numpy/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/types/numpy/#matches_artigraph","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/numpy/#matches_system","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , np . ndarray )","title":"matches_system"},{"location":"reference/arti/types/numpy/#to_artigraph","text":"def to_artigraph ( type_ : 'np.ndarray[Any, Any]' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : np . ndarray [ Any, Any ] , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : value = type_ [ 0 ] if isinstance ( type_ [ 0 ] , np . ndarray ) else type ( type_ [ 0 ] ) return cls . artigraph ( element = type_system . to_artigraph ( value , hints = hints ))","title":"to_artigraph"},{"location":"reference/arti/types/numpy/#to_system","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'np.ndarray[Any, Any]' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> np . ndarray [ Any, Any ] : assert isinstance ( type_ , List ) element_type = type_system . to_system ( type_ . element , hints = hints ) # scalar numpy dtypes can be instantiated to return a zero value , like the python types value = element_type if isinstance ( element_type , np . ndarray ) else element_type () return np . array ( [ value ] )","title":"to_system"},{"location":"reference/arti/types/pandas/","text":"Module arti.types.pandas None None View Source from __future__ import annotations from typing import Any , cast import numpy as np import pandas as pd from arti.types import List , String , Struct , Type , TypeAdapter , TypeSystem from arti.types.numpy import numpy_type_system # TODO: How should (multi)indexes be handled; perhaps as a \"hint\"? pandas_type_system = TypeSystem ( key = \"pandas\" , extends = ( numpy_type_system ,)) @pandas_type_system . register_adapter class SeriesAdapter ( TypeAdapter ): artigraph = List system = pd . Series @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : return ( isinstance ( type_ , cls . artigraph ) # List(element=Struct(...)) are handled by the DataFrameAdapter. and not isinstance ( type_ . element , Struct ) ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : dtype = type_ . dtype if dtype == np . dtype ( \"O\" ): # TODO: Should we handle empty series by defaulting to \"String\", but issuing # a warning? example_value = type_ . iloc [ 0 ] if isinstance ( example_value , str ): return List ( element = String ()) # TODO: Handle dicts, lists, etc. raise NotImplementedError ( f \"Non-string { dtype } is not supported yet, got values of: { example_value } \" ) return List ( element = type_system . to_artigraph ( dtype , hints = hints )) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) dtype = type_system . to_system ( type_ . element , hints = hints ) return pd . Series ([ dtype ()], dtype = dtype ) @pandas_type_system . register_adapter class DataFrameAdapter ( TypeAdapter ): \"\"\"Convert between a List of Structs and a pd.DataFrame. Expects a List type like: >>> from arti.types import Float64, Int8, List, Struct >>> from arti.types.pandas import pandas_type_system >>> >>> arti_type = List(element=Struct(fields={\"col1\": Int8(), \"col2\": Float64()})) >>> pandas_type_system.to_system(arti_type, hints={}) col1 col2 0 0 0.0 \"\"\" artigraph = List system = pd . DataFrame @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : # Match Lists of Structs, but not sub-fields (eg: a column containing lists). We may need to # pass a `hint` to identify when we're not at the root to distinguish the main dataframe # from columns containing list[dict[...]] values. return isinstance ( type_ , cls . artigraph ) and isinstance ( type_ . element , Struct ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : assert isinstance ( type_ , cls . system ) return List ( element = Struct ( fields = { name : cast ( List , type_system . to_artigraph ( type_ [ name ], hints = hints )) . element for name in type_ . columns } ) ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , Struct ) # NOTE: We automatically wrap the sub-types as List(...) to match the SeriesAdapter. return pd . DataFrame ( { name : type_system . to_system ( List ( element = subtype ), hints = hints ) for name , subtype in type_ . element . fields . items () } ) Variables pandas_type_system Classes DataFrameAdapter class DataFrameAdapter ( / , * args , ** kwargs ) View Source @pandas_type_system . register_adapter class DataFrameAdapter ( TypeAdapter ): \"\"\"Convert between a List of Structs and a pd.DataFrame. Expects a List type like: >>> from arti.types import Float64, Int8, List, Struct >>> from arti.types.pandas import pandas_type_system >>> >>> arti_type = List(element=Struct(fields={\"col1\": Int8(), \"col2\": Float64()})) >>> pandas_type_system.to_system(arti_type, hints={}) col1 col2 0 0 0.0 \"\"\" artigraph = List system = pd . DataFrame @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : # Match Lists of Structs, but not sub-fields (eg: a column containing lists). We may need to # pass a `hint` to identify when we're not at the root to distinguish the main dataframe # from columns containing list[dict[...]] values. return isinstance ( type_ , cls . artigraph ) and isinstance ( type_ . element , Struct ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : assert isinstance ( type_ , cls . system ) return List ( element = Struct ( fields = { name : cast ( List , type_system . to_artigraph ( type_ [ name ], hints = hints )) . element for name in type_ . columns } ) ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , Struct ) # NOTE: We automatically wrap the sub-types as List(...) to match the SeriesAdapter. return pd . DataFrame ( { name : type_system . to_system ( List ( element = subtype ), hints = hints ) for name , subtype in type_ . element . fields . items () } ) Ancestors (in MRO) arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ] ) -> bool : # Match Lists of Structs, but not sub-fields (eg: a column containing lists). We may need to # pass a `hint` to identify when we're not at the root to distinguish the main dataframe # from columns containing list[dict[...]] values. return isinstance ( type_ , cls . artigraph ) and isinstance ( type_ . element , Struct ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . system ) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : assert isinstance ( type_ , cls . system ) return List ( element = Struct ( fields = { name : cast ( List , type_system . to_artigraph ( type_ [ name ] , hints = hints )). element for name in type_ . columns } ) ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , Struct ) # NOTE : We automatically wrap the sub - types as List (...) to match the SeriesAdapter . return pd . DataFrame ( { name : type_system . to_system ( List ( element = subtype ), hints = hints ) for name , subtype in type_ . element . fields . items () } ) SeriesAdapter class SeriesAdapter ( / , * args , ** kwargs ) View Source @pandas_type_system . register_adapter class SeriesAdapter ( TypeAdapter ) : artigraph = List system = pd . Series @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return ( isinstance ( type_ , cls . artigraph ) # List ( element = Struct (...)) are handled by the DataFrameAdapter . and not isinstance ( type_ . element , Struct ) ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : dtype = type_ . dtype if dtype == np . dtype ( \"O\" ) : # TODO : Should we handle empty series by defaulting to \"String\" , but issuing # a warning ? example_value = type_ . iloc [ 0 ] if isinstance ( example_value , str ) : return List ( element = String ()) # TODO : Handle dicts , lists , etc . raise NotImplementedError ( f \"Non-string {dtype} is not supported yet, got values of: {example_value}\" ) return List ( element = type_system . to_artigraph ( dtype , hints = hints )) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) dtype = type_system . to_system ( type_ . element , hints = hints ) return pd . Series ( [ dtype() ] , dtype = dtype ) Ancestors (in MRO) arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return ( isinstance ( type_ , cls . artigraph ) # List ( element = Struct (...)) are handled by the DataFrameAdapter . and not isinstance ( type_ . element , Struct ) ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . system ) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : dtype = type_ . dtype if dtype == np . dtype ( \"O\" ) : # TODO : Should we handle empty series by defaulting to \"String\" , but issuing # a warning ? example_value = type_ . iloc [ 0 ] if isinstance ( example_value , str ) : return List ( element = String ()) # TODO : Handle dicts , lists , etc . raise NotImplementedError ( f \"Non-string {dtype} is not supported yet, got values of: {example_value}\" ) return List ( element = type_system . to_artigraph ( dtype , hints = hints )) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) dtype = type_system . to_system ( type_ . element , hints = hints ) return pd . Series ( [ dtype() ] , dtype = dtype )","title":"Pandas"},{"location":"reference/arti/types/pandas/#module-artitypespandas","text":"None None View Source from __future__ import annotations from typing import Any , cast import numpy as np import pandas as pd from arti.types import List , String , Struct , Type , TypeAdapter , TypeSystem from arti.types.numpy import numpy_type_system # TODO: How should (multi)indexes be handled; perhaps as a \"hint\"? pandas_type_system = TypeSystem ( key = \"pandas\" , extends = ( numpy_type_system ,)) @pandas_type_system . register_adapter class SeriesAdapter ( TypeAdapter ): artigraph = List system = pd . Series @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : return ( isinstance ( type_ , cls . artigraph ) # List(element=Struct(...)) are handled by the DataFrameAdapter. and not isinstance ( type_ . element , Struct ) ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : dtype = type_ . dtype if dtype == np . dtype ( \"O\" ): # TODO: Should we handle empty series by defaulting to \"String\", but issuing # a warning? example_value = type_ . iloc [ 0 ] if isinstance ( example_value , str ): return List ( element = String ()) # TODO: Handle dicts, lists, etc. raise NotImplementedError ( f \"Non-string { dtype } is not supported yet, got values of: { example_value } \" ) return List ( element = type_system . to_artigraph ( dtype , hints = hints )) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) dtype = type_system . to_system ( type_ . element , hints = hints ) return pd . Series ([ dtype ()], dtype = dtype ) @pandas_type_system . register_adapter class DataFrameAdapter ( TypeAdapter ): \"\"\"Convert between a List of Structs and a pd.DataFrame. Expects a List type like: >>> from arti.types import Float64, Int8, List, Struct >>> from arti.types.pandas import pandas_type_system >>> >>> arti_type = List(element=Struct(fields={\"col1\": Int8(), \"col2\": Float64()})) >>> pandas_type_system.to_system(arti_type, hints={}) col1 col2 0 0 0.0 \"\"\" artigraph = List system = pd . DataFrame @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : # Match Lists of Structs, but not sub-fields (eg: a column containing lists). We may need to # pass a `hint` to identify when we're not at the root to distinguish the main dataframe # from columns containing list[dict[...]] values. return isinstance ( type_ , cls . artigraph ) and isinstance ( type_ . element , Struct ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : assert isinstance ( type_ , cls . system ) return List ( element = Struct ( fields = { name : cast ( List , type_system . to_artigraph ( type_ [ name ], hints = hints )) . element for name in type_ . columns } ) ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , Struct ) # NOTE: We automatically wrap the sub-types as List(...) to match the SeriesAdapter. return pd . DataFrame ( { name : type_system . to_system ( List ( element = subtype ), hints = hints ) for name , subtype in type_ . element . fields . items () } )","title":"Module arti.types.pandas"},{"location":"reference/arti/types/pandas/#variables","text":"pandas_type_system","title":"Variables"},{"location":"reference/arti/types/pandas/#classes","text":"","title":"Classes"},{"location":"reference/arti/types/pandas/#dataframeadapter","text":"class DataFrameAdapter ( / , * args , ** kwargs ) View Source @pandas_type_system . register_adapter class DataFrameAdapter ( TypeAdapter ): \"\"\"Convert between a List of Structs and a pd.DataFrame. Expects a List type like: >>> from arti.types import Float64, Int8, List, Struct >>> from arti.types.pandas import pandas_type_system >>> >>> arti_type = List(element=Struct(fields={\"col1\": Int8(), \"col2\": Float64()})) >>> pandas_type_system.to_system(arti_type, hints={}) col1 col2 0 0 0.0 \"\"\" artigraph = List system = pd . DataFrame @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : # Match Lists of Structs, but not sub-fields (eg: a column containing lists). We may need to # pass a `hint` to identify when we're not at the root to distinguish the main dataframe # from columns containing list[dict[...]] values. return isinstance ( type_ , cls . artigraph ) and isinstance ( type_ . element , Struct ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : assert isinstance ( type_ , cls . system ) return List ( element = Struct ( fields = { name : cast ( List , type_system . to_artigraph ( type_ [ name ], hints = hints )) . element for name in type_ . columns } ) ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , Struct ) # NOTE: We automatically wrap the sub-types as List(...) to match the SeriesAdapter. return pd . DataFrame ( { name : type_system . to_system ( List ( element = subtype ), hints = hints ) for name , subtype in type_ . element . fields . items () } )","title":"DataFrameAdapter"},{"location":"reference/arti/types/pandas/#ancestors-in-mro","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/pandas/#class-variables","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/pandas/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/types/pandas/#matches_artigraph","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ] ) -> bool : # Match Lists of Structs, but not sub-fields (eg: a column containing lists). We may need to # pass a `hint` to identify when we're not at the root to distinguish the main dataframe # from columns containing list[dict[...]] values. return isinstance ( type_ , cls . artigraph ) and isinstance ( type_ . element , Struct )","title":"matches_artigraph"},{"location":"reference/arti/types/pandas/#matches_system","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . system )","title":"matches_system"},{"location":"reference/arti/types/pandas/#to_artigraph","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : assert isinstance ( type_ , cls . system ) return List ( element = Struct ( fields = { name : cast ( List , type_system . to_artigraph ( type_ [ name ] , hints = hints )). element for name in type_ . columns } ) )","title":"to_artigraph"},{"location":"reference/arti/types/pandas/#to_system","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , Struct ) # NOTE : We automatically wrap the sub - types as List (...) to match the SeriesAdapter . return pd . DataFrame ( { name : type_system . to_system ( List ( element = subtype ), hints = hints ) for name , subtype in type_ . element . fields . items () } )","title":"to_system"},{"location":"reference/arti/types/pandas/#seriesadapter","text":"class SeriesAdapter ( / , * args , ** kwargs ) View Source @pandas_type_system . register_adapter class SeriesAdapter ( TypeAdapter ) : artigraph = List system = pd . Series @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return ( isinstance ( type_ , cls . artigraph ) # List ( element = Struct (...)) are handled by the DataFrameAdapter . and not isinstance ( type_ . element , Struct ) ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : dtype = type_ . dtype if dtype == np . dtype ( \"O\" ) : # TODO : Should we handle empty series by defaulting to \"String\" , but issuing # a warning ? example_value = type_ . iloc [ 0 ] if isinstance ( example_value , str ) : return List ( element = String ()) # TODO : Handle dicts , lists , etc . raise NotImplementedError ( f \"Non-string {dtype} is not supported yet, got values of: {example_value}\" ) return List ( element = type_system . to_artigraph ( dtype , hints = hints )) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) dtype = type_system . to_system ( type_ . element , hints = hints ) return pd . Series ( [ dtype() ] , dtype = dtype )","title":"SeriesAdapter"},{"location":"reference/arti/types/pandas/#ancestors-in-mro_1","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/pandas/#class-variables_1","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/pandas/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/types/pandas/#matches_artigraph_1","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return ( isinstance ( type_ , cls . artigraph ) # List ( element = Struct (...)) are handled by the DataFrameAdapter . and not isinstance ( type_ . element , Struct ) )","title":"matches_artigraph"},{"location":"reference/arti/types/pandas/#matches_system_1","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . system )","title":"matches_system"},{"location":"reference/arti/types/pandas/#to_artigraph_1","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : dtype = type_ . dtype if dtype == np . dtype ( \"O\" ) : # TODO : Should we handle empty series by defaulting to \"String\" , but issuing # a warning ? example_value = type_ . iloc [ 0 ] if isinstance ( example_value , str ) : return List ( element = String ()) # TODO : Handle dicts , lists , etc . raise NotImplementedError ( f \"Non-string {dtype} is not supported yet, got values of: {example_value}\" ) return List ( element = type_system . to_artigraph ( dtype , hints = hints ))","title":"to_artigraph"},{"location":"reference/arti/types/pandas/#to_system_1","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) dtype = type_system . to_system ( type_ . element , hints = hints ) return pd . Series ( [ dtype() ] , dtype = dtype )","title":"to_system"},{"location":"reference/arti/types/pyarrow/","text":"Module arti.types.pyarrow None None View Source from __future__ import annotations import json from collections.abc import Callable from typing import Any , cast import pyarrow as pa from arti import Type , TypeAdapter , TypeSystem , types from arti.internal.utils import classproperty pyarrow_type_system = TypeSystem ( key = \"pyarrow\" ) # Not implemented: # decimal128(int precision, int scale=0), # dictionary(index_type, value_type, \u2026), # large_binary(), # large_list(value_type), # large_string(), class _PyarrowTypeAdapter ( TypeAdapter ): @classproperty def _is_system ( cls ) -> Callable [[ pa . DataType ], bool ]: return getattr ( pa . types , f \"is_ { cls . system . __name__ } \" ) # type: ignore[no-any-return] @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return cls . artigraph () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , pa . DataType ) and cls . _is_system ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : return cls . system () def _gen_adapter ( * , artigraph : type [ Type ], system : Any , priority : int = 0 ) -> type [ TypeAdapter ]: return pyarrow_type_system . register_adapter ( type ( f \"Pyarrow { system . __name__ } \" , ( _PyarrowTypeAdapter ,), { \"artigraph\" : artigraph , \"system\" : system , \"priority\" : priority }, ) ) _gen_adapter ( artigraph = types . String , system = pa . string ) _gen_adapter ( artigraph = types . Null , system = pa . null ) # Date matching requires `priority=_precision` since it is not 1:1, but the float/int ones are. for _precision in ( 32 , 64 ): _gen_adapter ( artigraph = types . Date , system = getattr ( pa , f \"date { _precision } \" ), priority = _precision , ) for _precision in ( 16 , 32 , 64 ): _gen_adapter ( artigraph = getattr ( types , f \"Float { _precision } \" ), system = getattr ( pa , f \"float { _precision } \" ), ) for _precision in ( 8 , 16 , 32 , 64 ): _gen_adapter ( artigraph = getattr ( types , f \"Int { _precision } \" ), system = getattr ( pa , f \"int { _precision } \" ), ) _gen_adapter ( artigraph = getattr ( types , f \"UInt { _precision } \" ), system = getattr ( pa , f \"uint { _precision } \" ), ) @pyarrow_type_system . register_adapter class BinaryTypeAdapter ( _PyarrowTypeAdapter ): artigraph = types . Binary system = pa . binary @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : if isinstance ( type_ , pa . FixedSizeBinaryType ): return cls . artigraph ( byte_size = type_ . byte_width ) return cls . artigraph () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : # pa.binary returns a DataType(binary) when length=-1, otherwise a FixedSizeBinaryType... # but pa.types.is_binary only checks for DataType(binary). return super () . matches_system ( type_ , hints = hints ) or pa . types . is_fixed_size_binary ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( length =- 1 if type_ . byte_size is None else type_ . byte_size ) # The pyarrow bool constructor and checker have different names @pyarrow_type_system . register_adapter class BoolTypeAdapter ( _PyarrowTypeAdapter ): artigraph = types . Boolean system = pa . bool_ @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return cast ( bool , pa . types . is_boolean ( type_ )) @pyarrow_type_system . register_adapter class GeographyTypeAdapter ( _PyarrowTypeAdapter ): # TODO: Can we do something with pa.field metadata to round trip (eg: format, srid, etc) or # infer GeoParquet? artigraph = types . Geography system = pa . string # or pa.binary if geography.format == \"WKB\" @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : # We don't have any metadata to differentiate normal strings from geographies, so avoid # matching. This will prevent round tripping. return False @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return pa . binary () if type_ . format == \"WKB\" else pa . string () @pyarrow_type_system . register_adapter class ListTypeAdapter ( _PyarrowTypeAdapter ): artigraph = types . List system = pa . list_ @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return cls . artigraph ( element = type_system . to_artigraph ( type_ . value_type , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return cast ( bool , pa . types . is_list ( type_ )) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( value_type = type_system . to_system ( type_ . element , hints = hints )) @pyarrow_type_system . register_adapter class MapTypeAdapter ( _PyarrowTypeAdapter ): artigraph = types . Map system = pa . map_ @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return cls . artigraph ( key = type_system . to_artigraph ( type_ . key_type , hints = hints ), value = type_system . to_artigraph ( type_ . item_type , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return cast ( bool , pa . types . is_map ( type_ )) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( key_type = type_system . to_system ( type_ . key , hints = hints ), item_type = type_system . to_system ( type_ . value , hints = hints ), ) @pyarrow_type_system . register_adapter class StructTypeAdapter ( _PyarrowTypeAdapter ): artigraph = types . Struct system = pa . struct @classmethod def _field_to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : ret = type_system . to_artigraph ( type_ . type , hints = hints ) if type_ . nullable != ret . nullable : # Avoid setting nullable if matching to minimize repr ret = ret . copy ( update = { \"nullable\" : type_ . nullable }) return ret @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return cls . artigraph ( fields = { field . name : cls . _field_to_artigraph ( field , hints = hints , type_system = type_system ) for field in type_ } ) @classmethod def _field_to_system ( cls , name : str , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : return pa . field ( name , type_system . to_system ( type_ , hints = hints ), nullable = type_ . nullable ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( [ cls . _field_to_system ( name , subtype , hints = hints , type_system = type_system ) for name , subtype in type_ . fields . items () ] ) # NOTE: pa.schema and pa.struct are structurally similar, but pa.schema has additional attributes # (eg: .metadata) and cannot be nested (like Collection). @pyarrow_type_system . register_adapter class SchemaTypeAdapter ( _PyarrowTypeAdapter ): artigraph = types . Collection system = pa . schema priority = ListTypeAdapter . priority + 1 @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : # Collection can hold arbitrary types, but `pa.schema` is only a struct (but with arbitrary # metadata) return super () . matches_artigraph ( type_ = type_ , hints = hints ) and isinstance ( type_ . element , types . Struct # type: ignore[attr-defined] ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : kwargs = {} # NOTE: pyarrow converts all metadata keys/values to bytes if type_ . metadata and b \"artigraph\" in type_ . metadata : kwargs = json . loads ( type_ . metadata [ b \"artigraph\" ] . decode ()) for key in [ \"partition_by\" , \"cluster_by\" ]: if key in kwargs : # pragma: no cover kwargs [ key ] = tuple ( kwargs [ key ]) return cls . artigraph ( element = StructTypeAdapter . to_artigraph ( type_ , hints = hints , type_system = type_system ), ** kwargs , ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , pa . lib . Schema ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , types . Struct ) return cls . system ( StructTypeAdapter . to_system ( type_ . element , hints = hints , type_system = type_system ), metadata = { \"artigraph\" : json . dumps ( { \"name\" : type_ . name , \"partition_by\" : type_ . partition_by , \"cluster_by\" : type_ . cluster_by , } ) }, ) class _BaseTimeTypeAdapter ( _PyarrowTypeAdapter ): precision_to_unit = { \"second\" : \"s\" , \"millisecond\" : \"ms\" , \"microsecond\" : \"us\" , \"nanosecond\" : \"ns\" , } @classproperty def unit_to_precision ( cls ) -> dict [ str , str ]: return { v : k for k , v in cls . precision_to_unit . items ()} @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : if ( precision := cls . unit_to_precision . get ( type_ . unit )) is None : # pragma: no cover raise ValueError ( f \" { type_ } .unit must be one of { tuple ( cls . unit_to_precision ) } , got { type_ . unit } \" ) assert issubclass ( cls . artigraph , types . _TimeMixin ) return cls . artigraph ( precision = precision ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : precision = type_ . precision # type: ignore[attr-defined] if ( unit := cls . precision_to_unit . get ( precision )) is None : # pragma: no cover raise ValueError ( f \" { type_ } .precision must be one of { tuple ( cls . precision_to_unit ) } , got { precision } \" ) return cls . system ( unit ) @pyarrow_type_system . register_adapter class DateTimeTypeAdapter ( _BaseTimeTypeAdapter ): artigraph = types . DateTime system = pa . timestamp @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return super () . matches_system ( type_ , hints = hints ) and type_ . tz is None @pyarrow_type_system . register_adapter class TimestampTypeAdapter ( _BaseTimeTypeAdapter ): artigraph = types . Timestamp system = pa . timestamp @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : tz = type_ . tz . upper () if tz != \"UTC\" : raise ValueError ( f \"Timestamp { type_ } .tz must be in UTC, got { tz } \" ) return super () . to_artigraph ( type_ , hints = hints , type_system = type_system ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return super () . matches_system ( type_ , hints = hints ) and type_ . tz is not None @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : ts = super () . to_system ( type_ , hints = hints , type_system = type_system ) return cls . system ( ts . unit , \"UTC\" ) class _BaseSizedTimeTypeAdapter ( _BaseTimeTypeAdapter ): artigraph = types . Time @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : return ( super () . matches_artigraph ( type_ = type_ , hints = hints ) and type_ . precision in cls . precision_to_unit # type: ignore[attr-defined] ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return ( super () . matches_system ( type_ = type_ , hints = hints ) and type_ . unit in cls . unit_to_precision ) @pyarrow_type_system . register_adapter class Time32TypeAdapter ( _BaseSizedTimeTypeAdapter ): precision_to_unit = { \"second\" : \"s\" , \"millisecond\" : \"ms\" , } system = pa . time32 @pyarrow_type_system . register_adapter class Time64TypeAdapter ( _BaseSizedTimeTypeAdapter ): precision_to_unit = { \"microsecond\" : \"us\" , \"nanosecond\" : \"ns\" , } system = pa . time64 Variables pyarrow_type_system Classes BinaryTypeAdapter class BinaryTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class BinaryTypeAdapter ( _PyarrowTypeAdapter ) : artigraph = types . Binary system = pa . binary @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : if isinstance ( type_ , pa . FixedSizeBinaryType ) : return cls . artigraph ( byte_size = type_ . byte_width ) return cls . artigraph () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # pa . binary returns a DataType ( binary ) when length =- 1 , otherwise a FixedSizeBinaryType ... # but pa . types . is_binary only checks for DataType ( binary ). return super (). matches_system ( type_ , hints = hints ) or pa . types . is_fixed_size_binary ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( length =- 1 if type_ . byte_size is None else type_ . byte_size ) Ancestors (in MRO) arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter Class variables artigraph key priority Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # pa . binary returns a DataType ( binary ) when length =- 1 , otherwise a FixedSizeBinaryType ... # but pa . types . is_binary only checks for DataType ( binary ). return super (). matches_system ( type_ , hints = hints ) or pa . types . is_fixed_size_binary ( type_ ) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : if isinstance ( type_ , pa . FixedSizeBinaryType ) : return cls . artigraph ( byte_size = type_ . byte_width ) return cls . artigraph () to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( length =- 1 if type_ . byte_size is None else type_ . byte_size ) Methods system def system ( ... ) binary(int length=-1) Create variable-length binary type. Parameters: Name Type Description Default length int, optional, default -1 If length == -1 then return a variable length binary type. If length is greater than or equal to 0 then return a fixed size binary type of width length . None BoolTypeAdapter class BoolTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class BoolTypeAdapter ( _PyarrowTypeAdapter ) : artigraph = types . Boolean system = pa . bool_ @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return cast ( bool , pa . types . is_boolean ( type_ )) Ancestors (in MRO) arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter Class variables artigraph key priority Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return cast ( bool , pa . types . is_boolean ( type_ )) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph () to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return cls . system () Methods system def system ( ... ) bool_() Create instance of boolean type. DateTimeTypeAdapter class DateTimeTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class DateTimeTypeAdapter ( _BaseTimeTypeAdapter ) : artigraph = types . DateTime system = pa . timestamp @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return super (). matches_system ( type_ , hints = hints ) and type_ . tz is None Ancestors (in MRO) arti.types.pyarrow._BaseTimeTypeAdapter arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter Class variables artigraph key precision_to_unit priority unit_to_precision Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return super (). matches_system ( type_ , hints = hints ) and type_ . tz is None to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : if ( precision : = cls . unit_to_precision . get ( type_ . unit )) is None : # pragma : no cover raise ValueError ( f \"{type_}.unit must be one of {tuple(cls.unit_to_precision)}, got {type_.unit}\" ) assert issubclass ( cls . artigraph , types . _TimeMixin ) return cls . artigraph ( precision = precision ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : precision = type_ . precision # type : ignore [ attr-defined ] if ( unit : = cls . precision_to_unit . get ( precision )) is None : # pragma : no cover raise ValueError ( f \"{type_}.precision must be one of {tuple(cls.precision_to_unit)}, got {precision}\" ) return cls . system ( unit ) Methods system def system ( ... ) timestamp(unit, tz=None) Create instance of timestamp type with resolution and optional time zone. Parameters: Name Type Description Default unit str one of 's' [second], 'ms' [millisecond], 'us' [microsecond], or 'ns' [nanosecond] None tz str, default None Time zone name. None indicates time zone naive None Returns: Type Description TimestampType None GeographyTypeAdapter class GeographyTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class GeographyTypeAdapter ( _PyarrowTypeAdapter ) : # TODO : Can we do something with pa . field metadata to round trip ( eg : format , srid , etc ) or # infer GeoParquet ? artigraph = types . Geography system = pa . string # or pa . binary if geography . format == \"WKB\" @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # We don ' t have any metadata to differentiate normal strings from geographies , so avoid # matching . This will prevent round tripping . return False @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return pa . binary () if type_ . format == \"WKB\" else pa . string () Ancestors (in MRO) arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter Class variables artigraph key priority Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # We don ' t have any metadata to differentiate normal strings from geographies , so avoid # matching . This will prevent round tripping . return False to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph () to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return pa . binary () if type_ . format == \"WKB\" else pa . string () Methods system def system ( ... ) string() Create UTF8 variable-length string type. ListTypeAdapter class ListTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class ListTypeAdapter ( _PyarrowTypeAdapter ) : artigraph = types . List system = pa . list_ @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( element = type_system . to_artigraph ( type_ . value_type , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return cast ( bool , pa . types . is_list ( type_ )) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( value_type = type_system . to_system ( type_ . element , hints = hints )) Ancestors (in MRO) arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter Class variables artigraph key priority Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return cast ( bool , pa . types . is_list ( type_ )) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( element = type_system . to_artigraph ( type_ . value_type , hints = hints ), ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( value_type = type_system . to_system ( type_ . element , hints = hints )) Methods system def system ( ... ) list_(value_type, int list_size=-1) Create ListType instance from child data type or field. Parameters: Name Type Description Default value_type DataType or Field None None list_size int, optional, default -1 If length == -1 then return a variable length list type. If length is greater than or equal to 0 then return a fixed size list type. None Returns: Type Description DataType None MapTypeAdapter class MapTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class MapTypeAdapter ( _PyarrowTypeAdapter ) : artigraph = types . Map system = pa . map_ @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( key = type_system . to_artigraph ( type_ . key_type , hints = hints ), value = type_system . to_artigraph ( type_ . item_type , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return cast ( bool , pa . types . is_map ( type_ )) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( key_type = type_system . to_system ( type_ . key , hints = hints ), item_type = type_system . to_system ( type_ . value , hints = hints ), ) Ancestors (in MRO) arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter Class variables artigraph key priority Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return cast ( bool , pa . types . is_map ( type_ )) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( key = type_system . to_artigraph ( type_ . key_type , hints = hints ), value = type_system . to_artigraph ( type_ . item_type , hints = hints ), ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( key_type = type_system . to_system ( type_ . key , hints = hints ), item_type = type_system . to_system ( type_ . value , hints = hints ), ) Methods system def system ( ... ) map_(key_type, item_type, keys_sorted=False) -> MapType Create MapType instance from key and item data types or fields. Parameters: Name Type Description Default key_type DataType None None item_type DataType None None keys_sorted bool None None Returns: Type Description DataType None SchemaTypeAdapter class SchemaTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class SchemaTypeAdapter ( _PyarrowTypeAdapter ) : artigraph = types . Collection system = pa . schema priority = ListTypeAdapter . priority + 1 @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : # Collection can hold arbitrary types , but ` pa . schema ` is only a struct ( but with arbitrary # metadata ) return super (). matches_artigraph ( type_ = type_ , hints = hints ) and isinstance ( type_ . element , types . Struct # type : ignore [ attr-defined ] ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : kwargs = {} # NOTE : pyarrow converts all metadata keys / values to bytes if type_ . metadata and b \"artigraph\" in type_ . metadata : kwargs = json . loads ( type_ . metadata [ b\"artigraph\" ] . decode ()) for key in [ \"partition_by\", \"cluster_by\" ] : if key in kwargs : # pragma : no cover kwargs [ key ] = tuple ( kwargs [ key ] ) return cls . artigraph ( element = StructTypeAdapter . to_artigraph ( type_ , hints = hints , type_system = type_system ), ** kwargs , ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , pa . lib . Schema ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , types . Struct ) return cls . system ( StructTypeAdapter . to_system ( type_ . element , hints = hints , type_system = type_system ), metadata = { \"artigraph\" : json . dumps ( { \"name\" : type_ . name , \"partition_by\" : type_ . partition_by , \"cluster_by\" : type_ . cluster_by , } ) } , ) Ancestors (in MRO) arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter Class variables artigraph key priority Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : # Collection can hold arbitrary types , but ` pa . schema ` is only a struct ( but with arbitrary # metadata ) return super (). matches_artigraph ( type_ = type_ , hints = hints ) and isinstance ( type_ . element , types . Struct # type : ignore [ attr-defined ] ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , pa . lib . Schema ) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : kwargs = {} # NOTE : pyarrow converts all metadata keys / values to bytes if type_ . metadata and b \"artigraph\" in type_ . metadata : kwargs = json . loads ( type_ . metadata [ b\"artigraph\" ] . decode ()) for key in [ \"partition_by\", \"cluster_by\" ] : if key in kwargs : # pragma : no cover kwargs [ key ] = tuple ( kwargs [ key ] ) return cls . artigraph ( element = StructTypeAdapter . to_artigraph ( type_ , hints = hints , type_system = type_system ), ** kwargs , ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , types . Struct ) return cls . system ( StructTypeAdapter . to_system ( type_ . element , hints = hints , type_system = type_system ), metadata = { \"artigraph\" : json . dumps ( { \"name\" : type_ . name , \"partition_by\" : type_ . partition_by , \"cluster_by\" : type_ . cluster_by , } ) } , ) Methods system def system ( ... ) schema(fields, metadata=None) Construct pyarrow.Schema from collection of fields. Parameters: Name Type Description Default fields iterable of Fields or tuples, or mapping of strings to DataTypes None None metadata dict, default None Keys and values must be coercible to bytes. None Returns: Type Description pyarrow.Schema None StructTypeAdapter class StructTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class StructTypeAdapter ( _PyarrowTypeAdapter ) : artigraph = types . Struct system = pa . struct @classmethod def _field_to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : ret = type_system . to_artigraph ( type_ . type , hints = hints ) if type_ . nullable != ret . nullable : # Avoid setting nullable if matching to minimize repr ret = ret . copy ( update = { \"nullable\" : type_ . nullable } ) return ret @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( fields = { field . name : cls . _field_to_artigraph ( field , hints = hints , type_system = type_system ) for field in type_ } ) @classmethod def _field_to_system ( cls , name : str , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return pa . field ( name , type_system . to_system ( type_ , hints = hints ), nullable = type_ . nullable ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( [ cls._field_to_system(name, subtype, hints=hints, type_system=type_system) for name, subtype in type_.fields.items() ] ) Ancestors (in MRO) arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter Class variables artigraph key priority Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , pa . DataType ) and cls . _is_system ( type_ ) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( fields = { field . name : cls . _field_to_artigraph ( field , hints = hints , type_system = type_system ) for field in type_ } ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( [ cls._field_to_system(name, subtype, hints=hints, type_system=type_system) for name, subtype in type_.fields.items() ] ) Methods system def system ( ... ) struct(fields) Create StructType instance from fields. A struct is a nested type parameterized by an ordered sequence of types (which can all be distinct), called its fields. Parameters: Name Type Description Default fields iterable of Fields or tuples, or mapping of strings to DataTypes Each field must have a UTF8-encoded name, and these field names are part of the type metadata. None Returns: Type Description DataType None Time32TypeAdapter class Time32TypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class Time32TypeAdapter ( _BaseSizedTimeTypeAdapter ) : precision_to_unit = { \"second\" : \"s\" , \"millisecond\" : \"ms\" , } system = pa . time32 Ancestors (in MRO) arti.types.pyarrow._BaseSizedTimeTypeAdapter arti.types.pyarrow._BaseTimeTypeAdapter arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter Class variables artigraph key precision_to_unit priority unit_to_precision Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return ( super (). matches_artigraph ( type_ = type_ , hints = hints ) and type_ . precision in cls . precision_to_unit # type : ignore [ attr-defined ] ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return ( super (). matches_system ( type_ = type_ , hints = hints ) and type_ . unit in cls . unit_to_precision ) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : if ( precision : = cls . unit_to_precision . get ( type_ . unit )) is None : # pragma : no cover raise ValueError ( f \"{type_}.unit must be one of {tuple(cls.unit_to_precision)}, got {type_.unit}\" ) assert issubclass ( cls . artigraph , types . _TimeMixin ) return cls . artigraph ( precision = precision ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : precision = type_ . precision # type : ignore [ attr-defined ] if ( unit : = cls . precision_to_unit . get ( precision )) is None : # pragma : no cover raise ValueError ( f \"{type_}.precision must be one of {tuple(cls.precision_to_unit)}, got {precision}\" ) return cls . system ( unit ) Methods system def system ( ... ) time32(unit) Create instance of 32-bit time (time of day) type with unit resolution. Parameters: Name Type Description Default unit str one of 's' [second], or 'ms' [millisecond] None Returns: Type Description pyarrow.Time32Type None Time64TypeAdapter class Time64TypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class Time64TypeAdapter ( _BaseSizedTimeTypeAdapter ) : precision_to_unit = { \"microsecond\" : \"us\" , \"nanosecond\" : \"ns\" , } system = pa . time64 Ancestors (in MRO) arti.types.pyarrow._BaseSizedTimeTypeAdapter arti.types.pyarrow._BaseTimeTypeAdapter arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter Class variables artigraph key precision_to_unit priority unit_to_precision Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return ( super (). matches_artigraph ( type_ = type_ , hints = hints ) and type_ . precision in cls . precision_to_unit # type : ignore [ attr-defined ] ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return ( super (). matches_system ( type_ = type_ , hints = hints ) and type_ . unit in cls . unit_to_precision ) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : if ( precision : = cls . unit_to_precision . get ( type_ . unit )) is None : # pragma : no cover raise ValueError ( f \"{type_}.unit must be one of {tuple(cls.unit_to_precision)}, got {type_.unit}\" ) assert issubclass ( cls . artigraph , types . _TimeMixin ) return cls . artigraph ( precision = precision ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : precision = type_ . precision # type : ignore [ attr-defined ] if ( unit : = cls . precision_to_unit . get ( precision )) is None : # pragma : no cover raise ValueError ( f \"{type_}.precision must be one of {tuple(cls.precision_to_unit)}, got {precision}\" ) return cls . system ( unit ) Methods system def system ( ... ) time64(unit) Create instance of 64-bit time (time of day) type with unit resolution. Parameters: Name Type Description Default unit str One of 'us' [microsecond], or 'ns' [nanosecond]. None Returns: Type Description pyarrow.Time64Type None TimestampTypeAdapter class TimestampTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class TimestampTypeAdapter ( _BaseTimeTypeAdapter ) : artigraph = types . Timestamp system = pa . timestamp @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : tz = type_ . tz . upper () if tz != \"UTC\" : raise ValueError ( f \"Timestamp {type_}.tz must be in UTC, got {tz}\" ) return super (). to_artigraph ( type_ , hints = hints , type_system = type_system ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return super (). matches_system ( type_ , hints = hints ) and type_ . tz is not None @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : ts = super (). to_system ( type_ , hints = hints , type_system = type_system ) return cls . system ( ts . unit , \"UTC\" ) Ancestors (in MRO) arti.types.pyarrow._BaseTimeTypeAdapter arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter Class variables artigraph key precision_to_unit priority unit_to_precision Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return super (). matches_system ( type_ , hints = hints ) and type_ . tz is not None to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : tz = type_ . tz . upper () if tz != \"UTC\" : raise ValueError ( f \"Timestamp {type_}.tz must be in UTC, got {tz}\" ) return super (). to_artigraph ( type_ , hints = hints , type_system = type_system ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : ts = super (). to_system ( type_ , hints = hints , type_system = type_system ) return cls . system ( ts . unit , \"UTC\" ) Methods system def system ( ... ) timestamp(unit, tz=None) Create instance of timestamp type with resolution and optional time zone. Parameters: Name Type Description Default unit str one of 's' [second], 'ms' [millisecond], 'us' [microsecond], or 'ns' [nanosecond] None tz str, default None Time zone name. None indicates time zone naive None Returns: Type Description TimestampType None","title":"Pyarrow"},{"location":"reference/arti/types/pyarrow/#module-artitypespyarrow","text":"None None View Source from __future__ import annotations import json from collections.abc import Callable from typing import Any , cast import pyarrow as pa from arti import Type , TypeAdapter , TypeSystem , types from arti.internal.utils import classproperty pyarrow_type_system = TypeSystem ( key = \"pyarrow\" ) # Not implemented: # decimal128(int precision, int scale=0), # dictionary(index_type, value_type, \u2026), # large_binary(), # large_list(value_type), # large_string(), class _PyarrowTypeAdapter ( TypeAdapter ): @classproperty def _is_system ( cls ) -> Callable [[ pa . DataType ], bool ]: return getattr ( pa . types , f \"is_ { cls . system . __name__ } \" ) # type: ignore[no-any-return] @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return cls . artigraph () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , pa . DataType ) and cls . _is_system ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : return cls . system () def _gen_adapter ( * , artigraph : type [ Type ], system : Any , priority : int = 0 ) -> type [ TypeAdapter ]: return pyarrow_type_system . register_adapter ( type ( f \"Pyarrow { system . __name__ } \" , ( _PyarrowTypeAdapter ,), { \"artigraph\" : artigraph , \"system\" : system , \"priority\" : priority }, ) ) _gen_adapter ( artigraph = types . String , system = pa . string ) _gen_adapter ( artigraph = types . Null , system = pa . null ) # Date matching requires `priority=_precision` since it is not 1:1, but the float/int ones are. for _precision in ( 32 , 64 ): _gen_adapter ( artigraph = types . Date , system = getattr ( pa , f \"date { _precision } \" ), priority = _precision , ) for _precision in ( 16 , 32 , 64 ): _gen_adapter ( artigraph = getattr ( types , f \"Float { _precision } \" ), system = getattr ( pa , f \"float { _precision } \" ), ) for _precision in ( 8 , 16 , 32 , 64 ): _gen_adapter ( artigraph = getattr ( types , f \"Int { _precision } \" ), system = getattr ( pa , f \"int { _precision } \" ), ) _gen_adapter ( artigraph = getattr ( types , f \"UInt { _precision } \" ), system = getattr ( pa , f \"uint { _precision } \" ), ) @pyarrow_type_system . register_adapter class BinaryTypeAdapter ( _PyarrowTypeAdapter ): artigraph = types . Binary system = pa . binary @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : if isinstance ( type_ , pa . FixedSizeBinaryType ): return cls . artigraph ( byte_size = type_ . byte_width ) return cls . artigraph () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : # pa.binary returns a DataType(binary) when length=-1, otherwise a FixedSizeBinaryType... # but pa.types.is_binary only checks for DataType(binary). return super () . matches_system ( type_ , hints = hints ) or pa . types . is_fixed_size_binary ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( length =- 1 if type_ . byte_size is None else type_ . byte_size ) # The pyarrow bool constructor and checker have different names @pyarrow_type_system . register_adapter class BoolTypeAdapter ( _PyarrowTypeAdapter ): artigraph = types . Boolean system = pa . bool_ @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return cast ( bool , pa . types . is_boolean ( type_ )) @pyarrow_type_system . register_adapter class GeographyTypeAdapter ( _PyarrowTypeAdapter ): # TODO: Can we do something with pa.field metadata to round trip (eg: format, srid, etc) or # infer GeoParquet? artigraph = types . Geography system = pa . string # or pa.binary if geography.format == \"WKB\" @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : # We don't have any metadata to differentiate normal strings from geographies, so avoid # matching. This will prevent round tripping. return False @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return pa . binary () if type_ . format == \"WKB\" else pa . string () @pyarrow_type_system . register_adapter class ListTypeAdapter ( _PyarrowTypeAdapter ): artigraph = types . List system = pa . list_ @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return cls . artigraph ( element = type_system . to_artigraph ( type_ . value_type , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return cast ( bool , pa . types . is_list ( type_ )) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( value_type = type_system . to_system ( type_ . element , hints = hints )) @pyarrow_type_system . register_adapter class MapTypeAdapter ( _PyarrowTypeAdapter ): artigraph = types . Map system = pa . map_ @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return cls . artigraph ( key = type_system . to_artigraph ( type_ . key_type , hints = hints ), value = type_system . to_artigraph ( type_ . item_type , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return cast ( bool , pa . types . is_map ( type_ )) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( key_type = type_system . to_system ( type_ . key , hints = hints ), item_type = type_system . to_system ( type_ . value , hints = hints ), ) @pyarrow_type_system . register_adapter class StructTypeAdapter ( _PyarrowTypeAdapter ): artigraph = types . Struct system = pa . struct @classmethod def _field_to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : ret = type_system . to_artigraph ( type_ . type , hints = hints ) if type_ . nullable != ret . nullable : # Avoid setting nullable if matching to minimize repr ret = ret . copy ( update = { \"nullable\" : type_ . nullable }) return ret @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return cls . artigraph ( fields = { field . name : cls . _field_to_artigraph ( field , hints = hints , type_system = type_system ) for field in type_ } ) @classmethod def _field_to_system ( cls , name : str , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : return pa . field ( name , type_system . to_system ( type_ , hints = hints ), nullable = type_ . nullable ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( [ cls . _field_to_system ( name , subtype , hints = hints , type_system = type_system ) for name , subtype in type_ . fields . items () ] ) # NOTE: pa.schema and pa.struct are structurally similar, but pa.schema has additional attributes # (eg: .metadata) and cannot be nested (like Collection). @pyarrow_type_system . register_adapter class SchemaTypeAdapter ( _PyarrowTypeAdapter ): artigraph = types . Collection system = pa . schema priority = ListTypeAdapter . priority + 1 @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : # Collection can hold arbitrary types, but `pa.schema` is only a struct (but with arbitrary # metadata) return super () . matches_artigraph ( type_ = type_ , hints = hints ) and isinstance ( type_ . element , types . Struct # type: ignore[attr-defined] ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : kwargs = {} # NOTE: pyarrow converts all metadata keys/values to bytes if type_ . metadata and b \"artigraph\" in type_ . metadata : kwargs = json . loads ( type_ . metadata [ b \"artigraph\" ] . decode ()) for key in [ \"partition_by\" , \"cluster_by\" ]: if key in kwargs : # pragma: no cover kwargs [ key ] = tuple ( kwargs [ key ]) return cls . artigraph ( element = StructTypeAdapter . to_artigraph ( type_ , hints = hints , type_system = type_system ), ** kwargs , ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , pa . lib . Schema ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , types . Struct ) return cls . system ( StructTypeAdapter . to_system ( type_ . element , hints = hints , type_system = type_system ), metadata = { \"artigraph\" : json . dumps ( { \"name\" : type_ . name , \"partition_by\" : type_ . partition_by , \"cluster_by\" : type_ . cluster_by , } ) }, ) class _BaseTimeTypeAdapter ( _PyarrowTypeAdapter ): precision_to_unit = { \"second\" : \"s\" , \"millisecond\" : \"ms\" , \"microsecond\" : \"us\" , \"nanosecond\" : \"ns\" , } @classproperty def unit_to_precision ( cls ) -> dict [ str , str ]: return { v : k for k , v in cls . precision_to_unit . items ()} @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : if ( precision := cls . unit_to_precision . get ( type_ . unit )) is None : # pragma: no cover raise ValueError ( f \" { type_ } .unit must be one of { tuple ( cls . unit_to_precision ) } , got { type_ . unit } \" ) assert issubclass ( cls . artigraph , types . _TimeMixin ) return cls . artigraph ( precision = precision ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : precision = type_ . precision # type: ignore[attr-defined] if ( unit := cls . precision_to_unit . get ( precision )) is None : # pragma: no cover raise ValueError ( f \" { type_ } .precision must be one of { tuple ( cls . precision_to_unit ) } , got { precision } \" ) return cls . system ( unit ) @pyarrow_type_system . register_adapter class DateTimeTypeAdapter ( _BaseTimeTypeAdapter ): artigraph = types . DateTime system = pa . timestamp @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return super () . matches_system ( type_ , hints = hints ) and type_ . tz is None @pyarrow_type_system . register_adapter class TimestampTypeAdapter ( _BaseTimeTypeAdapter ): artigraph = types . Timestamp system = pa . timestamp @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : tz = type_ . tz . upper () if tz != \"UTC\" : raise ValueError ( f \"Timestamp { type_ } .tz must be in UTC, got { tz } \" ) return super () . to_artigraph ( type_ , hints = hints , type_system = type_system ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return super () . matches_system ( type_ , hints = hints ) and type_ . tz is not None @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : ts = super () . to_system ( type_ , hints = hints , type_system = type_system ) return cls . system ( ts . unit , \"UTC\" ) class _BaseSizedTimeTypeAdapter ( _BaseTimeTypeAdapter ): artigraph = types . Time @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : return ( super () . matches_artigraph ( type_ = type_ , hints = hints ) and type_ . precision in cls . precision_to_unit # type: ignore[attr-defined] ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return ( super () . matches_system ( type_ = type_ , hints = hints ) and type_ . unit in cls . unit_to_precision ) @pyarrow_type_system . register_adapter class Time32TypeAdapter ( _BaseSizedTimeTypeAdapter ): precision_to_unit = { \"second\" : \"s\" , \"millisecond\" : \"ms\" , } system = pa . time32 @pyarrow_type_system . register_adapter class Time64TypeAdapter ( _BaseSizedTimeTypeAdapter ): precision_to_unit = { \"microsecond\" : \"us\" , \"nanosecond\" : \"ns\" , } system = pa . time64","title":"Module arti.types.pyarrow"},{"location":"reference/arti/types/pyarrow/#variables","text":"pyarrow_type_system","title":"Variables"},{"location":"reference/arti/types/pyarrow/#classes","text":"","title":"Classes"},{"location":"reference/arti/types/pyarrow/#binarytypeadapter","text":"class BinaryTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class BinaryTypeAdapter ( _PyarrowTypeAdapter ) : artigraph = types . Binary system = pa . binary @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : if isinstance ( type_ , pa . FixedSizeBinaryType ) : return cls . artigraph ( byte_size = type_ . byte_width ) return cls . artigraph () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # pa . binary returns a DataType ( binary ) when length =- 1 , otherwise a FixedSizeBinaryType ... # but pa . types . is_binary only checks for DataType ( binary ). return super (). matches_system ( type_ , hints = hints ) or pa . types . is_fixed_size_binary ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( length =- 1 if type_ . byte_size is None else type_ . byte_size )","title":"BinaryTypeAdapter"},{"location":"reference/arti/types/pyarrow/#ancestors-in-mro","text":"arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/pyarrow/#class-variables","text":"artigraph key priority","title":"Class variables"},{"location":"reference/arti/types/pyarrow/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/types/pyarrow/#matches_artigraph","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/pyarrow/#matches_system","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # pa . binary returns a DataType ( binary ) when length =- 1 , otherwise a FixedSizeBinaryType ... # but pa . types . is_binary only checks for DataType ( binary ). return super (). matches_system ( type_ , hints = hints ) or pa . types . is_fixed_size_binary ( type_ )","title":"matches_system"},{"location":"reference/arti/types/pyarrow/#to_artigraph","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : if isinstance ( type_ , pa . FixedSizeBinaryType ) : return cls . artigraph ( byte_size = type_ . byte_width ) return cls . artigraph ()","title":"to_artigraph"},{"location":"reference/arti/types/pyarrow/#to_system","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( length =- 1 if type_ . byte_size is None else type_ . byte_size )","title":"to_system"},{"location":"reference/arti/types/pyarrow/#methods","text":"","title":"Methods"},{"location":"reference/arti/types/pyarrow/#system","text":"def system ( ... ) binary(int length=-1) Create variable-length binary type. Parameters: Name Type Description Default length int, optional, default -1 If length == -1 then return a variable length binary type. If length is greater than or equal to 0 then return a fixed size binary type of width length . None","title":"system"},{"location":"reference/arti/types/pyarrow/#booltypeadapter","text":"class BoolTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class BoolTypeAdapter ( _PyarrowTypeAdapter ) : artigraph = types . Boolean system = pa . bool_ @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return cast ( bool , pa . types . is_boolean ( type_ ))","title":"BoolTypeAdapter"},{"location":"reference/arti/types/pyarrow/#ancestors-in-mro_1","text":"arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/pyarrow/#class-variables_1","text":"artigraph key priority","title":"Class variables"},{"location":"reference/arti/types/pyarrow/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/types/pyarrow/#matches_artigraph_1","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/pyarrow/#matches_system_1","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return cast ( bool , pa . types . is_boolean ( type_ ))","title":"matches_system"},{"location":"reference/arti/types/pyarrow/#to_artigraph_1","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ()","title":"to_artigraph"},{"location":"reference/arti/types/pyarrow/#to_system_1","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return cls . system ()","title":"to_system"},{"location":"reference/arti/types/pyarrow/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/types/pyarrow/#system_1","text":"def system ( ... ) bool_() Create instance of boolean type.","title":"system"},{"location":"reference/arti/types/pyarrow/#datetimetypeadapter","text":"class DateTimeTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class DateTimeTypeAdapter ( _BaseTimeTypeAdapter ) : artigraph = types . DateTime system = pa . timestamp @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return super (). matches_system ( type_ , hints = hints ) and type_ . tz is None","title":"DateTimeTypeAdapter"},{"location":"reference/arti/types/pyarrow/#ancestors-in-mro_2","text":"arti.types.pyarrow._BaseTimeTypeAdapter arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/pyarrow/#class-variables_2","text":"artigraph key precision_to_unit priority unit_to_precision","title":"Class variables"},{"location":"reference/arti/types/pyarrow/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/arti/types/pyarrow/#matches_artigraph_2","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/pyarrow/#matches_system_2","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return super (). matches_system ( type_ , hints = hints ) and type_ . tz is None","title":"matches_system"},{"location":"reference/arti/types/pyarrow/#to_artigraph_2","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : if ( precision : = cls . unit_to_precision . get ( type_ . unit )) is None : # pragma : no cover raise ValueError ( f \"{type_}.unit must be one of {tuple(cls.unit_to_precision)}, got {type_.unit}\" ) assert issubclass ( cls . artigraph , types . _TimeMixin ) return cls . artigraph ( precision = precision )","title":"to_artigraph"},{"location":"reference/arti/types/pyarrow/#to_system_2","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : precision = type_ . precision # type : ignore [ attr-defined ] if ( unit : = cls . precision_to_unit . get ( precision )) is None : # pragma : no cover raise ValueError ( f \"{type_}.precision must be one of {tuple(cls.precision_to_unit)}, got {precision}\" ) return cls . system ( unit )","title":"to_system"},{"location":"reference/arti/types/pyarrow/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/types/pyarrow/#system_2","text":"def system ( ... ) timestamp(unit, tz=None) Create instance of timestamp type with resolution and optional time zone. Parameters: Name Type Description Default unit str one of 's' [second], 'ms' [millisecond], 'us' [microsecond], or 'ns' [nanosecond] None tz str, default None Time zone name. None indicates time zone naive None Returns: Type Description TimestampType None","title":"system"},{"location":"reference/arti/types/pyarrow/#geographytypeadapter","text":"class GeographyTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class GeographyTypeAdapter ( _PyarrowTypeAdapter ) : # TODO : Can we do something with pa . field metadata to round trip ( eg : format , srid , etc ) or # infer GeoParquet ? artigraph = types . Geography system = pa . string # or pa . binary if geography . format == \"WKB\" @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # We don ' t have any metadata to differentiate normal strings from geographies , so avoid # matching . This will prevent round tripping . return False @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return pa . binary () if type_ . format == \"WKB\" else pa . string ()","title":"GeographyTypeAdapter"},{"location":"reference/arti/types/pyarrow/#ancestors-in-mro_3","text":"arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/pyarrow/#class-variables_3","text":"artigraph key priority","title":"Class variables"},{"location":"reference/arti/types/pyarrow/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/arti/types/pyarrow/#matches_artigraph_3","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/pyarrow/#matches_system_3","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # We don ' t have any metadata to differentiate normal strings from geographies , so avoid # matching . This will prevent round tripping . return False","title":"matches_system"},{"location":"reference/arti/types/pyarrow/#to_artigraph_3","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ()","title":"to_artigraph"},{"location":"reference/arti/types/pyarrow/#to_system_3","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return pa . binary () if type_ . format == \"WKB\" else pa . string ()","title":"to_system"},{"location":"reference/arti/types/pyarrow/#methods_3","text":"","title":"Methods"},{"location":"reference/arti/types/pyarrow/#system_3","text":"def system ( ... ) string() Create UTF8 variable-length string type.","title":"system"},{"location":"reference/arti/types/pyarrow/#listtypeadapter","text":"class ListTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class ListTypeAdapter ( _PyarrowTypeAdapter ) : artigraph = types . List system = pa . list_ @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( element = type_system . to_artigraph ( type_ . value_type , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return cast ( bool , pa . types . is_list ( type_ )) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( value_type = type_system . to_system ( type_ . element , hints = hints ))","title":"ListTypeAdapter"},{"location":"reference/arti/types/pyarrow/#ancestors-in-mro_4","text":"arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/pyarrow/#class-variables_4","text":"artigraph key priority","title":"Class variables"},{"location":"reference/arti/types/pyarrow/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/arti/types/pyarrow/#matches_artigraph_4","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/pyarrow/#matches_system_4","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return cast ( bool , pa . types . is_list ( type_ ))","title":"matches_system"},{"location":"reference/arti/types/pyarrow/#to_artigraph_4","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( element = type_system . to_artigraph ( type_ . value_type , hints = hints ), )","title":"to_artigraph"},{"location":"reference/arti/types/pyarrow/#to_system_4","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( value_type = type_system . to_system ( type_ . element , hints = hints ))","title":"to_system"},{"location":"reference/arti/types/pyarrow/#methods_4","text":"","title":"Methods"},{"location":"reference/arti/types/pyarrow/#system_4","text":"def system ( ... ) list_(value_type, int list_size=-1) Create ListType instance from child data type or field. Parameters: Name Type Description Default value_type DataType or Field None None list_size int, optional, default -1 If length == -1 then return a variable length list type. If length is greater than or equal to 0 then return a fixed size list type. None Returns: Type Description DataType None","title":"system"},{"location":"reference/arti/types/pyarrow/#maptypeadapter","text":"class MapTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class MapTypeAdapter ( _PyarrowTypeAdapter ) : artigraph = types . Map system = pa . map_ @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( key = type_system . to_artigraph ( type_ . key_type , hints = hints ), value = type_system . to_artigraph ( type_ . item_type , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return cast ( bool , pa . types . is_map ( type_ )) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( key_type = type_system . to_system ( type_ . key , hints = hints ), item_type = type_system . to_system ( type_ . value , hints = hints ), )","title":"MapTypeAdapter"},{"location":"reference/arti/types/pyarrow/#ancestors-in-mro_5","text":"arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/pyarrow/#class-variables_5","text":"artigraph key priority","title":"Class variables"},{"location":"reference/arti/types/pyarrow/#static-methods_5","text":"","title":"Static methods"},{"location":"reference/arti/types/pyarrow/#matches_artigraph_5","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/pyarrow/#matches_system_5","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return cast ( bool , pa . types . is_map ( type_ ))","title":"matches_system"},{"location":"reference/arti/types/pyarrow/#to_artigraph_5","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( key = type_system . to_artigraph ( type_ . key_type , hints = hints ), value = type_system . to_artigraph ( type_ . item_type , hints = hints ), )","title":"to_artigraph"},{"location":"reference/arti/types/pyarrow/#to_system_5","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( key_type = type_system . to_system ( type_ . key , hints = hints ), item_type = type_system . to_system ( type_ . value , hints = hints ), )","title":"to_system"},{"location":"reference/arti/types/pyarrow/#methods_5","text":"","title":"Methods"},{"location":"reference/arti/types/pyarrow/#system_5","text":"def system ( ... ) map_(key_type, item_type, keys_sorted=False) -> MapType Create MapType instance from key and item data types or fields. Parameters: Name Type Description Default key_type DataType None None item_type DataType None None keys_sorted bool None None Returns: Type Description DataType None","title":"system"},{"location":"reference/arti/types/pyarrow/#schematypeadapter","text":"class SchemaTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class SchemaTypeAdapter ( _PyarrowTypeAdapter ) : artigraph = types . Collection system = pa . schema priority = ListTypeAdapter . priority + 1 @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : # Collection can hold arbitrary types , but ` pa . schema ` is only a struct ( but with arbitrary # metadata ) return super (). matches_artigraph ( type_ = type_ , hints = hints ) and isinstance ( type_ . element , types . Struct # type : ignore [ attr-defined ] ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : kwargs = {} # NOTE : pyarrow converts all metadata keys / values to bytes if type_ . metadata and b \"artigraph\" in type_ . metadata : kwargs = json . loads ( type_ . metadata [ b\"artigraph\" ] . decode ()) for key in [ \"partition_by\", \"cluster_by\" ] : if key in kwargs : # pragma : no cover kwargs [ key ] = tuple ( kwargs [ key ] ) return cls . artigraph ( element = StructTypeAdapter . to_artigraph ( type_ , hints = hints , type_system = type_system ), ** kwargs , ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , pa . lib . Schema ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , types . Struct ) return cls . system ( StructTypeAdapter . to_system ( type_ . element , hints = hints , type_system = type_system ), metadata = { \"artigraph\" : json . dumps ( { \"name\" : type_ . name , \"partition_by\" : type_ . partition_by , \"cluster_by\" : type_ . cluster_by , } ) } , )","title":"SchemaTypeAdapter"},{"location":"reference/arti/types/pyarrow/#ancestors-in-mro_6","text":"arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/pyarrow/#class-variables_6","text":"artigraph key priority","title":"Class variables"},{"location":"reference/arti/types/pyarrow/#static-methods_6","text":"","title":"Static methods"},{"location":"reference/arti/types/pyarrow/#matches_artigraph_6","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : # Collection can hold arbitrary types , but ` pa . schema ` is only a struct ( but with arbitrary # metadata ) return super (). matches_artigraph ( type_ = type_ , hints = hints ) and isinstance ( type_ . element , types . Struct # type : ignore [ attr-defined ] )","title":"matches_artigraph"},{"location":"reference/arti/types/pyarrow/#matches_system_6","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , pa . lib . Schema )","title":"matches_system"},{"location":"reference/arti/types/pyarrow/#to_artigraph_6","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : kwargs = {} # NOTE : pyarrow converts all metadata keys / values to bytes if type_ . metadata and b \"artigraph\" in type_ . metadata : kwargs = json . loads ( type_ . metadata [ b\"artigraph\" ] . decode ()) for key in [ \"partition_by\", \"cluster_by\" ] : if key in kwargs : # pragma : no cover kwargs [ key ] = tuple ( kwargs [ key ] ) return cls . artigraph ( element = StructTypeAdapter . to_artigraph ( type_ , hints = hints , type_system = type_system ), ** kwargs , )","title":"to_artigraph"},{"location":"reference/arti/types/pyarrow/#to_system_6","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) assert isinstance ( type_ . element , types . Struct ) return cls . system ( StructTypeAdapter . to_system ( type_ . element , hints = hints , type_system = type_system ), metadata = { \"artigraph\" : json . dumps ( { \"name\" : type_ . name , \"partition_by\" : type_ . partition_by , \"cluster_by\" : type_ . cluster_by , } ) } , )","title":"to_system"},{"location":"reference/arti/types/pyarrow/#methods_6","text":"","title":"Methods"},{"location":"reference/arti/types/pyarrow/#system_6","text":"def system ( ... ) schema(fields, metadata=None) Construct pyarrow.Schema from collection of fields. Parameters: Name Type Description Default fields iterable of Fields or tuples, or mapping of strings to DataTypes None None metadata dict, default None Keys and values must be coercible to bytes. None Returns: Type Description pyarrow.Schema None","title":"system"},{"location":"reference/arti/types/pyarrow/#structtypeadapter","text":"class StructTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class StructTypeAdapter ( _PyarrowTypeAdapter ) : artigraph = types . Struct system = pa . struct @classmethod def _field_to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : ret = type_system . to_artigraph ( type_ . type , hints = hints ) if type_ . nullable != ret . nullable : # Avoid setting nullable if matching to minimize repr ret = ret . copy ( update = { \"nullable\" : type_ . nullable } ) return ret @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( fields = { field . name : cls . _field_to_artigraph ( field , hints = hints , type_system = type_system ) for field in type_ } ) @classmethod def _field_to_system ( cls , name : str , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return pa . field ( name , type_system . to_system ( type_ , hints = hints ), nullable = type_ . nullable ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( [ cls._field_to_system(name, subtype, hints=hints, type_system=type_system) for name, subtype in type_.fields.items() ] )","title":"StructTypeAdapter"},{"location":"reference/arti/types/pyarrow/#ancestors-in-mro_7","text":"arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/pyarrow/#class-variables_7","text":"artigraph key priority","title":"Class variables"},{"location":"reference/arti/types/pyarrow/#static-methods_7","text":"","title":"Static methods"},{"location":"reference/arti/types/pyarrow/#matches_artigraph_7","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/pyarrow/#matches_system_7","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , pa . DataType ) and cls . _is_system ( type_ )","title":"matches_system"},{"location":"reference/arti/types/pyarrow/#to_artigraph_7","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( fields = { field . name : cls . _field_to_artigraph ( field , hints = hints , type_system = type_system ) for field in type_ } )","title":"to_artigraph"},{"location":"reference/arti/types/pyarrow/#to_system_7","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system ( [ cls._field_to_system(name, subtype, hints=hints, type_system=type_system) for name, subtype in type_.fields.items() ] )","title":"to_system"},{"location":"reference/arti/types/pyarrow/#methods_7","text":"","title":"Methods"},{"location":"reference/arti/types/pyarrow/#system_7","text":"def system ( ... ) struct(fields) Create StructType instance from fields. A struct is a nested type parameterized by an ordered sequence of types (which can all be distinct), called its fields. Parameters: Name Type Description Default fields iterable of Fields or tuples, or mapping of strings to DataTypes Each field must have a UTF8-encoded name, and these field names are part of the type metadata. None Returns: Type Description DataType None","title":"system"},{"location":"reference/arti/types/pyarrow/#time32typeadapter","text":"class Time32TypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class Time32TypeAdapter ( _BaseSizedTimeTypeAdapter ) : precision_to_unit = { \"second\" : \"s\" , \"millisecond\" : \"ms\" , } system = pa . time32","title":"Time32TypeAdapter"},{"location":"reference/arti/types/pyarrow/#ancestors-in-mro_8","text":"arti.types.pyarrow._BaseSizedTimeTypeAdapter arti.types.pyarrow._BaseTimeTypeAdapter arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/pyarrow/#class-variables_8","text":"artigraph key precision_to_unit priority unit_to_precision","title":"Class variables"},{"location":"reference/arti/types/pyarrow/#static-methods_8","text":"","title":"Static methods"},{"location":"reference/arti/types/pyarrow/#matches_artigraph_8","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return ( super (). matches_artigraph ( type_ = type_ , hints = hints ) and type_ . precision in cls . precision_to_unit # type : ignore [ attr-defined ] )","title":"matches_artigraph"},{"location":"reference/arti/types/pyarrow/#matches_system_8","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return ( super (). matches_system ( type_ = type_ , hints = hints ) and type_ . unit in cls . unit_to_precision )","title":"matches_system"},{"location":"reference/arti/types/pyarrow/#to_artigraph_8","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : if ( precision : = cls . unit_to_precision . get ( type_ . unit )) is None : # pragma : no cover raise ValueError ( f \"{type_}.unit must be one of {tuple(cls.unit_to_precision)}, got {type_.unit}\" ) assert issubclass ( cls . artigraph , types . _TimeMixin ) return cls . artigraph ( precision = precision )","title":"to_artigraph"},{"location":"reference/arti/types/pyarrow/#to_system_8","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : precision = type_ . precision # type : ignore [ attr-defined ] if ( unit : = cls . precision_to_unit . get ( precision )) is None : # pragma : no cover raise ValueError ( f \"{type_}.precision must be one of {tuple(cls.precision_to_unit)}, got {precision}\" ) return cls . system ( unit )","title":"to_system"},{"location":"reference/arti/types/pyarrow/#methods_8","text":"","title":"Methods"},{"location":"reference/arti/types/pyarrow/#system_8","text":"def system ( ... ) time32(unit) Create instance of 32-bit time (time of day) type with unit resolution. Parameters: Name Type Description Default unit str one of 's' [second], or 'ms' [millisecond] None Returns: Type Description pyarrow.Time32Type None","title":"system"},{"location":"reference/arti/types/pyarrow/#time64typeadapter","text":"class Time64TypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class Time64TypeAdapter ( _BaseSizedTimeTypeAdapter ) : precision_to_unit = { \"microsecond\" : \"us\" , \"nanosecond\" : \"ns\" , } system = pa . time64","title":"Time64TypeAdapter"},{"location":"reference/arti/types/pyarrow/#ancestors-in-mro_9","text":"arti.types.pyarrow._BaseSizedTimeTypeAdapter arti.types.pyarrow._BaseTimeTypeAdapter arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/pyarrow/#class-variables_9","text":"artigraph key precision_to_unit priority unit_to_precision","title":"Class variables"},{"location":"reference/arti/types/pyarrow/#static-methods_9","text":"","title":"Static methods"},{"location":"reference/arti/types/pyarrow/#matches_artigraph_9","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return ( super (). matches_artigraph ( type_ = type_ , hints = hints ) and type_ . precision in cls . precision_to_unit # type : ignore [ attr-defined ] )","title":"matches_artigraph"},{"location":"reference/arti/types/pyarrow/#matches_system_9","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return ( super (). matches_system ( type_ = type_ , hints = hints ) and type_ . unit in cls . unit_to_precision )","title":"matches_system"},{"location":"reference/arti/types/pyarrow/#to_artigraph_9","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : if ( precision : = cls . unit_to_precision . get ( type_ . unit )) is None : # pragma : no cover raise ValueError ( f \"{type_}.unit must be one of {tuple(cls.unit_to_precision)}, got {type_.unit}\" ) assert issubclass ( cls . artigraph , types . _TimeMixin ) return cls . artigraph ( precision = precision )","title":"to_artigraph"},{"location":"reference/arti/types/pyarrow/#to_system_9","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : precision = type_ . precision # type : ignore [ attr-defined ] if ( unit : = cls . precision_to_unit . get ( precision )) is None : # pragma : no cover raise ValueError ( f \"{type_}.precision must be one of {tuple(cls.precision_to_unit)}, got {precision}\" ) return cls . system ( unit )","title":"to_system"},{"location":"reference/arti/types/pyarrow/#methods_9","text":"","title":"Methods"},{"location":"reference/arti/types/pyarrow/#system_9","text":"def system ( ... ) time64(unit) Create instance of 64-bit time (time of day) type with unit resolution. Parameters: Name Type Description Default unit str One of 'us' [microsecond], or 'ns' [nanosecond]. None Returns: Type Description pyarrow.Time64Type None","title":"system"},{"location":"reference/arti/types/pyarrow/#timestamptypeadapter","text":"class TimestampTypeAdapter ( / , * args , ** kwargs ) View Source @pyarrow_type_system . register_adapter class TimestampTypeAdapter ( _BaseTimeTypeAdapter ) : artigraph = types . Timestamp system = pa . timestamp @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : tz = type_ . tz . upper () if tz != \"UTC\" : raise ValueError ( f \"Timestamp {type_}.tz must be in UTC, got {tz}\" ) return super (). to_artigraph ( type_ , hints = hints , type_system = type_system ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return super (). matches_system ( type_ , hints = hints ) and type_ . tz is not None @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : ts = super (). to_system ( type_ , hints = hints , type_system = type_system ) return cls . system ( ts . unit , \"UTC\" )","title":"TimestampTypeAdapter"},{"location":"reference/arti/types/pyarrow/#ancestors-in-mro_10","text":"arti.types.pyarrow._BaseTimeTypeAdapter arti.types.pyarrow._PyarrowTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/pyarrow/#class-variables_10","text":"artigraph key precision_to_unit priority unit_to_precision","title":"Class variables"},{"location":"reference/arti/types/pyarrow/#static-methods_10","text":"","title":"Static methods"},{"location":"reference/arti/types/pyarrow/#matches_artigraph_10","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/pyarrow/#matches_system_10","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return super (). matches_system ( type_ , hints = hints ) and type_ . tz is not None","title":"matches_system"},{"location":"reference/arti/types/pyarrow/#to_artigraph_10","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : tz = type_ . tz . upper () if tz != \"UTC\" : raise ValueError ( f \"Timestamp {type_}.tz must be in UTC, got {tz}\" ) return super (). to_artigraph ( type_ , hints = hints , type_system = type_system )","title":"to_artigraph"},{"location":"reference/arti/types/pyarrow/#to_system_10","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : ts = super (). to_system ( type_ , hints = hints , type_system = type_system ) return cls . system ( ts . unit , \"UTC\" )","title":"to_system"},{"location":"reference/arti/types/pyarrow/#methods_10","text":"","title":"Methods"},{"location":"reference/arti/types/pyarrow/#system_10","text":"def system ( ... ) timestamp(unit, tz=None) Create instance of timestamp type with resolution and optional time zone. Parameters: Name Type Description Default unit str one of 's' [second], 'ms' [millisecond], 'us' [microsecond], or 'ns' [nanosecond] None tz str, default None Time zone name. None indicates time zone naive None Returns: Type Description TimestampType None","title":"system"},{"location":"reference/arti/types/pydantic/","text":"Module arti.types.pydantic None None View Source from __future__ import annotations from typing import Any , Protocol from pydantic import BaseModel from pydantic.fields import ModelField from pydantic.fields import UndefinedType as _PydanticUndefinedType from arti.internal.type_hints import lenient_issubclass from arti.types import Struct , Type , TypeAdapter , TypeSystem from arti.types.python import python_type_system pydantic_type_system = TypeSystem ( key = \"pydantic\" , extends = ( python_type_system ,)) class _PostFieldConversionHook ( Protocol ): def __call__ ( self , type_ : Type , * , name : str , required : bool ) -> Type : raise NotImplementedError () def get_post_field_conversion_hook ( type_ : Any ) -> _PostFieldConversionHook : if hasattr ( type_ , \"_pydantic_type_system_post_field_conversion_hook_\" ): return type_ . _pydantic_type_system_post_field_conversion_hook_ # type: ignore[no-any-return] return lambda type_ , * , name , required : type_ @pydantic_type_system . register_adapter class BaseModelAdapter ( TypeAdapter ): artigraph = Struct system = BaseModel @staticmethod def _field_to_artigraph ( field : ModelField , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : subtype = type_system . to_artigraph ( field . outer_type_ , hints = hints ) return get_post_field_conversion_hook ( subtype )( subtype , name = field . name , required = ( True if isinstance ( field . required , _PydanticUndefinedType ) else field . required ), ) @classmethod def to_artigraph ( cls , type_ : type [ BaseModel ], * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return Struct ( name = type_ . __name__ , fields = { field . name : cls . _field_to_artigraph ( field , hints = hints , type_system = type_system ) for field in type_ . __fields__ . values () }, ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return lenient_issubclass ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> type [ BaseModel ]: assert isinstance ( type_ , Struct ) return type ( f \" { type_ . name } \" , ( BaseModel ,), { \"__annotations__\" : { k : type_system . to_system ( v , hints = hints ) for k , v in type_ . fields . items () } }, ) Variables pydantic_type_system Functions get_post_field_conversion_hook def get_post_field_conversion_hook ( type_ : 'Any' ) -> '_PostFieldConversionHook' View Source def get_post_field_conversion_hook ( type_ : Any ) -> _PostFieldConversionHook : if hasattr ( type_ , \"_pydantic_type_system_post_field_conversion_hook_\" ) : return type_ . _pydantic_type_system_post_field_conversion_hook_ # type : ignore [ no - any - return ] return lambda type_ , * , name , required : type_ Classes BaseModelAdapter class BaseModelAdapter ( / , * args , ** kwargs ) View Source @pydantic_type_system . register_adapter class BaseModelAdapter ( TypeAdapter ) : artigraph = Struct system = BaseModel @staticmethod def _field_to_artigraph ( field : ModelField , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : subtype = type_system . to_artigraph ( field . outer_type_ , hints = hints ) return get_post_field_conversion_hook ( subtype )( subtype , name = field . name , required = ( True if isinstance ( field . required , _PydanticUndefinedType ) else field . required ), ) @classmethod def to_artigraph ( cls , type_ : type [ BaseModel ] , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return Struct ( name = type_ . __name__ , fields = { field . name : cls . _field_to_artigraph ( field , hints = hints , type_system = type_system ) for field in type_ . __fields__ . values () } , ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> type [ BaseModel ] : assert isinstance ( type_ , Struct ) return type ( f \"{type_.name}\" , ( BaseModel ,), { \"__annotations__\" : { k : type_system . to_system ( v , hints = hints ) for k , v in type_ . fields . items () } } , ) Ancestors (in MRO) arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( type_ , cls . system ) to_artigraph def to_artigraph ( type_ : 'type[BaseModel]' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : type [ BaseModel ] , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return Struct ( name = type_ . __name__ , fields = { field . name : cls . _field_to_artigraph ( field , hints = hints , type_system = type_system ) for field in type_ . __fields__ . values () } , ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'type[BaseModel]' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> type [ BaseModel ] : assert isinstance ( type_ , Struct ) return type ( f \"{type_.name}\" , ( BaseModel ,), { \"__annotations__\" : { k : type_system . to_system ( v , hints = hints ) for k , v in type_ . fields . items () } } , )","title":"Pydantic"},{"location":"reference/arti/types/pydantic/#module-artitypespydantic","text":"None None View Source from __future__ import annotations from typing import Any , Protocol from pydantic import BaseModel from pydantic.fields import ModelField from pydantic.fields import UndefinedType as _PydanticUndefinedType from arti.internal.type_hints import lenient_issubclass from arti.types import Struct , Type , TypeAdapter , TypeSystem from arti.types.python import python_type_system pydantic_type_system = TypeSystem ( key = \"pydantic\" , extends = ( python_type_system ,)) class _PostFieldConversionHook ( Protocol ): def __call__ ( self , type_ : Type , * , name : str , required : bool ) -> Type : raise NotImplementedError () def get_post_field_conversion_hook ( type_ : Any ) -> _PostFieldConversionHook : if hasattr ( type_ , \"_pydantic_type_system_post_field_conversion_hook_\" ): return type_ . _pydantic_type_system_post_field_conversion_hook_ # type: ignore[no-any-return] return lambda type_ , * , name , required : type_ @pydantic_type_system . register_adapter class BaseModelAdapter ( TypeAdapter ): artigraph = Struct system = BaseModel @staticmethod def _field_to_artigraph ( field : ModelField , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : subtype = type_system . to_artigraph ( field . outer_type_ , hints = hints ) return get_post_field_conversion_hook ( subtype )( subtype , name = field . name , required = ( True if isinstance ( field . required , _PydanticUndefinedType ) else field . required ), ) @classmethod def to_artigraph ( cls , type_ : type [ BaseModel ], * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return Struct ( name = type_ . __name__ , fields = { field . name : cls . _field_to_artigraph ( field , hints = hints , type_system = type_system ) for field in type_ . __fields__ . values () }, ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return lenient_issubclass ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> type [ BaseModel ]: assert isinstance ( type_ , Struct ) return type ( f \" { type_ . name } \" , ( BaseModel ,), { \"__annotations__\" : { k : type_system . to_system ( v , hints = hints ) for k , v in type_ . fields . items () } }, )","title":"Module arti.types.pydantic"},{"location":"reference/arti/types/pydantic/#variables","text":"pydantic_type_system","title":"Variables"},{"location":"reference/arti/types/pydantic/#functions","text":"","title":"Functions"},{"location":"reference/arti/types/pydantic/#get_post_field_conversion_hook","text":"def get_post_field_conversion_hook ( type_ : 'Any' ) -> '_PostFieldConversionHook' View Source def get_post_field_conversion_hook ( type_ : Any ) -> _PostFieldConversionHook : if hasattr ( type_ , \"_pydantic_type_system_post_field_conversion_hook_\" ) : return type_ . _pydantic_type_system_post_field_conversion_hook_ # type : ignore [ no - any - return ] return lambda type_ , * , name , required : type_","title":"get_post_field_conversion_hook"},{"location":"reference/arti/types/pydantic/#classes","text":"","title":"Classes"},{"location":"reference/arti/types/pydantic/#basemodeladapter","text":"class BaseModelAdapter ( / , * args , ** kwargs ) View Source @pydantic_type_system . register_adapter class BaseModelAdapter ( TypeAdapter ) : artigraph = Struct system = BaseModel @staticmethod def _field_to_artigraph ( field : ModelField , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : subtype = type_system . to_artigraph ( field . outer_type_ , hints = hints ) return get_post_field_conversion_hook ( subtype )( subtype , name = field . name , required = ( True if isinstance ( field . required , _PydanticUndefinedType ) else field . required ), ) @classmethod def to_artigraph ( cls , type_ : type [ BaseModel ] , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return Struct ( name = type_ . __name__ , fields = { field . name : cls . _field_to_artigraph ( field , hints = hints , type_system = type_system ) for field in type_ . __fields__ . values () } , ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> type [ BaseModel ] : assert isinstance ( type_ , Struct ) return type ( f \"{type_.name}\" , ( BaseModel ,), { \"__annotations__\" : { k : type_system . to_system ( v , hints = hints ) for k , v in type_ . fields . items () } } , )","title":"BaseModelAdapter"},{"location":"reference/arti/types/pydantic/#ancestors-in-mro","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/pydantic/#class-variables","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/pydantic/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/types/pydantic/#matches_artigraph","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/pydantic/#matches_system","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( type_ , cls . system )","title":"matches_system"},{"location":"reference/arti/types/pydantic/#to_artigraph","text":"def to_artigraph ( type_ : 'type[BaseModel]' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : type [ BaseModel ] , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return Struct ( name = type_ . __name__ , fields = { field . name : cls . _field_to_artigraph ( field , hints = hints , type_system = type_system ) for field in type_ . __fields__ . values () } , )","title":"to_artigraph"},{"location":"reference/arti/types/pydantic/#to_system","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'type[BaseModel]' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> type [ BaseModel ] : assert isinstance ( type_ , Struct ) return type ( f \"{type_.name}\" , ( BaseModel ,), { \"__annotations__\" : { k : type_system . to_system ( v , hints = hints ) for k , v in type_ . fields . items () } } , )","title":"to_system"},{"location":"reference/arti/types/python/","text":"Module arti.types.python None None View Source from __future__ import annotations import datetime from collections.abc import Mapping from functools import partial from itertools import chain from typing import Any , Literal , Optional , TypedDict , Union , get_args , get_origin , get_type_hints import arti.types from arti.internal.type_hints import ( NoneType , is_optional_hint , is_typeddict , is_union , lenient_issubclass , ) from arti.types import Type , TypeAdapter , TypeSystem , _ContainerMixin , _ScalarClassTypeAdapter python_type_system = TypeSystem ( key = \"python\" ) _generate = partial ( _ScalarClassTypeAdapter . generate , type_system = python_type_system ) _generate ( artigraph = arti . types . Binary , system = bytes ) # NOTE: issubclass(bool, int) is True, so set higher priority _generate ( artigraph = arti . types . Boolean , system = bool , priority = 1000 ) _generate ( artigraph = arti . types . Date , system = datetime . date ) _generate ( artigraph = arti . types . String , system = str ) for _precision in ( 16 , 32 , 64 ): _generate ( artigraph = getattr ( arti . types , f \"Float { _precision } \" ), system = float , priority = _precision , ) for _precision in ( 8 , 16 , 32 , 64 ): _generate ( artigraph = getattr ( arti . types , f \"Int { _precision } \" ), system = int , priority = _precision , ) @python_type_system . register_adapter class PyNone ( _ScalarClassTypeAdapter ): # Python represents None types in type hints with the `None` value (not `NoneType`). artigraph = arti . types . Null system = None @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return type_ is None @python_type_system . register_adapter class PyDatetime ( _ScalarClassTypeAdapter ): artigraph = arti . types . Timestamp system = datetime . datetime priority = 1 # Prioritize above Date (isinstance(datetime, date) is True) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return cls . artigraph ( precision = \"microsecond\" ) class PyValueContainer ( TypeAdapter ): @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : ( element ,) = get_args ( type_ ) assert issubclass ( cls . artigraph , _ContainerMixin ) return cls . artigraph ( element = type_system . to_artigraph ( element , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ type_system . to_system ( type_ . element , hints = hints )] # type: ignore[attr-defined] @python_type_system . register_adapter class PyList ( PyValueContainer ): artigraph = arti . types . List system = list priority = 1 # NOTE: PyTuple only covers sequences (eg: tuple[int, ...]), not structure (eg: tuple[int, str]). @python_type_system . register_adapter class PyTuple ( PyValueContainer ): artigraph = arti . types . List system = tuple @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : origin , args = get_origin ( type_ ), get_args ( type_ ) assert origin is not None assert len ( args ) == 2 assert args [ 1 ] is ... return super () . to_artigraph ( origin [ args [ 0 ]], hints = hints , type_system = type_system ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : if super () . matches_system ( type_ , hints = hints ): args = get_args ( type_ ) if len ( args ) == 2 and args [ 1 ] is ... : return True return False @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : ret = super () . to_system ( type_ , hints = hints , type_system = type_system ) origin , args = get_origin ( ret ), get_args ( ret ) assert origin is not None assert len ( args ) == 1 return origin [ args [ 0 ], ... ] @python_type_system . register_adapter class PyFrozenset ( PyValueContainer ): artigraph = arti . types . Set system = frozenset @python_type_system . register_adapter class PySet ( PyValueContainer ): artigraph = arti . types . Set system = set priority = 1 # Set above frozenset @python_type_system . register_adapter class PyLiteral ( TypeAdapter ): artigraph = arti . types . Enum system = Literal @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : origin , items = get_origin ( type_ ), get_args ( type_ ) if is_union ( origin ): assert not is_optional_hint ( type_ ) # Should be handled by PyOptional # We only support Enums currently, so all subtypes must be Literal if non_literals := [ sub for sub in items if get_origin ( sub ) is not Literal ]: raise NotImplementedError ( f \"Only Union[Literal[...], ...] (enums) are currently supported, got invalid subtypes: { non_literals } \" ) # Flatten Union[Literal[1], Literal[1,2,3]] origin , items = Literal , tuple ( chain . from_iterable ( get_args ( sub ) for sub in items )) assert origin is Literal assert isinstance ( items , tuple ) if len ( items ) == 0 : raise NotImplementedError ( f \"Invalid Literal with no values: { type_ } \" ) py_type , * other_types = ( type ( v ) for v in items ) if not all ( t is py_type for t in other_types ): raise ValueError ( \"All Literals must be the same type, got: {(py_type, *other_types)}\" ) return cls . artigraph ( type = type_system . to_artigraph ( py_type , hints = hints ), items = items ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : # We don't (currently) support arbitrary Unions, but can map Union[Literal[1], Literal[2]] # to an Enum. Python's Optional is also represented as a Union, but we handle that with the # high priority PyOptional. origin , items = get_origin ( type_ ), get_args ( type_ ) return origin is Literal or ( is_union ( origin ) and all ( get_origin ( sub ) is Literal for sub in items ) ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ tuple ( type_ . items )] @python_type_system . register_adapter class PyMap ( TypeAdapter ): artigraph = arti . types . Map system = dict @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : key , value = get_args ( type_ ) return cls . artigraph ( key = type_system . to_artigraph ( key , hints = hints ), value = type_system . to_artigraph ( value , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return lenient_issubclass ( get_origin ( type_ ), ( cls . system , Mapping )) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ # type: ignore[index] type_system . to_system ( type_ . key , hints = hints ), type_system . to_system ( type_ . value , hints = hints ), ] @python_type_system . register_adapter class PyOptional ( TypeAdapter ): artigraph = arti . types . Type # Check against isinstance *and* .nullable system = Optional # Set very high priority to intercept other matching arti.types.Types/py Union in order to set .nullable priority = int ( 1e9 ) @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : return super () . matches_artigraph ( type_ , hints = hints ) and type_ . nullable @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : # Optional is represented as a Union; strip out NoneType before dispatching type_ = Union [ tuple ( subtype for subtype in get_args ( type_ ) if subtype is not NoneType )] return type_system . to_artigraph ( type_ , hints = hints ) . copy ( update = { \"nullable\" : True }) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return is_optional_hint ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : return cls . system [ type_system . to_system ( type_ . copy ( update = { \"nullable\" : False }), hints = hints ) ] @python_type_system . register_adapter class PyStruct ( TypeAdapter ): artigraph = arti . types . Struct system = TypedDict # TODO: Support and inspect TypedDict's '__optional_keys__', '__required_keys__', '__total__' @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return arti . types . Struct ( name = type_ . __name__ , fields = { field_name : type_system . to_artigraph ( field_type , hints = hints ) for field_name , field_type in get_type_hints ( type_ ) . items () }, ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : # NOTE: This check is probably a little shaky, particularly across python versions. Consider # using the typing_inspect package. return is_typeddict ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return TypedDict ( # type: ignore[operator] type_ . name , { field_name : type_system . to_system ( field_type , hints = hints ) for field_name , field_type in type_ . fields . items () }, ) Variables python_type_system Classes PyDatetime class PyDatetime ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyDatetime ( _ScalarClassTypeAdapter ) : artigraph = arti . types . Timestamp system = datetime . datetime priority = 1 # Prioritize above Date ( isinstance ( datetime , date ) is True ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( precision = \"microsecond\" ) Ancestors (in MRO) arti.types._ScalarClassTypeAdapter arti.types.TypeAdapter Class variables artigraph key priority system Static methods generate def generate ( * , artigraph : 'type[Type]' , system : 'Any' , priority : 'int' = 0 , type_system : 'TypeSystem' , name : 'Optional[str]' = None ) -> 'type[TypeAdapter]' Generate a _ScalarClassTypeAdapter subclass for the scalar system type. View Source @classmethod def generate ( cls , * , artigraph : type [ Type ] , system : Any , priority : int = 0 , type_system : TypeSystem , name : Optional [ str ] = None , ) -> type [ TypeAdapter ] : \"\"\"Generate a _ScalarClassTypeAdapter subclass for the scalar system type.\"\"\" name = name or f \"{type_system.key}{artigraph.__name__}\" return type_system . register_adapter ( type ( name , ( cls ,), { \"artigraph\" : artigraph , \"system\" : system , \"priority\" : priority } , ) ) matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( type_ , cls . system ) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( precision = \"microsecond\" ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return cls . system PyFrozenset class PyFrozenset ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyFrozenset ( PyValueContainer ) : artigraph = arti . types . Set system = frozenset Ancestors (in MRO) arti.types.python.PyValueContainer arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system ) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : ( element ,) = get_args ( type_ ) assert issubclass ( cls . artigraph , _ContainerMixin ) return cls . artigraph ( element = type_system . to_artigraph ( element , hints = hints ), ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ type_system.to_system(type_.element, hints=hints) ] # type : ignore [ attr-defined ] PyList class PyList ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyList ( PyValueContainer ) : artigraph = arti . types . List system = list priority = 1 Ancestors (in MRO) arti.types.python.PyValueContainer arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system ) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : ( element ,) = get_args ( type_ ) assert issubclass ( cls . artigraph , _ContainerMixin ) return cls . artigraph ( element = type_system . to_artigraph ( element , hints = hints ), ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ type_system.to_system(type_.element, hints=hints) ] # type : ignore [ attr-defined ] PyLiteral class PyLiteral ( / , * args , ** kwargs ) View Source @ python_type_system . register_adapter class PyLiteral ( TypeAdapter ): artigraph = arti . types . Enum system = Literal @ classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : origin , items = get_origin ( type_ ), get_args ( type_ ) if is_union ( origin ): assert not is_optional_hint ( type_ ) # Should be handled by PyOptional # We only support Enums currently, so all subtypes must be Literal if non_literals : = [ sub for sub in items if get_origin ( sub ) is not Literal ]: raise NotImplementedError ( f \"Only Union[Literal[...], ...] (enums) are currently supported, got invalid subtypes: {non_literals}\" ) # Flatten Union[Literal[1], Literal[1,2,3]] origin , items = Literal , tuple ( chain . from_iterable ( get_args ( sub ) for sub in items )) assert origin is Literal assert isinstance ( items , tuple ) if len ( items ) == 0 : raise NotImplementedError ( f \"Invalid Literal with no values: {type_}\" ) py_type , * other_types = ( type ( v ) for v in items ) if not all ( t is py_type for t in other_types ): raise ValueError ( \"All Literals must be the same type, got: {(py_type, *other_types)}\" ) return cls . artigraph ( type = type_system . to_artigraph ( py_type , hints = hints ), items = items ) @ classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : # We don't (currently) support arbitrary Unions, but can map Union[Literal[1], Literal[2]] # to an Enum. Python's Optional is also represented as a Union, but we handle that with the # high priority PyOptional. origin , items = get_origin ( type_ ), get_args ( type_ ) return origin is Literal or ( is_union ( origin ) and all ( get_origin ( sub ) is Literal for sub in items ) ) @ classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ tuple ( type_ . items )] Ancestors (in MRO) arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # We don 't (currently) support arbitrary Unions, but can map Union[Literal[1], Literal[2]] # to an Enum. Python' s Optional is also represented as a Union , but we handle that with the # high priority PyOptional . origin , items = get_origin ( type_ ), get_args ( type_ ) return origin is Literal or ( is_union ( origin ) and all ( get_origin ( sub ) is Literal for sub in items ) ) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @ classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : origin , items = get_origin ( type_ ), get_args ( type_ ) if is_union ( origin ): assert not is_optional_hint ( type_ ) # Should be handled by PyOptional # We only support Enums currently, so all subtypes must be Literal if non_literals : = [ sub for sub in items if get_origin ( sub ) is not Literal ]: raise NotImplementedError ( f \"Only Union[Literal[...], ...] (enums) are currently supported, got invalid subtypes: {non_literals}\" ) # Flatten Union[Literal[1], Literal[1,2,3]] origin , items = Literal , tuple ( chain . from_iterable ( get_args ( sub ) for sub in items )) assert origin is Literal assert isinstance ( items , tuple ) if len ( items ) == 0 : raise NotImplementedError ( f \"Invalid Literal with no values: {type_}\" ) py_type , * other_types = ( type ( v ) for v in items ) if not all ( t is py_type for t in other_types ): raise ValueError ( \"All Literals must be the same type, got: {(py_type, *other_types)}\" ) return cls . artigraph ( type = type_system . to_artigraph ( py_type , hints = hints ), items = items ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ tuple(type_.items) ] PyMap class PyMap ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyMap ( TypeAdapter ) : artigraph = arti . types . Map system = dict @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : key , value = get_args ( type_ ) return cls . artigraph ( key = type_system . to_artigraph ( key , hints = hints ), value = type_system . to_artigraph ( value , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), ( cls . system , Mapping )) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ # type: ignore[index ] type_system . to_system ( type_ . key , hints = hints ), type_system . to_system ( type_ . value , hints = hints ), ] Ancestors (in MRO) arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), ( cls . system , Mapping )) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : key , value = get_args ( type_ ) return cls . artigraph ( key = type_system . to_artigraph ( key , hints = hints ), value = type_system . to_artigraph ( value , hints = hints ), ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ # type: ignore[index ] type_system . to_system ( type_ . key , hints = hints ), type_system . to_system ( type_ . value , hints = hints ), ] PyNone class PyNone ( / , * args , ** kwargs ) View Source @python_type_system.register_adapter class PyNone ( _ScalarClassTypeAdapter ) : # Python represents None types in type hints with the `None` value (not `NoneType`). artigraph = arti . types . Null system = None @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ] ) -> bool : return type_ is None Ancestors (in MRO) arti.types._ScalarClassTypeAdapter arti.types.TypeAdapter Class variables artigraph key priority system Static methods generate def generate ( * , artigraph : 'type[Type]' , system : 'Any' , priority : 'int' = 0 , type_system : 'TypeSystem' , name : 'Optional[str]' = None ) -> 'type[TypeAdapter]' Generate a _ScalarClassTypeAdapter subclass for the scalar system type. View Source @classmethod def generate ( cls , * , artigraph : type [ Type ] , system : Any , priority : int = 0 , type_system : TypeSystem , name : Optional [ str ] = None , ) -> type [ TypeAdapter ] : \"\"\"Generate a _ScalarClassTypeAdapter subclass for the scalar system type.\"\"\" name = name or f \"{type_system.key}{artigraph.__name__}\" return type_system . register_adapter ( type ( name , ( cls ,), { \"artigraph\" : artigraph , \"system\" : system , \"priority\" : priority } , ) ) matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return type_ is None to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph () to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return cls . system PyOptional class PyOptional ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyOptional ( TypeAdapter ) : artigraph = arti . types . Type # Check against isinstance * and * . nullable system = Optional # Set very high priority to intercept other matching arti . types . Types / py Union in order to set . nullable priority = int ( 1e9 ) @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return super (). matches_artigraph ( type_ , hints = hints ) and type_ . nullable @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : # Optional is represented as a Union ; strip out NoneType before dispatching type_ = Union [ tuple(subtype for subtype in get_args(type_) if subtype is not NoneType) ] return type_system . to_artigraph ( type_ , hints = hints ). copy ( update = { \"nullable\" : True } ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return is_optional_hint ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return cls . system [ type_system.to_system(type_.copy(update={\"nullable\": False}), hints=hints) ] Ancestors (in MRO) arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return super (). matches_artigraph ( type_ , hints = hints ) and type_ . nullable matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return is_optional_hint ( type_ ) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : # Optional is represented as a Union ; strip out NoneType before dispatching type_ = Union [ tuple(subtype for subtype in get_args(type_) if subtype is not NoneType) ] return type_system . to_artigraph ( type_ , hints = hints ). copy ( update = { \"nullable\" : True } ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return cls . system [ type_system.to_system(type_.copy(update={\"nullable\": False}), hints=hints) ] PySet class PySet ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PySet ( PyValueContainer ) : artigraph = arti . types . Set system = set priority = 1 # Set above frozenset Ancestors (in MRO) arti.types.python.PyValueContainer arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system ) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : ( element ,) = get_args ( type_ ) assert issubclass ( cls . artigraph , _ContainerMixin ) return cls . artigraph ( element = type_system . to_artigraph ( element , hints = hints ), ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ type_system.to_system(type_.element, hints=hints) ] # type : ignore [ attr-defined ] PyStruct class PyStruct ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyStruct ( TypeAdapter ) : artigraph = arti . types . Struct system = TypedDict # TODO : Support and inspect TypedDict 's ' __optional_keys__ ', ' __required_keys__ ', ' __total__ ' @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return arti . types . Struct ( name = type_ . __name__ , fields = { field_name : type_system . to_artigraph ( field_type , hints = hints ) for field_name , field_type in get_type_hints ( type_ ). items () } , ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # NOTE : This check is probably a little shaky , particularly across python versions . Consider # using the typing_inspect package . return is_typeddict ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return TypedDict ( # type : ignore [ operator ] type_ . name , { field_name : type_system . to_system ( field_type , hints = hints ) for field_name , field_type in type_ . fields . items () } , ) Ancestors (in MRO) arti.types.TypeAdapter Class variables artigraph key priority Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # NOTE : This check is probably a little shaky , particularly across python versions . Consider # using the typing_inspect package . return is_typeddict ( type_ ) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return arti . types . Struct ( name = type_ . __name__ , fields = { field_name : type_system . to_artigraph ( field_type , hints = hints ) for field_name , field_type in get_type_hints ( type_ ). items () } , ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return TypedDict ( # type : ignore [ operator ] type_ . name , { field_name : type_system . to_system ( field_type , hints = hints ) for field_name , field_type in type_ . fields . items () } , ) Methods system def system ( typename , fields = None , / , * , total = True , ** kwargs ) A simple typed namespace. At runtime it is equivalent to a plain dict. TypedDict creates a dictionary type that expects all of its instances to have a certain set of keys, where each key is associated with a value of a consistent type. This expectation is not checked at runtime but is only enforced by type checkers. Usage:: class Point2D ( TypedDict ): x : int y: int label: str a: Point2D = { 'x' : 1 , 'y' : 2 , 'label' : 'good' } # OK b: Point2D = { 'z' : 3 , 'label' : 'bad' } # Fails type check assert Point2D ( x = 1 , y = 2 , label = 'first' ) == dict ( x = 1 , y = 2 , label = 'first' ) The type info can be accessed via the Point2D. annotations dict, and the Point2D. required_keys and Point2D. optional_keys frozensets. TypedDict supports an additional equivalent form:: Point2D = TypedDict('Point2D', {'x': int, 'y': int, 'label': str}) By default, all keys must be present in a TypedDict. It is possible to override this by specifying totality. Usage:: class point2D ( TypedDict , total = False ): x : int y: int This means that a point2D TypedDict can have any of the keys omitted.A type checker is only expected to support a literal False or True as the value of the total argument. True is the default, and makes all items defined in the class body be required. The class syntax is only supported in Python 3.6+, while the other syntax form works for Python 2.7 and 3.2+ View Source def TypedDict ( typename , fields = None , / , * , total = True , ** kwargs ) : \"\" \"A simple typed namespace. At runtime it is equivalent to a plain dict. TypedDict creates a dictionary type that expects all of its instances to have a certain set of keys, where each key is associated with a value of a consistent type. This expectation is not checked at runtime but is only enforced by type checkers. Usage:: class Point2D(TypedDict): x: int y: int label: str a: Point2D = {'x': 1, 'y': 2, 'label': 'good'} # OK b: Point2D = {'z': 3, 'label': 'bad'} # Fails type check assert Point2D(x=1, y=2, label='first') == dict(x=1, y=2, label='first') The type info can be accessed via the Point2D.__annotations__ dict, and the Point2D.__required_keys__ and Point2D.__optional_keys__ frozensets. TypedDict supports an additional equivalent form:: Point2D = TypedDict('Point2D', {'x': int, 'y': int, 'label': str}) By default, all keys must be present in a TypedDict. It is possible to override this by specifying totality. Usage:: class point2D(TypedDict, total=False): x: int y: int This means that a point2D TypedDict can have any of the keys omitted.A type checker is only expected to support a literal False or True as the value of the total argument. True is the default, and makes all items defined in the class body be required. The class syntax is only supported in Python 3.6+, while the other syntax form works for Python 2.7 and 3.2+ \"\" \" if fields is None: fields = kwargs elif kwargs: raise TypeError ( \"TypedDict takes either a dict or keyword arguments,\" \" but not both\" ) if kwargs : warnings . warn ( \"The kwargs-based syntax for TypedDict definitions is deprecated \" \"in Python 3.11, will be removed in Python 3.13, and may not be \" \"understood by third-party type checkers.\" , DeprecationWarning , stacklevel = 2 , ) ns = { '__annotations__' : dict ( fields ) } module = _caller () if module is not None : # Setting correct module is necessary to make typed dict classes pickleable . ns [ '__module__' ] = module return _TypedDictMeta ( typename , () , ns , total = total ) PyTuple class PyTuple ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyTuple ( PyValueContainer ) : artigraph = arti . types . List system = tuple @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : origin , args = get_origin ( type_ ), get_args ( type_ ) assert origin is not None assert len ( args ) == 2 assert args [ 1 ] is ... return super (). to_artigraph ( origin [ args[0 ] ] , hints = hints , type_system = type_system ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : if super (). matches_system ( type_ , hints = hints ) : args = get_args ( type_ ) if len ( args ) == 2 and args [ 1 ] is ... : return True return False @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : ret = super (). to_system ( type_ , hints = hints , type_system = type_system ) origin , args = get_origin ( ret ), get_args ( ret ) assert origin is not None assert len ( args ) == 1 return origin [ args[0 ] , ... ] Ancestors (in MRO) arti.types.python.PyValueContainer arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : if super (). matches_system ( type_ , hints = hints ) : args = get_args ( type_ ) if len ( args ) == 2 and args [ 1 ] is ... : return True return False to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : origin , args = get_origin ( type_ ), get_args ( type_ ) assert origin is not None assert len ( args ) == 2 assert args [ 1 ] is ... return super (). to_artigraph ( origin [ args[0 ] ] , hints = hints , type_system = type_system ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : ret = super (). to_system ( type_ , hints = hints , type_system = type_system ) origin , args = get_origin ( ret ), get_args ( ret ) assert origin is not None assert len ( args ) == 1 return origin [ args[0 ] , ... ] PyValueContainer class PyValueContainer ( / , * args , ** kwargs ) View Source class PyValueContainer ( TypeAdapter ) : @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : ( element ,) = get_args ( type_ ) assert issubclass ( cls . artigraph , _ContainerMixin ) return cls . artigraph ( element = type_system . to_artigraph ( element , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ type_system.to_system(type_.element, hints=hints) ] # type : ignore [ attr-defined ] Ancestors (in MRO) arti.types.TypeAdapter Descendants arti.types.python.PyList arti.types.python.PyTuple arti.types.python.PyFrozenset arti.types.python.PySet Class variables key priority Static methods matches_artigraph def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system ) to_artigraph def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : ( element ,) = get_args ( type_ ) assert issubclass ( cls . artigraph , _ContainerMixin ) return cls . artigraph ( element = type_system . to_artigraph ( element , hints = hints ), ) to_system def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ type_system.to_system(type_.element, hints=hints) ] # type : ignore [ attr-defined ]","title":"Python"},{"location":"reference/arti/types/python/#module-artitypespython","text":"None None View Source from __future__ import annotations import datetime from collections.abc import Mapping from functools import partial from itertools import chain from typing import Any , Literal , Optional , TypedDict , Union , get_args , get_origin , get_type_hints import arti.types from arti.internal.type_hints import ( NoneType , is_optional_hint , is_typeddict , is_union , lenient_issubclass , ) from arti.types import Type , TypeAdapter , TypeSystem , _ContainerMixin , _ScalarClassTypeAdapter python_type_system = TypeSystem ( key = \"python\" ) _generate = partial ( _ScalarClassTypeAdapter . generate , type_system = python_type_system ) _generate ( artigraph = arti . types . Binary , system = bytes ) # NOTE: issubclass(bool, int) is True, so set higher priority _generate ( artigraph = arti . types . Boolean , system = bool , priority = 1000 ) _generate ( artigraph = arti . types . Date , system = datetime . date ) _generate ( artigraph = arti . types . String , system = str ) for _precision in ( 16 , 32 , 64 ): _generate ( artigraph = getattr ( arti . types , f \"Float { _precision } \" ), system = float , priority = _precision , ) for _precision in ( 8 , 16 , 32 , 64 ): _generate ( artigraph = getattr ( arti . types , f \"Int { _precision } \" ), system = int , priority = _precision , ) @python_type_system . register_adapter class PyNone ( _ScalarClassTypeAdapter ): # Python represents None types in type hints with the `None` value (not `NoneType`). artigraph = arti . types . Null system = None @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return type_ is None @python_type_system . register_adapter class PyDatetime ( _ScalarClassTypeAdapter ): artigraph = arti . types . Timestamp system = datetime . datetime priority = 1 # Prioritize above Date (isinstance(datetime, date) is True) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return cls . artigraph ( precision = \"microsecond\" ) class PyValueContainer ( TypeAdapter ): @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : ( element ,) = get_args ( type_ ) assert issubclass ( cls . artigraph , _ContainerMixin ) return cls . artigraph ( element = type_system . to_artigraph ( element , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ type_system . to_system ( type_ . element , hints = hints )] # type: ignore[attr-defined] @python_type_system . register_adapter class PyList ( PyValueContainer ): artigraph = arti . types . List system = list priority = 1 # NOTE: PyTuple only covers sequences (eg: tuple[int, ...]), not structure (eg: tuple[int, str]). @python_type_system . register_adapter class PyTuple ( PyValueContainer ): artigraph = arti . types . List system = tuple @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : origin , args = get_origin ( type_ ), get_args ( type_ ) assert origin is not None assert len ( args ) == 2 assert args [ 1 ] is ... return super () . to_artigraph ( origin [ args [ 0 ]], hints = hints , type_system = type_system ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : if super () . matches_system ( type_ , hints = hints ): args = get_args ( type_ ) if len ( args ) == 2 and args [ 1 ] is ... : return True return False @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : ret = super () . to_system ( type_ , hints = hints , type_system = type_system ) origin , args = get_origin ( ret ), get_args ( ret ) assert origin is not None assert len ( args ) == 1 return origin [ args [ 0 ], ... ] @python_type_system . register_adapter class PyFrozenset ( PyValueContainer ): artigraph = arti . types . Set system = frozenset @python_type_system . register_adapter class PySet ( PyValueContainer ): artigraph = arti . types . Set system = set priority = 1 # Set above frozenset @python_type_system . register_adapter class PyLiteral ( TypeAdapter ): artigraph = arti . types . Enum system = Literal @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : origin , items = get_origin ( type_ ), get_args ( type_ ) if is_union ( origin ): assert not is_optional_hint ( type_ ) # Should be handled by PyOptional # We only support Enums currently, so all subtypes must be Literal if non_literals := [ sub for sub in items if get_origin ( sub ) is not Literal ]: raise NotImplementedError ( f \"Only Union[Literal[...], ...] (enums) are currently supported, got invalid subtypes: { non_literals } \" ) # Flatten Union[Literal[1], Literal[1,2,3]] origin , items = Literal , tuple ( chain . from_iterable ( get_args ( sub ) for sub in items )) assert origin is Literal assert isinstance ( items , tuple ) if len ( items ) == 0 : raise NotImplementedError ( f \"Invalid Literal with no values: { type_ } \" ) py_type , * other_types = ( type ( v ) for v in items ) if not all ( t is py_type for t in other_types ): raise ValueError ( \"All Literals must be the same type, got: {(py_type, *other_types)}\" ) return cls . artigraph ( type = type_system . to_artigraph ( py_type , hints = hints ), items = items ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : # We don't (currently) support arbitrary Unions, but can map Union[Literal[1], Literal[2]] # to an Enum. Python's Optional is also represented as a Union, but we handle that with the # high priority PyOptional. origin , items = get_origin ( type_ ), get_args ( type_ ) return origin is Literal or ( is_union ( origin ) and all ( get_origin ( sub ) is Literal for sub in items ) ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ tuple ( type_ . items )] @python_type_system . register_adapter class PyMap ( TypeAdapter ): artigraph = arti . types . Map system = dict @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : key , value = get_args ( type_ ) return cls . artigraph ( key = type_system . to_artigraph ( key , hints = hints ), value = type_system . to_artigraph ( value , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return lenient_issubclass ( get_origin ( type_ ), ( cls . system , Mapping )) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ # type: ignore[index] type_system . to_system ( type_ . key , hints = hints ), type_system . to_system ( type_ . value , hints = hints ), ] @python_type_system . register_adapter class PyOptional ( TypeAdapter ): artigraph = arti . types . Type # Check against isinstance *and* .nullable system = Optional # Set very high priority to intercept other matching arti.types.Types/py Union in order to set .nullable priority = int ( 1e9 ) @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : return super () . matches_artigraph ( type_ , hints = hints ) and type_ . nullable @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : # Optional is represented as a Union; strip out NoneType before dispatching type_ = Union [ tuple ( subtype for subtype in get_args ( type_ ) if subtype is not NoneType )] return type_system . to_artigraph ( type_ , hints = hints ) . copy ( update = { \"nullable\" : True }) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return is_optional_hint ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : return cls . system [ type_system . to_system ( type_ . copy ( update = { \"nullable\" : False }), hints = hints ) ] @python_type_system . register_adapter class PyStruct ( TypeAdapter ): artigraph = arti . types . Struct system = TypedDict # TODO: Support and inspect TypedDict's '__optional_keys__', '__required_keys__', '__total__' @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : return arti . types . Struct ( name = type_ . __name__ , fields = { field_name : type_system . to_artigraph ( field_type , hints = hints ) for field_name , field_type in get_type_hints ( type_ ) . items () }, ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : # NOTE: This check is probably a little shaky, particularly across python versions. Consider # using the typing_inspect package. return is_typeddict ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return TypedDict ( # type: ignore[operator] type_ . name , { field_name : type_system . to_system ( field_type , hints = hints ) for field_name , field_type in type_ . fields . items () }, )","title":"Module arti.types.python"},{"location":"reference/arti/types/python/#variables","text":"python_type_system","title":"Variables"},{"location":"reference/arti/types/python/#classes","text":"","title":"Classes"},{"location":"reference/arti/types/python/#pydatetime","text":"class PyDatetime ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyDatetime ( _ScalarClassTypeAdapter ) : artigraph = arti . types . Timestamp system = datetime . datetime priority = 1 # Prioritize above Date ( isinstance ( datetime , date ) is True ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( precision = \"microsecond\" )","title":"PyDatetime"},{"location":"reference/arti/types/python/#ancestors-in-mro","text":"arti.types._ScalarClassTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#generate","text":"def generate ( * , artigraph : 'type[Type]' , system : 'Any' , priority : 'int' = 0 , type_system : 'TypeSystem' , name : 'Optional[str]' = None ) -> 'type[TypeAdapter]' Generate a _ScalarClassTypeAdapter subclass for the scalar system type. View Source @classmethod def generate ( cls , * , artigraph : type [ Type ] , system : Any , priority : int = 0 , type_system : TypeSystem , name : Optional [ str ] = None , ) -> type [ TypeAdapter ] : \"\"\"Generate a _ScalarClassTypeAdapter subclass for the scalar system type.\"\"\" name = name or f \"{type_system.key}{artigraph.__name__}\" return type_system . register_adapter ( type ( name , ( cls ,), { \"artigraph\" : artigraph , \"system\" : system , \"priority\" : priority } , ) )","title":"generate"},{"location":"reference/arti/types/python/#matches_artigraph","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( type_ , cls . system )","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ( precision = \"microsecond\" )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return cls . system","title":"to_system"},{"location":"reference/arti/types/python/#pyfrozenset","text":"class PyFrozenset ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyFrozenset ( PyValueContainer ) : artigraph = arti . types . Set system = frozenset","title":"PyFrozenset"},{"location":"reference/arti/types/python/#ancestors-in-mro_1","text":"arti.types.python.PyValueContainer arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_1","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_1","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_1","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system )","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_1","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : ( element ,) = get_args ( type_ ) assert issubclass ( cls . artigraph , _ContainerMixin ) return cls . artigraph ( element = type_system . to_artigraph ( element , hints = hints ), )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_1","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ type_system.to_system(type_.element, hints=hints) ] # type : ignore [ attr-defined ]","title":"to_system"},{"location":"reference/arti/types/python/#pylist","text":"class PyList ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyList ( PyValueContainer ) : artigraph = arti . types . List system = list priority = 1","title":"PyList"},{"location":"reference/arti/types/python/#ancestors-in-mro_2","text":"arti.types.python.PyValueContainer arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_2","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_2","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_2","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system )","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_2","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : ( element ,) = get_args ( type_ ) assert issubclass ( cls . artigraph , _ContainerMixin ) return cls . artigraph ( element = type_system . to_artigraph ( element , hints = hints ), )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_2","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ type_system.to_system(type_.element, hints=hints) ] # type : ignore [ attr-defined ]","title":"to_system"},{"location":"reference/arti/types/python/#pyliteral","text":"class PyLiteral ( / , * args , ** kwargs ) View Source @ python_type_system . register_adapter class PyLiteral ( TypeAdapter ): artigraph = arti . types . Enum system = Literal @ classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : origin , items = get_origin ( type_ ), get_args ( type_ ) if is_union ( origin ): assert not is_optional_hint ( type_ ) # Should be handled by PyOptional # We only support Enums currently, so all subtypes must be Literal if non_literals : = [ sub for sub in items if get_origin ( sub ) is not Literal ]: raise NotImplementedError ( f \"Only Union[Literal[...], ...] (enums) are currently supported, got invalid subtypes: {non_literals}\" ) # Flatten Union[Literal[1], Literal[1,2,3]] origin , items = Literal , tuple ( chain . from_iterable ( get_args ( sub ) for sub in items )) assert origin is Literal assert isinstance ( items , tuple ) if len ( items ) == 0 : raise NotImplementedError ( f \"Invalid Literal with no values: {type_}\" ) py_type , * other_types = ( type ( v ) for v in items ) if not all ( t is py_type for t in other_types ): raise ValueError ( \"All Literals must be the same type, got: {(py_type, *other_types)}\" ) return cls . artigraph ( type = type_system . to_artigraph ( py_type , hints = hints ), items = items ) @ classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : # We don't (currently) support arbitrary Unions, but can map Union[Literal[1], Literal[2]] # to an Enum. Python's Optional is also represented as a Union, but we handle that with the # high priority PyOptional. origin , items = get_origin ( type_ ), get_args ( type_ ) return origin is Literal or ( is_union ( origin ) and all ( get_origin ( sub ) is Literal for sub in items ) ) @ classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ tuple ( type_ . items )]","title":"PyLiteral"},{"location":"reference/arti/types/python/#ancestors-in-mro_3","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_3","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_3","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_3","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # We don 't (currently) support arbitrary Unions, but can map Union[Literal[1], Literal[2]] # to an Enum. Python' s Optional is also represented as a Union , but we handle that with the # high priority PyOptional . origin , items = get_origin ( type_ ), get_args ( type_ ) return origin is Literal or ( is_union ( origin ) and all ( get_origin ( sub ) is Literal for sub in items ) )","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_3","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @ classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ], type_system : TypeSystem ) -> Type : origin , items = get_origin ( type_ ), get_args ( type_ ) if is_union ( origin ): assert not is_optional_hint ( type_ ) # Should be handled by PyOptional # We only support Enums currently, so all subtypes must be Literal if non_literals : = [ sub for sub in items if get_origin ( sub ) is not Literal ]: raise NotImplementedError ( f \"Only Union[Literal[...], ...] (enums) are currently supported, got invalid subtypes: {non_literals}\" ) # Flatten Union[Literal[1], Literal[1,2,3]] origin , items = Literal , tuple ( chain . from_iterable ( get_args ( sub ) for sub in items )) assert origin is Literal assert isinstance ( items , tuple ) if len ( items ) == 0 : raise NotImplementedError ( f \"Invalid Literal with no values: {type_}\" ) py_type , * other_types = ( type ( v ) for v in items ) if not all ( t is py_type for t in other_types ): raise ValueError ( \"All Literals must be the same type, got: {(py_type, *other_types)}\" ) return cls . artigraph ( type = type_system . to_artigraph ( py_type , hints = hints ), items = items )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_3","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ tuple(type_.items) ]","title":"to_system"},{"location":"reference/arti/types/python/#pymap","text":"class PyMap ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyMap ( TypeAdapter ) : artigraph = arti . types . Map system = dict @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : key , value = get_args ( type_ ) return cls . artigraph ( key = type_system . to_artigraph ( key , hints = hints ), value = type_system . to_artigraph ( value , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), ( cls . system , Mapping )) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ # type: ignore[index ] type_system . to_system ( type_ . key , hints = hints ), type_system . to_system ( type_ . value , hints = hints ), ]","title":"PyMap"},{"location":"reference/arti/types/python/#ancestors-in-mro_4","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_4","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_4","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_4","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), ( cls . system , Mapping ))","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_4","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : key , value = get_args ( type_ ) return cls . artigraph ( key = type_system . to_artigraph ( key , hints = hints ), value = type_system . to_artigraph ( value , hints = hints ), )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_4","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ # type: ignore[index ] type_system . to_system ( type_ . key , hints = hints ), type_system . to_system ( type_ . value , hints = hints ), ]","title":"to_system"},{"location":"reference/arti/types/python/#pynone","text":"class PyNone ( / , * args , ** kwargs ) View Source @python_type_system.register_adapter class PyNone ( _ScalarClassTypeAdapter ) : # Python represents None types in type hints with the `None` value (not `NoneType`). artigraph = arti . types . Null system = None @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ] ) -> bool : return type_ is None","title":"PyNone"},{"location":"reference/arti/types/python/#ancestors-in-mro_5","text":"arti.types._ScalarClassTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_5","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_5","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#generate_1","text":"def generate ( * , artigraph : 'type[Type]' , system : 'Any' , priority : 'int' = 0 , type_system : 'TypeSystem' , name : 'Optional[str]' = None ) -> 'type[TypeAdapter]' Generate a _ScalarClassTypeAdapter subclass for the scalar system type. View Source @classmethod def generate ( cls , * , artigraph : type [ Type ] , system : Any , priority : int = 0 , type_system : TypeSystem , name : Optional [ str ] = None , ) -> type [ TypeAdapter ] : \"\"\"Generate a _ScalarClassTypeAdapter subclass for the scalar system type.\"\"\" name = name or f \"{type_system.key}{artigraph.__name__}\" return type_system . register_adapter ( type ( name , ( cls ,), { \"artigraph\" : artigraph , \"system\" : system , \"priority\" : priority } , ) )","title":"generate"},{"location":"reference/arti/types/python/#matches_artigraph_5","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_5","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return type_ is None","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_5","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return cls . artigraph ()","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_5","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return cls . system","title":"to_system"},{"location":"reference/arti/types/python/#pyoptional","text":"class PyOptional ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyOptional ( TypeAdapter ) : artigraph = arti . types . Type # Check against isinstance * and * . nullable system = Optional # Set very high priority to intercept other matching arti . types . Types / py Union in order to set . nullable priority = int ( 1e9 ) @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return super (). matches_artigraph ( type_ , hints = hints ) and type_ . nullable @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : # Optional is represented as a Union ; strip out NoneType before dispatching type_ = Union [ tuple(subtype for subtype in get_args(type_) if subtype is not NoneType) ] return type_system . to_artigraph ( type_ , hints = hints ). copy ( update = { \"nullable\" : True } ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return is_optional_hint ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return cls . system [ type_system.to_system(type_.copy(update={\"nullable\": False}), hints=hints) ]","title":"PyOptional"},{"location":"reference/arti/types/python/#ancestors-in-mro_6","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_6","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_6","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_6","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return super (). matches_artigraph ( type_ , hints = hints ) and type_ . nullable","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_6","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return is_optional_hint ( type_ )","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_6","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : # Optional is represented as a Union ; strip out NoneType before dispatching type_ = Union [ tuple(subtype for subtype in get_args(type_) if subtype is not NoneType) ] return type_system . to_artigraph ( type_ , hints = hints ). copy ( update = { \"nullable\" : True } )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_6","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : return cls . system [ type_system.to_system(type_.copy(update={\"nullable\": False}), hints=hints) ]","title":"to_system"},{"location":"reference/arti/types/python/#pyset","text":"class PySet ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PySet ( PyValueContainer ) : artigraph = arti . types . Set system = set priority = 1 # Set above frozenset","title":"PySet"},{"location":"reference/arti/types/python/#ancestors-in-mro_7","text":"arti.types.python.PyValueContainer arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_7","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_7","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_7","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_7","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system )","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_7","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : ( element ,) = get_args ( type_ ) assert issubclass ( cls . artigraph , _ContainerMixin ) return cls . artigraph ( element = type_system . to_artigraph ( element , hints = hints ), )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_7","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ type_system.to_system(type_.element, hints=hints) ] # type : ignore [ attr-defined ]","title":"to_system"},{"location":"reference/arti/types/python/#pystruct","text":"class PyStruct ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyStruct ( TypeAdapter ) : artigraph = arti . types . Struct system = TypedDict # TODO : Support and inspect TypedDict 's ' __optional_keys__ ', ' __required_keys__ ', ' __total__ ' @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return arti . types . Struct ( name = type_ . __name__ , fields = { field_name : type_system . to_artigraph ( field_type , hints = hints ) for field_name , field_type in get_type_hints ( type_ ). items () } , ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # NOTE : This check is probably a little shaky , particularly across python versions . Consider # using the typing_inspect package . return is_typeddict ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return TypedDict ( # type : ignore [ operator ] type_ . name , { field_name : type_system . to_system ( field_type , hints = hints ) for field_name , field_type in type_ . fields . items () } , )","title":"PyStruct"},{"location":"reference/arti/types/python/#ancestors-in-mro_8","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_8","text":"artigraph key priority","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_8","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_8","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_8","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # NOTE : This check is probably a little shaky , particularly across python versions . Consider # using the typing_inspect package . return is_typeddict ( type_ )","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_8","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : return arti . types . Struct ( name = type_ . __name__ , fields = { field_name : type_system . to_artigraph ( field_type , hints = hints ) for field_name , field_type in get_type_hints ( type_ ). items () } , )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_8","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return TypedDict ( # type : ignore [ operator ] type_ . name , { field_name : type_system . to_system ( field_type , hints = hints ) for field_name , field_type in type_ . fields . items () } , )","title":"to_system"},{"location":"reference/arti/types/python/#methods","text":"","title":"Methods"},{"location":"reference/arti/types/python/#system","text":"def system ( typename , fields = None , / , * , total = True , ** kwargs ) A simple typed namespace. At runtime it is equivalent to a plain dict. TypedDict creates a dictionary type that expects all of its instances to have a certain set of keys, where each key is associated with a value of a consistent type. This expectation is not checked at runtime but is only enforced by type checkers. Usage:: class Point2D ( TypedDict ): x : int y: int label: str a: Point2D = { 'x' : 1 , 'y' : 2 , 'label' : 'good' } # OK b: Point2D = { 'z' : 3 , 'label' : 'bad' } # Fails type check assert Point2D ( x = 1 , y = 2 , label = 'first' ) == dict ( x = 1 , y = 2 , label = 'first' ) The type info can be accessed via the Point2D. annotations dict, and the Point2D. required_keys and Point2D. optional_keys frozensets. TypedDict supports an additional equivalent form:: Point2D = TypedDict('Point2D', {'x': int, 'y': int, 'label': str}) By default, all keys must be present in a TypedDict. It is possible to override this by specifying totality. Usage:: class point2D ( TypedDict , total = False ): x : int y: int This means that a point2D TypedDict can have any of the keys omitted.A type checker is only expected to support a literal False or True as the value of the total argument. True is the default, and makes all items defined in the class body be required. The class syntax is only supported in Python 3.6+, while the other syntax form works for Python 2.7 and 3.2+ View Source def TypedDict ( typename , fields = None , / , * , total = True , ** kwargs ) : \"\" \"A simple typed namespace. At runtime it is equivalent to a plain dict. TypedDict creates a dictionary type that expects all of its instances to have a certain set of keys, where each key is associated with a value of a consistent type. This expectation is not checked at runtime but is only enforced by type checkers. Usage:: class Point2D(TypedDict): x: int y: int label: str a: Point2D = {'x': 1, 'y': 2, 'label': 'good'} # OK b: Point2D = {'z': 3, 'label': 'bad'} # Fails type check assert Point2D(x=1, y=2, label='first') == dict(x=1, y=2, label='first') The type info can be accessed via the Point2D.__annotations__ dict, and the Point2D.__required_keys__ and Point2D.__optional_keys__ frozensets. TypedDict supports an additional equivalent form:: Point2D = TypedDict('Point2D', {'x': int, 'y': int, 'label': str}) By default, all keys must be present in a TypedDict. It is possible to override this by specifying totality. Usage:: class point2D(TypedDict, total=False): x: int y: int This means that a point2D TypedDict can have any of the keys omitted.A type checker is only expected to support a literal False or True as the value of the total argument. True is the default, and makes all items defined in the class body be required. The class syntax is only supported in Python 3.6+, while the other syntax form works for Python 2.7 and 3.2+ \"\" \" if fields is None: fields = kwargs elif kwargs: raise TypeError ( \"TypedDict takes either a dict or keyword arguments,\" \" but not both\" ) if kwargs : warnings . warn ( \"The kwargs-based syntax for TypedDict definitions is deprecated \" \"in Python 3.11, will be removed in Python 3.13, and may not be \" \"understood by third-party type checkers.\" , DeprecationWarning , stacklevel = 2 , ) ns = { '__annotations__' : dict ( fields ) } module = _caller () if module is not None : # Setting correct module is necessary to make typed dict classes pickleable . ns [ '__module__' ] = module return _TypedDictMeta ( typename , () , ns , total = total )","title":"system"},{"location":"reference/arti/types/python/#pytuple","text":"class PyTuple ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyTuple ( PyValueContainer ) : artigraph = arti . types . List system = tuple @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : origin , args = get_origin ( type_ ), get_args ( type_ ) assert origin is not None assert len ( args ) == 2 assert args [ 1 ] is ... return super (). to_artigraph ( origin [ args[0 ] ] , hints = hints , type_system = type_system ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : if super (). matches_system ( type_ , hints = hints ) : args = get_args ( type_ ) if len ( args ) == 2 and args [ 1 ] is ... : return True return False @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : ret = super (). to_system ( type_ , hints = hints , type_system = type_system ) origin , args = get_origin ( ret ), get_args ( ret ) assert origin is not None assert len ( args ) == 1 return origin [ args[0 ] , ... ]","title":"PyTuple"},{"location":"reference/arti/types/python/#ancestors-in-mro_9","text":"arti.types.python.PyValueContainer arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_9","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_9","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_9","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_9","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : if super (). matches_system ( type_ , hints = hints ) : args = get_args ( type_ ) if len ( args ) == 2 and args [ 1 ] is ... : return True return False","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_9","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : origin , args = get_origin ( type_ ), get_args ( type_ ) assert origin is not None assert len ( args ) == 2 assert args [ 1 ] is ... return super (). to_artigraph ( origin [ args[0 ] ] , hints = hints , type_system = type_system )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_9","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : ret = super (). to_system ( type_ , hints = hints , type_system = type_system ) origin , args = get_origin ( ret ), get_args ( ret ) assert origin is not None assert len ( args ) == 1 return origin [ args[0 ] , ... ]","title":"to_system"},{"location":"reference/arti/types/python/#pyvaluecontainer","text":"class PyValueContainer ( / , * args , ** kwargs ) View Source class PyValueContainer ( TypeAdapter ) : @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : ( element ,) = get_args ( type_ ) assert issubclass ( cls . artigraph , _ContainerMixin ) return cls . artigraph ( element = type_system . to_artigraph ( element , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ type_system.to_system(type_.element, hints=hints) ] # type : ignore [ attr-defined ]","title":"PyValueContainer"},{"location":"reference/arti/types/python/#ancestors-in-mro_10","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#descendants","text":"arti.types.python.PyList arti.types.python.PyTuple arti.types.python.PyFrozenset arti.types.python.PySet","title":"Descendants"},{"location":"reference/arti/types/python/#class-variables_10","text":"key priority","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_10","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_10","text":"def matches_artigraph ( type_ : 'Type' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_10","text":"def matches_system ( type_ : 'Any' , * , hints : 'dict[str, Any]' ) -> 'bool' View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system )","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_10","text":"def to_artigraph ( type_ : 'Any' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Type' View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Type : ( element ,) = get_args ( type_ ) assert issubclass ( cls . artigraph , _ContainerMixin ) return cls . artigraph ( element = type_system . to_artigraph ( element , hints = hints ), )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_10","text":"def to_system ( type_ : 'Type' , * , hints : 'dict[str, Any]' , type_system : 'TypeSystem' ) -> 'Any' View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] , type_system : TypeSystem ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ type_system.to_system(type_.element, hints=hints) ] # type : ignore [ attr-defined ]","title":"to_system"},{"location":"reference/arti/views/","text":"Module arti.views None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import builtins from typing import Any , ClassVar , Literal , Optional , get_origin from pydantic import validator from arti import io from arti.artifacts import Artifact from arti.internal.models import Model , get_field_default from arti.internal.type_hints import discard_Annotated , get_item_from_annotated , lenient_issubclass from arti.internal.utils import import_submodules , register from arti.types import Type , TypeSystem MODE = Literal [ \"READ\" , \"WRITE\" , \"READWRITE\" ] class View ( Model ): \"\"\"View represents the in-memory representation of the artifact. Examples include pandas.DataFrame, dask.DataFrame, a BigQuery table. \"\"\" _abstract_ = True _by_python_type_ : ClassVar [ dict [ Optional [ type ], type [ View ]]] = {} priority : ClassVar [ int ] = 0 # Set priority of this view for its python_type. Higher is better. python_type : ClassVar [ Optional [ type ]] type_system : ClassVar [ TypeSystem ] mode : MODE artifact_class : type [ Artifact ] = Artifact type : Type @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if not cls . _abstract_ : register ( cls . _by_python_type_ , cls . python_type , cls , lambda x : x . priority ) @classmethod def _check_type_compatibility ( cls , view_type : Type , artifact_type : Type ) -> None : # TODO: Consider supporting some form of \"reader schema\" where the View's Type is a subset # of the Artifact's Type (and we filter the columns on read). We could also consider # allowing the Producer's Type to be a superset of the Artifact's Type and we'd filter the # columns on write. # # If implementing, we can leverage the `mode` to determine which should be the \"superset\". if view_type != artifact_type : raise ValueError ( f \"the specified Type (` { view_type } `) is not compatible with the Artifact's Type (` { artifact_type } `).\" ) @validator ( \"type\" ) @classmethod def _validate_type ( cls , type_ : Type , values : dict [ str , Any ]) -> Type : artifact_class : Optional [ type [ Artifact ]] = values . get ( \"artifact_class\" ) if artifact_class is None : return type_ # pragma: no cover artifact_type : Optional [ Type ] = get_field_default ( artifact_class , \"type\" ) if artifact_type is not None : cls . _check_type_compatibility ( view_type = type_ , artifact_type = artifact_type ) return type_ @classmethod def _get_kwargs_from_annotation ( cls , annotation : Any ) -> dict [ str , Any ]: artifact_class = get_item_from_annotated ( annotation , Artifact , is_subclass = True ) or get_field_default ( cls , \"artifact_class\" ) assert artifact_class is not None assert issubclass ( artifact_class , Artifact ) # Try to extract or infer the Type. We prefer: an explicit Type in the annotation, followed # by an Artifact's default type, falling back to inferring a Type from the type hint. type_ = get_item_from_annotated ( annotation , Type , is_subclass = False ) if type_ is None : artifact_type : Optional [ Type ] = get_field_default ( artifact_class , \"type\" ) if artifact_type is None : from arti.types.python import python_type_system type_ = python_type_system . to_artigraph ( discard_Annotated ( annotation ), hints = {}) else : type_ = artifact_type # NOTE: We validate that type_ and artifact_type (if set) are compatible in _validate_type, # which will run for *any* instance, not just those created with `.from_annotation`. return { \"artifact_class\" : artifact_class , \"type\" : type_ } @classmethod # TODO: Use typing.Self for return, pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ]: view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We've already searched for a View instance in the original Annotated args, so just # extract the root annotation. annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration. import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic (eg: `list[int]`), try to unwrap any # extra type variables. if view_class is None and ( origin := get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \" { annotation } cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class @classmethod # TODO: Use typing.Self for return, pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view def check_annotation_compatibility ( self , annotation : Any ) -> None : # We're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated ( annotation ) system_type = self . type_system . to_system ( self . type , hints = {}) if not ( lenient_issubclass ( system_type , annotation ) or lenient_issubclass ( type ( system_type ), annotation ) ): raise ValueError ( f \" { annotation } cannot be used to represent { self . type } \" ) def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ): raise ValueError ( f \"expected an instance of { self . artifact_class } , got { type ( artifact ) } \" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" }: io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type: ignore[name-defined] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" }: io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), ) Sub-modules arti.views.python Variables MODE Classes View class View ( __pydantic_self__ , ** data : Any ) View Source class View ( Model ) : \"\"\"View represents the in-memory representation of the artifact. Examples include pandas.DataFrame, dask.DataFrame, a BigQuery table. \"\"\" _abstract_ = True _by_python_type_ : ClassVar [ dict[Optional[type ] , type [ View ] ]] = {} priority : ClassVar [ int ] = 0 # Set priority of this view for its python_type . Higher is better . python_type : ClassVar [ Optional[type ] ] type_system : ClassVar [ TypeSystem ] mode : MODE artifact_class : type [ Artifact ] = Artifact type : Type @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if not cls . _abstract_ : register ( cls . _by_python_type_ , cls . python_type , cls , lambda x : x . priority ) @classmethod def _check_type_compatibility ( cls , view_type : Type , artifact_type : Type ) -> None : # TODO : Consider supporting some form of \"reader schema\" where the View 's Type is a subset # of the Artifact' s Type ( and we filter the columns on read ). We could also consider # allowing the Producer 's Type to be a superset of the Artifact' s Type and we 'd filter the # columns on write. # # If implementing, we can leverage the `mode` to determine which should be the \"superset\". if view_type != artifact_type: raise ValueError( f\"the specified Type (`{view_type}`) is not compatible with the Artifact' s Type ( `{ artifact_type }` ). \" ) @validator(\" type \") @classmethod def _validate_type(cls, type_: Type, values: dict[str, Any]) -> Type: artifact_class: Optional[type[Artifact]] = values.get(\" artifact_class \") if artifact_class is None: return type_ # pragma: no cover artifact_type: Optional[Type] = get_field_default(artifact_class, \" type \") if artifact_type is not None: cls._check_type_compatibility(view_type=type_, artifact_type=artifact_type) return type_ @classmethod def _get_kwargs_from_annotation(cls, annotation: Any) -> dict[str, Any]: artifact_class = get_item_from_annotated( annotation, Artifact, is_subclass=True ) or get_field_default(cls, \" artifact_class \") assert artifact_class is not None assert issubclass(artifact_class, Artifact) # Try to extract or infer the Type. We prefer: an explicit Type in the annotation, followed # by an Artifact's default type, falling back to inferring a Type from the type hint. type_ = get_item_from_annotated(annotation, Type, is_subclass=False) if type_ is None: artifact_type: Optional[Type] = get_field_default(artifact_class, \" type \") if artifact_type is None: from arti.types.python import python_type_system type_ = python_type_system.to_artigraph(discard_Annotated(annotation), hints={}) else: type_ = artifact_type # NOTE: We validate that type_ and artifact_type (if set) are compatible in _validate_type, # which will run for *any* instance, not just those created with `.from_annotation`. return {\" artifact_class \": artifact_class, \" type \": type_} @classmethod # TODO: Use typing.Self for return, pending mypy support def get_class_for(cls, annotation: Any) -> builtins.type[View]: view_class = get_item_from_annotated(annotation, cls, is_subclass=True) if view_class is None: # We've already searched for a View instance in the original Annotated args, so just # extract the root annotation. annotation = discard_Annotated(annotation) # Import the View submodules to trigger registration. import_submodules(__path__, __name__) view_class = cls._by_python_type_.get(annotation) # If no match and the type is a subscripted Generic (eg: `list[int]`), try to unwrap any # extra type variables. if view_class is None and (origin := get_origin(annotation)) is not None: view_class = cls._by_python_type_.get(origin) if view_class is None: raise ValueError( f\" { annotation } cannot be matched to a View , try setting one explicitly ( eg : ` Annotated [ int, arti.views.python.Int ] ` ) \" ) return view_class @classmethod # TODO: Use typing.Self for return, pending mypy support def from_annotation(cls, annotation: Any, *, mode: MODE) -> View: view_class = cls.get_class_for(annotation) view = view_class(mode=mode, **cls._get_kwargs_from_annotation(annotation)) view.check_annotation_compatibility(annotation) return view def check_annotation_compatibility(self, annotation: Any) -> None: # We're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \") def check_artifact_compatibility(self, artifact: Artifact) -> None: if not isinstance(artifact, self.artifact_class): raise ValueError(f\" expected an instance of { self . artifact_class } , got { type ( artifact ) } \") self._check_type_compatibility(view_type=self.type, artifact_type=artifact.type) if self.mode in {\" READ \", \" READWRITE \"}: io._read.lookup( type(artifact.type), type(artifact.format), list[artifact.storage.storage_partition_type], # type: ignore[name-defined] type(self), ) if self.mode in {\" WRITE \", \" READWRITE \"}: io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.views.python.PythonBuiltin Class variables Config priority Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_annotation def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods check_annotation_compatibility def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \") check_artifact_compatibility def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Index"},{"location":"reference/arti/views/#module-artiviews","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import builtins from typing import Any , ClassVar , Literal , Optional , get_origin from pydantic import validator from arti import io from arti.artifacts import Artifact from arti.internal.models import Model , get_field_default from arti.internal.type_hints import discard_Annotated , get_item_from_annotated , lenient_issubclass from arti.internal.utils import import_submodules , register from arti.types import Type , TypeSystem MODE = Literal [ \"READ\" , \"WRITE\" , \"READWRITE\" ] class View ( Model ): \"\"\"View represents the in-memory representation of the artifact. Examples include pandas.DataFrame, dask.DataFrame, a BigQuery table. \"\"\" _abstract_ = True _by_python_type_ : ClassVar [ dict [ Optional [ type ], type [ View ]]] = {} priority : ClassVar [ int ] = 0 # Set priority of this view for its python_type. Higher is better. python_type : ClassVar [ Optional [ type ]] type_system : ClassVar [ TypeSystem ] mode : MODE artifact_class : type [ Artifact ] = Artifact type : Type @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if not cls . _abstract_ : register ( cls . _by_python_type_ , cls . python_type , cls , lambda x : x . priority ) @classmethod def _check_type_compatibility ( cls , view_type : Type , artifact_type : Type ) -> None : # TODO: Consider supporting some form of \"reader schema\" where the View's Type is a subset # of the Artifact's Type (and we filter the columns on read). We could also consider # allowing the Producer's Type to be a superset of the Artifact's Type and we'd filter the # columns on write. # # If implementing, we can leverage the `mode` to determine which should be the \"superset\". if view_type != artifact_type : raise ValueError ( f \"the specified Type (` { view_type } `) is not compatible with the Artifact's Type (` { artifact_type } `).\" ) @validator ( \"type\" ) @classmethod def _validate_type ( cls , type_ : Type , values : dict [ str , Any ]) -> Type : artifact_class : Optional [ type [ Artifact ]] = values . get ( \"artifact_class\" ) if artifact_class is None : return type_ # pragma: no cover artifact_type : Optional [ Type ] = get_field_default ( artifact_class , \"type\" ) if artifact_type is not None : cls . _check_type_compatibility ( view_type = type_ , artifact_type = artifact_type ) return type_ @classmethod def _get_kwargs_from_annotation ( cls , annotation : Any ) -> dict [ str , Any ]: artifact_class = get_item_from_annotated ( annotation , Artifact , is_subclass = True ) or get_field_default ( cls , \"artifact_class\" ) assert artifact_class is not None assert issubclass ( artifact_class , Artifact ) # Try to extract or infer the Type. We prefer: an explicit Type in the annotation, followed # by an Artifact's default type, falling back to inferring a Type from the type hint. type_ = get_item_from_annotated ( annotation , Type , is_subclass = False ) if type_ is None : artifact_type : Optional [ Type ] = get_field_default ( artifact_class , \"type\" ) if artifact_type is None : from arti.types.python import python_type_system type_ = python_type_system . to_artigraph ( discard_Annotated ( annotation ), hints = {}) else : type_ = artifact_type # NOTE: We validate that type_ and artifact_type (if set) are compatible in _validate_type, # which will run for *any* instance, not just those created with `.from_annotation`. return { \"artifact_class\" : artifact_class , \"type\" : type_ } @classmethod # TODO: Use typing.Self for return, pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ]: view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We've already searched for a View instance in the original Annotated args, so just # extract the root annotation. annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration. import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic (eg: `list[int]`), try to unwrap any # extra type variables. if view_class is None and ( origin := get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \" { annotation } cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class @classmethod # TODO: Use typing.Self for return, pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view def check_annotation_compatibility ( self , annotation : Any ) -> None : # We're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated ( annotation ) system_type = self . type_system . to_system ( self . type , hints = {}) if not ( lenient_issubclass ( system_type , annotation ) or lenient_issubclass ( type ( system_type ), annotation ) ): raise ValueError ( f \" { annotation } cannot be used to represent { self . type } \" ) def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ): raise ValueError ( f \"expected an instance of { self . artifact_class } , got { type ( artifact ) } \" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" }: io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type: ignore[name-defined] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" }: io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), )","title":"Module arti.views"},{"location":"reference/arti/views/#sub-modules","text":"arti.views.python","title":"Sub-modules"},{"location":"reference/arti/views/#variables","text":"MODE","title":"Variables"},{"location":"reference/arti/views/#classes","text":"","title":"Classes"},{"location":"reference/arti/views/#view","text":"class View ( __pydantic_self__ , ** data : Any ) View Source class View ( Model ) : \"\"\"View represents the in-memory representation of the artifact. Examples include pandas.DataFrame, dask.DataFrame, a BigQuery table. \"\"\" _abstract_ = True _by_python_type_ : ClassVar [ dict[Optional[type ] , type [ View ] ]] = {} priority : ClassVar [ int ] = 0 # Set priority of this view for its python_type . Higher is better . python_type : ClassVar [ Optional[type ] ] type_system : ClassVar [ TypeSystem ] mode : MODE artifact_class : type [ Artifact ] = Artifact type : Type @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if not cls . _abstract_ : register ( cls . _by_python_type_ , cls . python_type , cls , lambda x : x . priority ) @classmethod def _check_type_compatibility ( cls , view_type : Type , artifact_type : Type ) -> None : # TODO : Consider supporting some form of \"reader schema\" where the View 's Type is a subset # of the Artifact' s Type ( and we filter the columns on read ). We could also consider # allowing the Producer 's Type to be a superset of the Artifact' s Type and we 'd filter the # columns on write. # # If implementing, we can leverage the `mode` to determine which should be the \"superset\". if view_type != artifact_type: raise ValueError( f\"the specified Type (`{view_type}`) is not compatible with the Artifact' s Type ( `{ artifact_type }` ). \" ) @validator(\" type \") @classmethod def _validate_type(cls, type_: Type, values: dict[str, Any]) -> Type: artifact_class: Optional[type[Artifact]] = values.get(\" artifact_class \") if artifact_class is None: return type_ # pragma: no cover artifact_type: Optional[Type] = get_field_default(artifact_class, \" type \") if artifact_type is not None: cls._check_type_compatibility(view_type=type_, artifact_type=artifact_type) return type_ @classmethod def _get_kwargs_from_annotation(cls, annotation: Any) -> dict[str, Any]: artifact_class = get_item_from_annotated( annotation, Artifact, is_subclass=True ) or get_field_default(cls, \" artifact_class \") assert artifact_class is not None assert issubclass(artifact_class, Artifact) # Try to extract or infer the Type. We prefer: an explicit Type in the annotation, followed # by an Artifact's default type, falling back to inferring a Type from the type hint. type_ = get_item_from_annotated(annotation, Type, is_subclass=False) if type_ is None: artifact_type: Optional[Type] = get_field_default(artifact_class, \" type \") if artifact_type is None: from arti.types.python import python_type_system type_ = python_type_system.to_artigraph(discard_Annotated(annotation), hints={}) else: type_ = artifact_type # NOTE: We validate that type_ and artifact_type (if set) are compatible in _validate_type, # which will run for *any* instance, not just those created with `.from_annotation`. return {\" artifact_class \": artifact_class, \" type \": type_} @classmethod # TODO: Use typing.Self for return, pending mypy support def get_class_for(cls, annotation: Any) -> builtins.type[View]: view_class = get_item_from_annotated(annotation, cls, is_subclass=True) if view_class is None: # We've already searched for a View instance in the original Annotated args, so just # extract the root annotation. annotation = discard_Annotated(annotation) # Import the View submodules to trigger registration. import_submodules(__path__, __name__) view_class = cls._by_python_type_.get(annotation) # If no match and the type is a subscripted Generic (eg: `list[int]`), try to unwrap any # extra type variables. if view_class is None and (origin := get_origin(annotation)) is not None: view_class = cls._by_python_type_.get(origin) if view_class is None: raise ValueError( f\" { annotation } cannot be matched to a View , try setting one explicitly ( eg : ` Annotated [ int, arti.views.python.Int ] ` ) \" ) return view_class @classmethod # TODO: Use typing.Self for return, pending mypy support def from_annotation(cls, annotation: Any, *, mode: MODE) -> View: view_class = cls.get_class_for(annotation) view = view_class(mode=mode, **cls._get_kwargs_from_annotation(annotation)) view.check_annotation_compatibility(annotation) return view def check_annotation_compatibility(self, annotation: Any) -> None: # We're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \") def check_artifact_compatibility(self, artifact: Artifact) -> None: if not isinstance(artifact, self.artifact_class): raise ValueError(f\" expected an instance of { self . artifact_class } , got { type ( artifact ) } \") self._check_type_compatibility(view_type=self.type, artifact_type=artifact.type) if self.mode in {\" READ \", \" READWRITE \"}: io._read.lookup( type(artifact.type), type(artifact.format), list[artifact.storage.storage_partition_type], # type: ignore[name-defined] type(self), ) if self.mode in {\" WRITE \", \" READWRITE \"}: io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), )","title":"View"},{"location":"reference/arti/views/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/#descendants","text":"arti.views.python.PythonBuiltin","title":"Descendants"},{"location":"reference/arti/views/#class-variables","text":"Config priority","title":"Class variables"},{"location":"reference/arti/views/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/views/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/#from_annotation","text":"def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view","title":"from_annotation"},{"location":"reference/arti/views/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/#get_class_for","text":"def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class","title":"get_class_for"},{"location":"reference/arti/views/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/#methods","text":"","title":"Methods"},{"location":"reference/arti/views/#check_annotation_compatibility","text":"def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \")","title":"check_annotation_compatibility"},{"location":"reference/arti/views/#check_artifact_compatibility","text":"def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), )","title":"check_artifact_compatibility"},{"location":"reference/arti/views/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/","text":"Module arti.views.python None None View Source from __future__ import annotations from datetime import date , datetime from arti.types.python import python_type_system from arti.views import View class PythonBuiltin ( View ): _abstract_ = True type_system = python_type_system class Date ( PythonBuiltin ): python_type = date class Datetime ( PythonBuiltin ): python_type = datetime class Dict ( PythonBuiltin ): python_type = dict class Float ( PythonBuiltin ): python_type = float class Int ( PythonBuiltin ): python_type = int class List ( PythonBuiltin ): python_type = list class Null ( PythonBuiltin ): python_type = None class Str ( PythonBuiltin ): python_type = str Classes Date class Date ( __pydantic_self__ , ** data : Any ) View Source class Date ( PythonBuiltin ): python_type = date Ancestors (in MRO) arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config priority python_type type_system Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_annotation def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods check_annotation_compatibility def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \") check_artifact_compatibility def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Datetime class Datetime ( __pydantic_self__ , ** data : Any ) View Source class Datetime ( PythonBuiltin ): python_type = datetime Ancestors (in MRO) arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config priority python_type type_system Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_annotation def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods check_annotation_compatibility def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \") check_artifact_compatibility def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Dict class Dict ( __pydantic_self__ , ** data : Any ) View Source class Dict ( PythonBuiltin ): python_type = dict Ancestors (in MRO) arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config priority python_type type_system Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_annotation def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods check_annotation_compatibility def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \") check_artifact_compatibility def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Float class Float ( __pydantic_self__ , ** data : Any ) View Source class Float ( PythonBuiltin ): python_type = float Ancestors (in MRO) arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config priority python_type type_system Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_annotation def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods check_annotation_compatibility def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \") check_artifact_compatibility def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int class Int ( __pydantic_self__ , ** data : Any ) View Source class Int ( PythonBuiltin ): python_type = int Ancestors (in MRO) arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config priority python_type type_system Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_annotation def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods check_annotation_compatibility def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \") check_artifact_compatibility def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . List class List ( __pydantic_self__ , ** data : Any ) View Source class List ( PythonBuiltin ): python_type = list Ancestors (in MRO) arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config priority python_type type_system Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_annotation def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods check_annotation_compatibility def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \") check_artifact_compatibility def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Null class Null ( __pydantic_self__ , ** data : Any ) View Source class Null ( PythonBuiltin ): python_type = None Ancestors (in MRO) arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config priority python_type type_system Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_annotation def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods check_annotation_compatibility def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \") check_artifact_compatibility def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . PythonBuiltin class PythonBuiltin ( __pydantic_self__ , ** data : Any ) View Source class PythonBuiltin ( View ): _abstract_ = True type_system = python_type_system Ancestors (in MRO) arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.views.python.Date arti.views.python.Datetime arti.views.python.Dict arti.views.python.Float arti.views.python.Int arti.views.python.List arti.views.python.Null arti.views.python.Str Class variables Config priority type_system Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_annotation def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods check_annotation_compatibility def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \") check_artifact_compatibility def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Str class Str ( __pydantic_self__ , ** data : Any ) View Source class Str ( PythonBuiltin ): python_type = str Ancestors (in MRO) arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config priority python_type type_system Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_annotation def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods check_annotation_compatibility def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \") check_artifact_compatibility def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), ) copy def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Python"},{"location":"reference/arti/views/python/#module-artiviewspython","text":"None None View Source from __future__ import annotations from datetime import date , datetime from arti.types.python import python_type_system from arti.views import View class PythonBuiltin ( View ): _abstract_ = True type_system = python_type_system class Date ( PythonBuiltin ): python_type = date class Datetime ( PythonBuiltin ): python_type = datetime class Dict ( PythonBuiltin ): python_type = dict class Float ( PythonBuiltin ): python_type = float class Int ( PythonBuiltin ): python_type = int class List ( PythonBuiltin ): python_type = list class Null ( PythonBuiltin ): python_type = None class Str ( PythonBuiltin ): python_type = str","title":"Module arti.views.python"},{"location":"reference/arti/views/python/#classes","text":"","title":"Classes"},{"location":"reference/arti/views/python/#date","text":"class Date ( __pydantic_self__ , ** data : Any ) View Source class Date ( PythonBuiltin ): python_type = date","title":"Date"},{"location":"reference/arti/views/python/#ancestors-in-mro","text":"arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#class-variables","text":"Config priority python_type type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_annotation","text":"def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view","title":"from_annotation"},{"location":"reference/arti/views/python/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for","text":"def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods","text":"","title":"Methods"},{"location":"reference/arti/views/python/#check_annotation_compatibility","text":"def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \")","title":"check_annotation_compatibility"},{"location":"reference/arti/views/python/#check_artifact_compatibility","text":"def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), )","title":"check_artifact_compatibility"},{"location":"reference/arti/views/python/#copy","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/#datetime","text":"class Datetime ( __pydantic_self__ , ** data : Any ) View Source class Datetime ( PythonBuiltin ): python_type = datetime","title":"Datetime"},{"location":"reference/arti/views/python/#ancestors-in-mro_1","text":"arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#class-variables_1","text":"Config priority python_type type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_annotation_1","text":"def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view","title":"from_annotation"},{"location":"reference/arti/views/python/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for_1","text":"def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables_1","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/views/python/#check_annotation_compatibility_1","text":"def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \")","title":"check_annotation_compatibility"},{"location":"reference/arti/views/python/#check_artifact_compatibility_1","text":"def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), )","title":"check_artifact_compatibility"},{"location":"reference/arti/views/python/#copy_1","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/#dict_2","text":"class Dict ( __pydantic_self__ , ** data : Any ) View Source class Dict ( PythonBuiltin ): python_type = dict","title":"Dict"},{"location":"reference/arti/views/python/#ancestors-in-mro_2","text":"arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#class-variables_2","text":"Config priority python_type type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#construct_2","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_annotation_2","text":"def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view","title":"from_annotation"},{"location":"reference/arti/views/python/#from_orm_2","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for_2","text":"def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file_2","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj_2","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw_2","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema_2","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json_2","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs_2","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate_2","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables_2","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/views/python/#check_annotation_compatibility_2","text":"def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \")","title":"check_annotation_compatibility"},{"location":"reference/arti/views/python/#check_artifact_compatibility_2","text":"def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), )","title":"check_artifact_compatibility"},{"location":"reference/arti/views/python/#copy_2","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict_3","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json_2","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/#float","text":"class Float ( __pydantic_self__ , ** data : Any ) View Source class Float ( PythonBuiltin ): python_type = float","title":"Float"},{"location":"reference/arti/views/python/#ancestors-in-mro_3","text":"arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#class-variables_3","text":"Config priority python_type type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#construct_3","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_annotation_3","text":"def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view","title":"from_annotation"},{"location":"reference/arti/views/python/#from_orm_3","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for_3","text":"def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file_3","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj_3","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw_3","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema_3","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json_3","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs_3","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate_3","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables_3","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods_3","text":"","title":"Methods"},{"location":"reference/arti/views/python/#check_annotation_compatibility_3","text":"def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \")","title":"check_annotation_compatibility"},{"location":"reference/arti/views/python/#check_artifact_compatibility_3","text":"def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), )","title":"check_artifact_compatibility"},{"location":"reference/arti/views/python/#copy_3","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict_4","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json_3","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/#int","text":"class Int ( __pydantic_self__ , ** data : Any ) View Source class Int ( PythonBuiltin ): python_type = int","title":"Int"},{"location":"reference/arti/views/python/#ancestors-in-mro_4","text":"arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#class-variables_4","text":"Config priority python_type type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#construct_4","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_annotation_4","text":"def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view","title":"from_annotation"},{"location":"reference/arti/views/python/#from_orm_4","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for_4","text":"def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file_4","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj_4","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw_4","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema_4","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json_4","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs_4","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate_4","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables_4","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods_4","text":"","title":"Methods"},{"location":"reference/arti/views/python/#check_annotation_compatibility_4","text":"def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \")","title":"check_annotation_compatibility"},{"location":"reference/arti/views/python/#check_artifact_compatibility_4","text":"def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), )","title":"check_artifact_compatibility"},{"location":"reference/arti/views/python/#copy_4","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict_5","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json_4","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/#list","text":"class List ( __pydantic_self__ , ** data : Any ) View Source class List ( PythonBuiltin ): python_type = list","title":"List"},{"location":"reference/arti/views/python/#ancestors-in-mro_5","text":"arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#class-variables_5","text":"Config priority python_type type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods_5","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#construct_5","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_annotation_5","text":"def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view","title":"from_annotation"},{"location":"reference/arti/views/python/#from_orm_5","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for_5","text":"def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file_5","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj_5","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw_5","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema_5","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json_5","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs_5","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate_5","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables_5","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods_5","text":"","title":"Methods"},{"location":"reference/arti/views/python/#check_annotation_compatibility_5","text":"def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \")","title":"check_annotation_compatibility"},{"location":"reference/arti/views/python/#check_artifact_compatibility_5","text":"def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), )","title":"check_artifact_compatibility"},{"location":"reference/arti/views/python/#copy_5","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict_6","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json_5","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/#null","text":"class Null ( __pydantic_self__ , ** data : Any ) View Source class Null ( PythonBuiltin ): python_type = None","title":"Null"},{"location":"reference/arti/views/python/#ancestors-in-mro_6","text":"arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#class-variables_6","text":"Config priority python_type type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods_6","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#construct_6","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_annotation_6","text":"def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view","title":"from_annotation"},{"location":"reference/arti/views/python/#from_orm_6","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for_6","text":"def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file_6","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj_6","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw_6","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema_6","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json_6","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs_6","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate_6","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables_6","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods_6","text":"","title":"Methods"},{"location":"reference/arti/views/python/#check_annotation_compatibility_6","text":"def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \")","title":"check_annotation_compatibility"},{"location":"reference/arti/views/python/#check_artifact_compatibility_6","text":"def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), )","title":"check_artifact_compatibility"},{"location":"reference/arti/views/python/#copy_6","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict_7","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json_6","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/#pythonbuiltin","text":"class PythonBuiltin ( __pydantic_self__ , ** data : Any ) View Source class PythonBuiltin ( View ): _abstract_ = True type_system = python_type_system","title":"PythonBuiltin"},{"location":"reference/arti/views/python/#ancestors-in-mro_7","text":"arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#descendants","text":"arti.views.python.Date arti.views.python.Datetime arti.views.python.Dict arti.views.python.Float arti.views.python.Int arti.views.python.List arti.views.python.Null arti.views.python.Str","title":"Descendants"},{"location":"reference/arti/views/python/#class-variables_7","text":"Config priority type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods_7","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#construct_7","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_annotation_7","text":"def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view","title":"from_annotation"},{"location":"reference/arti/views/python/#from_orm_7","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for_7","text":"def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file_7","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj_7","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw_7","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema_7","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json_7","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs_7","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate_7","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables_7","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods_7","text":"","title":"Methods"},{"location":"reference/arti/views/python/#check_annotation_compatibility_7","text":"def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \")","title":"check_annotation_compatibility"},{"location":"reference/arti/views/python/#check_artifact_compatibility_7","text":"def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), )","title":"check_artifact_compatibility"},{"location":"reference/arti/views/python/#copy_7","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict_8","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json_7","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/#str","text":"class Str ( __pydantic_self__ , ** data : Any ) View Source class Str ( PythonBuiltin ): python_type = str","title":"Str"},{"location":"reference/arti/views/python/#ancestors-in-mro_8","text":"arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#class-variables_8","text":"Config priority python_type type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods_8","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#construct_8","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_annotation_8","text":"def from_annotation ( annotation : 'Any' , * , mode : 'MODE' ) -> 'View' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def from_annotation ( cls , annotation : Any , * , mode : MODE ) -> View : view_class = cls . get_class_for ( annotation ) view = view_class ( mode = mode , ** cls . _get_kwargs_from_annotation ( annotation )) view . check_annotation_compatibility ( annotation ) return view","title":"from_annotation"},{"location":"reference/arti/views/python/#from_orm_8","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for_8","text":"def get_class_for ( annotation : 'Any' ) -> 'builtins.type[View]' View Source @classmethod # TODO : Use typing . Self for return , pending mypy support def get_class_for ( cls , annotation : Any ) -> builtins . type [ View ] : view_class = get_item_from_annotated ( annotation , cls , is_subclass = True ) if view_class is None : # We ' ve already searched for a View instance in the original Annotated args , so just # extract the root annotation . annotation = discard_Annotated ( annotation ) # Import the View submodules to trigger registration . import_submodules ( __path__ , __name__ ) view_class = cls . _by_python_type_ . get ( annotation ) # If no match and the type is a subscripted Generic ( eg : ` list [ int ] ` ), try to unwrap any # extra type variables . if view_class is None and ( origin : = get_origin ( annotation )) is not None : view_class = cls . _by_python_type_ . get ( origin ) if view_class is None : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[int, arti.views.python.Int]`)\" ) return view_class","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file_8","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj_8","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw_8","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema_8","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json_8","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs_8","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate_8","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables_8","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods_8","text":"","title":"Methods"},{"location":"reference/arti/views/python/#check_annotation_compatibility_8","text":"def check_annotation_compatibility ( self , annotation : 'Any' ) -> 'None' View Source def check_annotation_compatibility ( self , annotation : Any ) -> None : # We 're only checking the root annotation (lenient_issubclass ignores Annotated anyway), so # tidy up the value to improve error messages. annotation = discard_Annotated(annotation) system_type = self.type_system.to_system(self.type, hints={}) if not ( lenient_issubclass(system_type, annotation) or lenient_issubclass(type(system_type), annotation) ): raise ValueError(f\" { annotation } cannot be used to represent { self . type } \")","title":"check_annotation_compatibility"},{"location":"reference/arti/views/python/#check_artifact_compatibility_8","text":"def check_artifact_compatibility ( self , artifact : 'Artifact' ) -> 'None' View Source def check_artifact_compatibility ( self , artifact : Artifact ) -> None : if not isinstance ( artifact , self . artifact_class ) : raise ValueError ( f \"expected an instance of {self.artifact_class}, got {type(artifact)}\" ) self . _check_type_compatibility ( view_type = self . type , artifact_type = artifact . type ) if self . mode in { \"READ\" , \"READWRITE\" } : io . _read . lookup ( type ( artifact . type ), type ( artifact . format ), list [ artifact . storage . storage_partition_type ], # type : ignore [ name - defined ] type ( self ), ) if self . mode in { \"WRITE\" , \"READWRITE\" } : io . _write . lookup ( self . python_type , type ( artifact . type ), type ( artifact . format ), artifact . storage . storage_partition_type , type ( self ), )","title":"check_artifact_compatibility"},{"location":"reference/arti/views/python/#copy_8","text":"def copy ( self , * , deep : 'bool' = False , validate : 'bool' = True , ** kwargs : 'Any' ) -> 'Self' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> Self : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict_9","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json_8","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Optional [ bool ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"}]}